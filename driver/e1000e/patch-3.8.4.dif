diff -ruN intel-e1000e-3.8.4/src/e1000.h e1000e-3.8.4/src/e1000.h
--- intel-e1000e-3.8.4/src/e1000.h	2020-03-15 01:19:51.863950925 -0700
+++ e1000e-3.8.4/src/e1000.h	2021-04-25 17:48:51.476867042 -0700
@@ -30,90 +30,93 @@
 #endif
 #include "hw.h"
 
+#include "entl_device.h"
+
 struct e1000_info;
 
 #define e_dbg(format, arg...) \
-	netdev_dbg(hw->adapter->netdev, format, ## arg)
+	netdev_dbg(hw->adapter->netdev, format, ##arg)
 #define e_err(format, arg...) \
-	netdev_err(adapter->netdev, format, ## arg)
+	netdev_err(adapter->netdev, format, ##arg)
 #define e_info(format, arg...) \
-	netdev_info(adapter->netdev, format, ## arg)
+	netdev_info(adapter->netdev, format, ##arg)
 #define e_warn(format, arg...) \
-	netdev_warn(adapter->netdev, format, ## arg)
+	netdev_warn(adapter->netdev, format, ##arg)
 #define e_notice(format, arg...) \
-	netdev_notice(adapter->netdev, format, ## arg)
+	netdev_notice(adapter->netdev, format, ##arg)
 
 /* Interrupt modes, as used by the IntMode parameter */
-#define E1000E_INT_MODE_LEGACY		0
-#define E1000E_INT_MODE_MSI		1
-#define E1000E_INT_MODE_MSIX		2
+#define E1000E_INT_MODE_LEGACY 0
+#define E1000E_INT_MODE_MSI 1
+#define E1000E_INT_MODE_MSIX 2
 
 #ifndef CONFIG_E1000E_NAPI
 #define E1000_MAX_INTR 10
 
 #endif /* CONFIG_E1000E_NAPI */
 /* Tx/Rx descriptor defines */
-#define E1000_DEFAULT_TXD		256
-#define E1000_MAX_TXD			4096
-#define E1000_MIN_TXD			64
-
-#define E1000_DEFAULT_RXD		256
-#define E1000_MAX_RXD			4096
-#define E1000_MIN_RXD			64
+#define E1000_DEFAULT_TXD 256
+#define E1000_MAX_TXD 4096
+#define E1000_MIN_TXD 64
+
+#define E1000_DEFAULT_RXD 256
+#define E1000_MAX_RXD 4096
+#define E1000_MIN_RXD 64
 
-#define E1000_MIN_ITR_USECS		10 /* 100000 irq/sec */
-#define E1000_MAX_ITR_USECS		10000 /* 100    irq/sec */
+#define E1000_MIN_ITR_USECS 10	  /* 100000 irq/sec */
+#define E1000_MAX_ITR_USECS 10000 /* 100    irq/sec */
 
-#define E1000_FC_PAUSE_TIME		0x0680 /* 858 usec */
+#define E1000_FC_PAUSE_TIME 0x0680 /* 858 usec */
 
 /* How many Tx Descriptors do we need to call netif_wake_queue ? */
 /* How many Rx Buffers do we bundle into one write to the hardware ? */
-#define E1000_RX_BUFFER_WRITE		16 /* Must be power of 2 */
+#define E1000_RX_BUFFER_WRITE 16 /* Must be power of 2 */
 
-#define AUTO_ALL_MODES			0
-#define E1000_EEPROM_APME		0x0400
+#define AUTO_ALL_MODES 0
+#define E1000_EEPROM_APME 0x0400
 
-#define E1000_MNG_VLAN_NONE		(-1)
+#define E1000_MNG_VLAN_NONE (-1)
 
-#define DEFAULT_JUMBO			9234
+#define DEFAULT_JUMBO 9234
 
 /* Time to wait before putting the device into D3 if there's no link (in ms). */
-#define LINK_TIMEOUT		100
+#define LINK_TIMEOUT 100
 
 /* Count for polling __E1000_RESET condition every 10-20msec.
  * Experimentation has shown the reset can take approximately 210msec.
  */
-#define E1000_CHECK_RESET_COUNT		25
+#define E1000_CHECK_RESET_COUNT 25
 
-#define DEFAULT_RDTR			0
-#define DEFAULT_RADV			8
-#define BURST_RDTR			0x20
-#define BURST_RADV			0x20
-#define PCICFG_DESC_RING_STATUS		0xe4
-#define FLUSH_DESC_REQUIRED		0x100
+#define DEFAULT_RDTR 0
+#define DEFAULT_RADV 8
+#define BURST_RDTR 0x20
+#define BURST_RADV 0x20
+#define PCICFG_DESC_RING_STATUS 0xe4
+#define FLUSH_DESC_REQUIRED 0x100
 
 /* in the case of WTHRESH, it appears at least the 82571/2 hardware
  * writes back 4 descriptors when WTHRESH=5, and 3 descriptors when
  * WTHRESH=4, so a setting of 5 gives the most efficient bus
  * utilization but to avoid possible Tx stalls, set it to 1
  */
-#define E1000_TXDCTL_DMA_BURST_ENABLE                          \
-	(E1000_TXDCTL_GRAN | /* set descriptor granularity */  \
-	 E1000_TXDCTL_COUNT_DESC |                             \
-	 (1u << 16) | /* wthresh must be +1 more than desired */\
-	 (1u << 8)  | /* hthresh */                             \
-	 0x1f)       /* pthresh */
-
-#define E1000_RXDCTL_DMA_BURST_ENABLE                          \
-	(0x01000000 | /* set descriptor granularity */         \
-	 (4u << 16) | /* set writeback threshold    */         \
-	 (4u << 8)  | /* set prefetch threshold     */         \
-	 0x20)        /* set hthresh                */
+#define E1000_TXDCTL_DMA_BURST_ENABLE                        \
+	(E1000_TXDCTL_GRAN | /* set descriptor granularity */    \
+	 E1000_TXDCTL_COUNT_DESC |                               \
+	 (1u << 16) | /* wthresh must be +1 more than desired */ \
+	 (1u << 8) |  /* hthresh */                              \
+	 0x1f)		  /* pthresh */
+
+#define E1000_RXDCTL_DMA_BURST_ENABLE              \
+	(0x01000000 | /* set descriptor granularity */ \
+	 (4u << 16) | /* set writeback threshold    */ \
+	 (4u << 8) |  /* set prefetch threshold     */ \
+	 0x20)		  /* set hthresh                */
 
 #define E1000_TIDV_FPD BIT(31)
 #define E1000_RDTR_FPD BIT(31)
 
-enum e1000_boards {
+enum e1000_boards
+{
 	board_82571,
 	board_82572,
 	board_82573,
@@ -130,7 +133,8 @@
 	board_pch_cnp
 };
 
-struct e1000_ps_page {
+struct e1000_ps_page
+{
 	struct page *page;
 	u64 dma; /* must be u64 - written to hw */
 };
@@ -138,12 +142,15 @@
 /* wrappers around a pointer to a socket buffer,
  * so a DMA handle can be stored along with the buffer
  */
-struct e1000_buffer {
+struct e1000_buffer
+{
 	dma_addr_t dma;
 	struct sk_buff *skb;
-	union {
+	union
+	{
 		/* Tx */
-		struct {
+		struct
+		{
 			unsigned long time_stamp;
 			u16 length;
 			u16 next_to_watch;
@@ -152,7 +159,8 @@
 			u16 mapped_as_page;
 		};
 		/* Rx */
-		struct {
+		struct
+		{
 			/* arrays of page information for packet split */
 			struct e1000_ps_page *ps_pages;
 			struct page *page;
@@ -160,12 +168,13 @@
 	};
 };
 
-struct e1000_ring {
-	struct e1000_adapter *adapter;	/* back pointer to adapter */
-	void *desc;			/* pointer to ring memory  */
-	dma_addr_t dma;			/* phys address of ring    */
-	unsigned int size;		/* length of ring in bytes */
-	unsigned int count;		/* number of desc. in ring */
+struct e1000_ring
+{
+	struct e1000_adapter *adapter; /* back pointer to adapter */
+	void *desc;					   /* pointer to ring memory  */
+	dma_addr_t dma;				   /* phys address of ring    */
+	unsigned int size;			   /* length of ring in bytes */
+	unsigned int count;			   /* number of desc. in ring */
 
 	u16 next_to_use;
 	u16 next_to_clean;
@@ -187,20 +196,22 @@
 
 #ifdef SIOCGMIIPHY
 /* PHY register snapshot values */
-struct e1000_phy_regs {
-	u16 bmcr;		/* basic mode control register    */
-	u16 bmsr;		/* basic mode status register     */
-	u16 advertise;		/* auto-negotiation advertisement */
-	u16 lpa;		/* link partner ability register  */
-	u16 expansion;		/* auto-negotiation expansion reg */
-	u16 ctrl1000;		/* 1000BASE-T control register    */
-	u16 stat1000;		/* 1000BASE-T status register     */
-	u16 estatus;		/* extended status register       */
+struct e1000_phy_regs
+{
+	u16 bmcr;	   /* basic mode control register    */
+	u16 bmsr;	   /* basic mode status register     */
+	u16 advertise; /* auto-negotiation advertisement */
+	u16 lpa;	   /* link partner ability register  */
+	u16 expansion; /* auto-negotiation expansion reg */
+	u16 ctrl1000;  /* 1000BASE-T control register    */
+	u16 stat1000;  /* 1000BASE-T status register     */
+	u16 estatus;   /* extended status register       */
 };
 #endif
 
 /* board specific private data structure */
-struct e1000_adapter {
+struct e1000_adapter
+{
 	struct timer_list watchdog_timer;
 	struct timer_list phy_info_timer;
 	struct timer_list blink_timer;
@@ -239,7 +250,7 @@
 	struct napi_struct napi;
 #endif
 
-	unsigned int uncorr_errors;	/* uncorrectable ECC errors */
+	unsigned int uncorr_errors; /* uncorrectable ECC errors */
 	unsigned int corr_errors;	/* correctable ECC errors */
 	unsigned int restart_queue;
 	u32 txd_cmd;
@@ -272,12 +283,12 @@
 	/* Rx */
 #ifdef CONFIG_E1000E_NAPI
 	bool (*clean_rx)(struct e1000_ring *ring, int *work_done,
-			 int work_to_do) ____cacheline_aligned_in_smp;
+					 int work_to_do) ____cacheline_aligned_in_smp;
 #else
 	bool (*clean_rx)(struct e1000_ring *ring) ____cacheline_aligned_in_smp;
 #endif
 	void (*alloc_rx_buf)(struct e1000_ring *ring, int cleaned_count,
-			     gfp_t gfp);
+						 gfp_t gfp);
 	struct e1000_ring *rx_ring;
 
 	u32 rx_int_delay;
@@ -295,16 +306,16 @@
 	u32 rx_hwtstamp_cleared;
 #endif
 #ifdef DYNAMIC_LTR_SUPPORT
-	u64 c10_mpc_count;	/* frequently updated MPC count */
-	u64 c10_rx_bytes;	/* frequently updated RX bytes count */
-	u32 c10_pba_bytes;	/* current PBA RXA converted to bytes*/
-	bool c10_demote_ltr;	/* is/should LTR be demoted */
-#endif /* DYNAMIC_LTR_SUPPORT */
+	u64 c10_mpc_count;	 /* frequently updated MPC count */
+	u64 c10_rx_bytes;	 /* frequently updated RX bytes count */
+	u32 c10_pba_bytes;	 /* current PBA RXA converted to bytes*/
+	bool c10_demote_ltr; /* is/should LTR be demoted */
+#endif					 /* DYNAMIC_LTR_SUPPORT */
 
 	unsigned int rx_ps_pages;
 	u16 rx_ps_bsize0;
 #ifndef CONFIG_E1000E_NAPI
-	u64 rx_dropped_backlog;		/* count drops from rx int handler */
+	u64 rx_dropped_backlog; /* count drops from rx int handler */
 #endif
 	u32 max_frame_size;
 	u32 min_frame_size;
@@ -320,7 +331,7 @@
 	struct e1000_hw hw;
 
 #ifdef HAVE_NDO_GET_STATS64
-	spinlock_t stats64_lock;	/* protects statistics counters */
+	spinlock_t stats64_lock; /* protects statistics counters */
 #endif
 	struct e1000_hw_stats stats;
 	struct e1000_phy_info phy_info;
@@ -375,7 +386,7 @@
 	struct sk_buff *tx_hwtstamp_skb;
 	unsigned long tx_hwtstamp_start;
 	struct work_struct tx_hwtstamp_work;
-	spinlock_t systim_lock;	/* protects SYSTIML/H regsters */
+	spinlock_t systim_lock; /* protects SYSTIML/H regsters */
 	struct cyclecounter cc;
 	struct timecounter tc;
 #endif
@@ -391,15 +402,20 @@
 #endif
 	s32 ptp_delta;
 	u16 eee_advert;
+
+	u32 entl_flag;
+	entl_device_t entl_dev;
+	spinlock_t entl_txring_lock;
 };
 
-struct e1000_info {
-	enum e1000_mac_type	mac;
-	unsigned int		flags;
-	unsigned int		flags2;
-	u32			pba;
-	u32			max_hw_frame_size;
-	s32			(*get_variants)(struct e1000_adapter *);
+struct e1000_info
+{
+	enum e1000_mac_type mac;
+	unsigned int flags;
+	unsigned int flags2;
+	u32 pba;
+	u32 max_hw_frame_size;
+	s32 (*get_variants)(struct e1000_adapter *);
 	const struct e1000_mac_operations *mac_ops;
 	const struct e1000_phy_operations *phy_ops;
 	const struct e1000_nvm_operations *nvm_ops;
@@ -422,22 +438,22 @@
  * INCVALUE_n into the TIMINCA register allowing 32+8+(24-INCVALUE_SHIFT_n)
  * bits to count nanoseconds leaving the rest for fractional nonseconds.
  */
-#define INCVALUE_96MHZ		125
-#define INCVALUE_SHIFT_96MHZ	17
-#define INCPERIOD_SHIFT_96MHZ	2
-#define INCPERIOD_96MHZ		(12 >> INCPERIOD_SHIFT_96MHZ)
-
-#define INCVALUE_25MHZ		40
-#define INCVALUE_SHIFT_25MHZ	18
-#define INCPERIOD_25MHZ		1
-
-#define INCVALUE_24MHZ		125
-#define INCVALUE_SHIFT_24MHZ	14
-#define INCPERIOD_24MHZ		3
-
-#define INCVALUE_38400KHZ	26
-#define INCVALUE_SHIFT_38400KHZ	19
-#define INCPERIOD_38400KHZ	1
+#define INCVALUE_96MHZ 125
+#define INCVALUE_SHIFT_96MHZ 17
+#define INCPERIOD_SHIFT_96MHZ 2
+#define INCPERIOD_96MHZ (12 >> INCPERIOD_SHIFT_96MHZ)
+
+#define INCVALUE_25MHZ 40
+#define INCVALUE_SHIFT_25MHZ 18
+#define INCPERIOD_25MHZ 1
+
+#define INCVALUE_24MHZ 125
+#define INCVALUE_SHIFT_24MHZ 14
+#define INCPERIOD_24MHZ 3
+
+#define INCVALUE_38400KHZ 26
+#define INCVALUE_SHIFT_38400KHZ 19
+#define INCPERIOD_38400KHZ 1
 
 /* Another drawback of scaling the incvalue by a large factor is the
  * 64-bit SYSTIM register overflows more quickly.  This is dealt with
@@ -448,74 +464,75 @@
  * 96MHz	47-bit	2^(47-INCPERIOD_SHIFT_96MHz) / 10^9 / 3600 = 9.77 hrs
  * 25MHz	46-bit	2^46 / 10^9 / 3600 = 19.55 hours
  */
-#define E1000_SYSTIM_OVERFLOW_PERIOD	(HZ * 60 * 60 * 4)
-#define E1000_MAX_82574_SYSTIM_REREADS	50
-#define E1000_82574_SYSTIM_EPSILON	(1ULL << 35ULL)
+#define E1000_SYSTIM_OVERFLOW_PERIOD (HZ * 60 * 60 * 4)
+#define E1000_MAX_82574_SYSTIM_REREADS 50
+#define E1000_82574_SYSTIM_EPSILON (1ULL << 35ULL)
 #endif /* HAVE_HW_TIME_STAMP */
 
 /* hardware capability, feature, and workaround flags */
-#define FLAG_HAS_AMT                      BIT(0)
-#define FLAG_HAS_FLASH                    BIT(1)
-#define FLAG_HAS_HW_VLAN_FILTER           BIT(2)
-#define FLAG_HAS_WOL                      BIT(3)
+#define FLAG_HAS_AMT BIT(0)
+#define FLAG_HAS_FLASH BIT(1)
+#define FLAG_HAS_HW_VLAN_FILTER BIT(2)
+#define FLAG_HAS_WOL BIT(3)
 /* reserved BIT(4) */
-#define FLAG_HAS_CTRLEXT_ON_LOAD          BIT(5)
-#define FLAG_HAS_SWSM_ON_LOAD             BIT(6)
-#define FLAG_HAS_JUMBO_FRAMES             BIT(7)
+#define FLAG_HAS_CTRLEXT_ON_LOAD BIT(5)
+#define FLAG_HAS_SWSM_ON_LOAD BIT(6)
+#define FLAG_HAS_JUMBO_FRAMES BIT(7)
 /* reserved BIT(8) */
-#define FLAG_IS_ICH                       BIT(9)
-#define FLAG_HAS_MSIX                     BIT(10)
-#define FLAG_HAS_SMART_POWER_DOWN         BIT(11)
-#define FLAG_IS_QUAD_PORT_A               BIT(12)
-#define FLAG_IS_QUAD_PORT                 BIT(13)
-#define FLAG_HAS_HW_TIMESTAMP             BIT(14)
-#define FLAG_APME_IN_WUC                  BIT(15)
-#define FLAG_APME_IN_CTRL3                BIT(16)
-#define FLAG_APME_CHECK_PORT_B            BIT(17)
-#define FLAG_DISABLE_FC_PAUSE_TIME        BIT(18)
-#define FLAG_NO_WAKE_UCAST                BIT(19)
-#define FLAG_MNG_PT_ENABLED               BIT(20)
-#define FLAG_RESET_OVERWRITES_LAA         BIT(21)
-#define FLAG_TARC_SPEED_MODE_BIT          BIT(22)
-#define FLAG_TARC_SET_BIT_ZERO            BIT(23)
-#define FLAG_RX_NEEDS_RESTART             BIT(24)
-#define FLAG_LSC_GIG_SPEED_DROP           BIT(25)
-#define FLAG_SMART_POWER_DOWN             BIT(26)
-#define FLAG_MSI_ENABLED                  BIT(27)
+#define FLAG_IS_ICH BIT(9)
+#define FLAG_HAS_MSIX BIT(10)
+#define FLAG_HAS_SMART_POWER_DOWN BIT(11)
+#define FLAG_IS_QUAD_PORT_A BIT(12)
+#define FLAG_IS_QUAD_PORT BIT(13)
+#define FLAG_HAS_HW_TIMESTAMP BIT(14)
+#define FLAG_APME_IN_WUC BIT(15)
+#define FLAG_APME_IN_CTRL3 BIT(16)
+#define FLAG_APME_CHECK_PORT_B BIT(17)
+#define FLAG_DISABLE_FC_PAUSE_TIME BIT(18)
+#define FLAG_NO_WAKE_UCAST BIT(19)
+#define FLAG_MNG_PT_ENABLED BIT(20)
+#define FLAG_RESET_OVERWRITES_LAA BIT(21)
+#define FLAG_TARC_SPEED_MODE_BIT BIT(22)
+#define FLAG_TARC_SET_BIT_ZERO BIT(23)
+#define FLAG_RX_NEEDS_RESTART BIT(24)
+#define FLAG_LSC_GIG_SPEED_DROP BIT(25)
+#define FLAG_SMART_POWER_DOWN BIT(26)
+#define FLAG_MSI_ENABLED BIT(27)
 #ifndef HAVE_NDO_SET_FEATURES
-#define FLAG_RX_CSUM_ENABLED              BIT(28)
+#define FLAG_RX_CSUM_ENABLED BIT(28)
 #else
 /* reserved BIT(28) */
 #endif
-#define FLAG_TSO_FORCE                    BIT(29)
-#define FLAG_RESTART_NOW                  BIT(30)
-#define FLAG_MSI_TEST_FAILED              BIT(31)
-
-#define FLAG2_CRC_STRIPPING               BIT(0)
-#define FLAG2_HAS_PHY_WAKEUP              BIT(1)
-#define FLAG2_IS_DISCARDING               BIT(2)
-#define FLAG2_DISABLE_ASPM_L1             BIT(3)
-#define FLAG2_HAS_PHY_STATS               BIT(4)
-#define FLAG2_HAS_EEE                     BIT(5)
-#define FLAG2_DMA_BURST                   BIT(6)
-#define FLAG2_DISABLE_ASPM_L0S            BIT(7)
-#define FLAG2_DISABLE_AIM                 BIT(8)
-#define FLAG2_CHECK_PHY_HANG              BIT(9)
-#define FLAG2_NO_DISABLE_RX               BIT(10)
-#define FLAG2_PCIM2PCI_ARBITER_WA         BIT(11)
-#define FLAG2_DFLT_CRC_STRIPPING          BIT(12)
-#define FLAG2_CHECK_RX_HWTSTAMP           BIT(13)
-#define FLAG2_CHECK_SYSTIM_OVERFLOW       BIT(14)
+#define FLAG_TSO_FORCE BIT(29)
+#define FLAG_RESTART_NOW BIT(30)
+#define FLAG_MSI_TEST_FAILED BIT(31)
+
+#define FLAG2_CRC_STRIPPING BIT(0)
+#define FLAG2_HAS_PHY_WAKEUP BIT(1)
+#define FLAG2_IS_DISCARDING BIT(2)
+#define FLAG2_DISABLE_ASPM_L1 BIT(3)
+#define FLAG2_HAS_PHY_STATS BIT(4)
+#define FLAG2_HAS_EEE BIT(5)
+#define FLAG2_DMA_BURST BIT(6)
+#define FLAG2_DISABLE_ASPM_L0S BIT(7)
+#define FLAG2_DISABLE_AIM BIT(8)
+#define FLAG2_CHECK_PHY_HANG BIT(9)
+#define FLAG2_NO_DISABLE_RX BIT(10)
+#define FLAG2_PCIM2PCI_ARBITER_WA BIT(11)
+#define FLAG2_DFLT_CRC_STRIPPING BIT(12)
+#define FLAG2_CHECK_RX_HWTSTAMP BIT(13)
+#define FLAG2_CHECK_SYSTIM_OVERFLOW BIT(14)
 
-#define E1000_RX_DESC_PS(R, i)	    \
+#define E1000_RX_DESC_PS(R, i) \
 	(&(((union e1000_rx_desc_packet_split *)((R).desc))[i]))
-#define E1000_RX_DESC_EXT(R, i)	    \
+#define E1000_RX_DESC_EXT(R, i) \
 	(&(((union e1000_rx_desc_extended *)((R).desc))[i]))
-#define E1000_GET_DESC(R, i, type)	(&(((struct type *)((R).desc))[i]))
-#define E1000_TX_DESC(R, i)		E1000_GET_DESC(R, i, e1000_tx_desc)
-#define E1000_CONTEXT_DESC(R, i)	E1000_GET_DESC(R, i, e1000_context_desc)
+#define E1000_GET_DESC(R, i, type) (&(((struct type *)((R).desc))[i]))
+#define E1000_TX_DESC(R, i) E1000_GET_DESC(R, i, e1000_tx_desc)
+#define E1000_CONTEXT_DESC(R, i) E1000_GET_DESC(R, i, e1000_context_desc)
 
-enum e1000_state_t {
+enum e1000_state_t
+{
 	__E1000_OBFF_DISABLED,
 	__E1000_TESTING,
 	__E1000_RESETTING,
@@ -523,7 +540,8 @@
 	__E1000_DOWN
 };
 
-enum latency_range {
+enum latency_range
+{
 	lowest_latency = 0,
 	low_latency = 1,
 	bulk_latency = 2,
@@ -556,12 +574,12 @@
 #ifdef HAVE_NDO_GET_STATS64
 #ifdef HAVE_VOID_NDO_GET_STATS64
 void e1000e_get_stats64(struct net_device *netdev,
-			struct rtnl_link_stats64 *stats);
+						struct rtnl_link_stats64 *stats);
 #else
 struct rtnl_link_stats64 *e1000e_get_stats64(struct net_device *netdev,
-					     struct rtnl_link_stats64 *stats);
+											 struct rtnl_link_stats64 *stats);
 #endif /* HAVE_VOID_NDO_GET_STATS64 */
-#else /* HAVE_NDO_GET_STATS64 */
+#else  /* HAVE_NDO_GET_STATS64 */
 extern void e1000e_update_stats(struct e1000_adapter *adapter);
 #endif /* HAVE_NDO_GET_STATS64 */
 void e1000e_set_interrupt_capability(struct e1000_adapter *adapter);
@@ -591,8 +609,14 @@
 void e1000e_ptp_init(struct e1000_adapter *adapter);
 void e1000e_ptp_remove(struct e1000_adapter *adapter);
 #else
-#define e1000e_ptp_init(adapter) do {} while (0)
-#define e1000e_ptp_remove(adapter) do {} while (0)
+#define e1000e_ptp_init(adapter) \
+	do                           \
+	{                            \
+	} while (0)
+#define e1000e_ptp_remove(adapter) \
+	do                             \
+	{                              \
+	} while (0)
 #endif
 
 static inline s32 e1000_phy_hw_reset(struct e1000_hw *hw)
@@ -641,13 +665,13 @@
 }
 
 static inline s32 e1000_read_nvm(struct e1000_hw *hw, u16 offset, u16 words,
-				 u16 *data)
+								 u16 *data)
 {
 	return hw->nvm.ops.read(hw, offset, words, data);
 }
 
 static inline s32 e1000_write_nvm(struct e1000_hw *hw, u16 offset, u16 words,
-				  u16 *data)
+								  u16 *data)
 {
 	return hw->nvm.ops.write(hw, offset, words, data);
 }
@@ -662,14 +686,14 @@
 	return readl(hw->hw_addr + reg);
 }
 
-#define er32(reg)	__er32(hw, E1000_##reg)
+#define er32(reg) __er32(hw, E1000_##reg)
 
 s32 __ew32_prepare(struct e1000_hw *hw);
 void __ew32(struct e1000_hw *hw, unsigned long reg, u32 val);
 
-#define ew32(reg, val)	__ew32(hw, E1000_##reg, (val))
+#define ew32(reg, val) __ew32(hw, E1000_##reg, (val))
 
-#define e1e_flush()	er32(STATUS)
+#define e1e_flush() er32(STATUS)
 
 #define E1000_WRITE_REG_ARRAY(a, reg, offset, value) \
 	(__ew32((a), (reg + ((offset) << 2)), (value)))
diff -ruN intel-e1000e-3.8.4/src/ecnl_entl_if.h e1000e-3.8.4/src/ecnl_entl_if.h
--- intel-e1000e-3.8.4/src/ecnl_entl_if.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/ecnl_entl_if.h	2021-04-25 17:41:46.969475596 -0700
@@ -0,0 +1,74 @@
+#ifndef ECNL_ENTL_IF_H
+#define ECNL_ENTL_IF_H
+
+typedef struct ec_state {
+    uint64_t ecs_recover_count;
+    uint64_t ecs_recovered_count;
+    uint64_t ecs_s_count;
+    uint64_t ecs_r_count;
+    uint64_t ecs_entt_count;
+    uint64_t ecs_aop_count;
+    int ecs_link_state;
+    int ecs_num_queued;
+    struct timespec ecs_update_time;
+#ifdef ENTL_SPEED_CHECK
+    struct timespec ecs_interval_time;
+    struct timespec ecs_max_interval_time;
+    struct timespec ecs_min_interval_time;
+#endif
+} ec_state_t;
+
+// for now, allow jumbo packets
+#define EC_MESSAGE_MAX 9000
+typedef struct ec_ait_data {
+    uint32_t ecad_message_len;
+    char ecad_data[EC_MESSAGE_MAX];
+    // uint32_t op_code;
+} ec_ait_data_t;
+
+typedef struct ec_alo_reg {
+    uint32_t ecar_index;
+    uint64_t ecar_reg;
+} ec_alo_reg_t;
+
+typedef struct ec_alo_regs {
+    uint64_t ecars_regs[32];
+    uint32_t ecars_flags;
+} ec_alo_regs_t;
+
+#define ENCL_ENTL_MAGIC 0x5a00dead
+typedef struct entl_driver_funcs {
+    int (*edf_validate)           (struct net_device *dev, int magic, entl_mgr_t *mgr);
+    netdev_tx_t (*edf_start_xmit) (struct sk_buff *skb, struct net_device *dev);
+    int (*edf_send_AIT)           (struct sk_buff *skb, struct net_device *dev);
+    int (*edf_retrieve_AIT)       (struct net_device *dev, entt_ioctl_ait_data_t *data);
+    int (*edf_write_reg)          (struct net_device *dev, ec_alo_reg_t *reg);
+    int (*edf_read_regset)        (struct net_device *dev, ec_alo_regs_t *regs);
+    int (*edf_get_state)          (struct net_device *dev, ec_state_t *state);
+} entl_driver_funcs_t;
+
+#ifdef ENTL_ADAPT_IMPL
+static int adapt_validate(struct net_device *dev, int magic, entl_mgr_t *mgr); // { return 1; }
+static netdev_tx_t adapt_start_xmit(struct sk_buff *skb, struct net_device *e1000e); // { return NETDEV_TX_BUSY; }
+static int adapt_send_AIT(struct sk_buff *skb, struct net_device *e1000e); // { return -1; }
+static int adapt_retrieve_AIT(struct net_device *e1000e, entt_ioctl_ait_data_t *data); // { return -1; }
+static int adapt_write_reg(struct net_device *e1000e, ec_alo_reg_t *reg); // { return -1; }
+static int adapt_read_regset(struct net_device *e1000e, ec_alo_regs_t *regs); // { return -1; }
+static int adapt_get_state(struct net_device *dev, ec_state_t *state); // { return -1; }
+
+static entl_driver_funcs_t entl_adapt_funcs = {
+    .edf_validate = &adapt_validate,
+    .edf_start_xmit = &adapt_start_xmit,
+    .edf_send_AIT = &adapt_send_AIT,
+    .edf_retrieve_AIT = &adapt_retrieve_AIT,
+    .edf_write_reg = &adapt_write_reg,
+    .edf_read_regset = &adapt_read_regset,
+    .edf_get_state = &adapt_get_state,
+};
+
+EXPORT_SYMBOL(entl_adapt_funcs);
+#else
+extern entl_driver_funcs_t entl_adapt_funcs;
+#endif
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entl_device.c e1000e-3.8.4/src/entl_device.c
--- intel-e1000e-3.8.4/src/entl_device.c	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_device.c	2021-04-25 17:41:46.970475221 -0700
@@ -0,0 +1,1387 @@
+
+#define FETCH_STATE(stm) 0 /* ((stm)->current_state.current_state) */
+
+// newline should be unnecessary here - https://lwn.net/Articles/732420/
+#define ENTL_DEBUG(fmt, args...) printk(KERN_ALERT "ENTL: " fmt "\n", ## args)
+#define ADAPT_INFO(fmt, args...) printk(KERN_INFO "ADAPT: " fmt "\n", ## args)
+#define ENTL_DEBUG_NAME(_name, fmt, args...) ENTL_DEBUG("%s " fmt, _name, ## args)
+#define ADAPT_INFO_NAME(_name, fmt, args...) ADAPT_INFO("%s " fmt, _name, ## args)
+#define ADAPTER_DEBUG(adapter, fmt, args...) ENTL_DEBUG_NAME(adapter->netdev->name, fmt, ## args)
+
+#include "entl_skb_queue.h"
+#include "entl_state_machine.h"
+#include "entl_ioctl.h"
+#include "entl_device.h"
+#include "entl_user_api.h"
+
+#include "netdev_entl_if.h"
+#include "entl_stm_if.h"
+
+#define ENTL_ADAPT_IMPL
+#include "ecnl_entl_if.h"
+
+// copied e1000 routines:
+static void entl_e1000_configure(struct e1000_adapter *adapter);
+static void entl_e1000e_set_rx_mode(struct net_device *netdev);
+static void entl_e1000_setup_rctl(struct e1000_adapter *adapter);
+static void entl_e1000_configure_rx(struct e1000_adapter *adapter);
+
+// forward declarations (internal/private)
+static int inject_message(entl_device_t *dev, uint16_t emsg_raw, uint32_t seqno, int send_action);
+#ifndef BIONIC_3421
+static void entl_watchdog(unsigned long data);
+#else
+static void entl_watchdog(struct timer_list *t);
+#endif
+static void entl_watchdog_task(struct work_struct *work);
+// static void dump_state(char *type, entl_state_t *st, int flag); // debug
+
+// in theory, only the STM can do 'inject_message',
+// however we also have to guard against other driver actions (interrupts, hw maint)
+// STM_LOCK - stm->state_lock
+static inline int locked_inject(entl_device_t *dev, struct e1000_adapter *adapter, uint16_t emsg_raw, uint32_t seqno, int send_action) {
+    unsigned long flags;
+    spin_lock_irqsave(&adapter->entl_txring_lock, flags);
+    int inject_action = inject_message(dev, emsg_raw, seqno, send_action);
+    spin_unlock_irqrestore(&adapter->entl_txring_lock, flags);
+    return inject_action;
+}
+
+static char *emsg_names[] = {
+    "HELLO", // 0x0000
+    "EVENT", // 0x0001
+    "NOP",   // 0x0002
+    "AIT",   // 0x0003
+    "ACK"    // 0x0004
+};
+
+static inline char *emsg_op(uint16_t u_daddr) {
+    int opnum = get_entl_msg(u_daddr);
+    return (opnum < 5) ? emsg_names[opnum] : "??";
+}
+
+static char *letters = "0123456789abcdef";
+extern void dump_ait_data(entl_state_machine_t *stm, char *tag, struct entt_ioctl_ait_data *ait_data) {
+    void *d = ait_data->data;
+    int nbytes = ait_data->message_len;
+    int queued = ait_data->num_queued;
+    int space = ait_data->num_messages;
+    char window[3*41];
+    int f = 0;
+    for (int i = 0; i < nbytes; i++) {
+        char ch = ((char *) d)[i] & 0xff;
+        int n0 = (ch & 0xf0) >> 4;
+        int n1 = (ch & 0x0f);
+        window[f+0] = ' ';
+        window[f+1] = letters[n0];
+        window[f+2] = letters[n1];
+        window[f+3] = '\0';
+        f += 3;
+        if (f >= 3*40) break;
+    }
+    ENTL_DEBUG_NAME(stm->name, "%s - queued: %d space: %d nbytes: %d - %s", tag, queued, space, nbytes, window);
+}
+
+// inline helpers:
+static inline void unpack_eth(const uint8_t *p, uint16_t *u, uint32_t *l) {
+    uint16_t mac_hi = (uint16_t) p[0] << 8
+                    | (uint16_t) p[1];
+    uint32_t mac_lo = (uint32_t) p[2] << 24
+                    | (uint32_t) p[3] << 16
+                    | (uint32_t) p[4] <<  8
+                    | (uint32_t) p[5];
+    *u = mac_hi;
+    *l = mac_lo;
+}
+
+static inline void encode_dest(uint8_t *h_dest, uint16_t mac_hi, uint32_t mac_lo) {
+    unsigned char mac_addr[ETH_ALEN];
+    mac_addr[0] = (mac_hi >>  8) & 0xff;
+    mac_addr[1] = (mac_hi)       & 0xff;
+    mac_addr[2] = (mac_lo >> 24) & 0xff;
+    mac_addr[3] = (mac_lo >> 16) & 0xff;
+    mac_addr[4] = (mac_lo >>  8) & 0xff;
+    mac_addr[5] = (mac_lo)       & 0xff;
+    memcpy(h_dest, mac_addr, ETH_ALEN);
+}
+
+// netdev entry points: (from e1000_probe) w/adapter->entl_dev
+static void entl_device_init(entl_device_t *edev) {
+    memset(edev, 0, sizeof(struct entl_device));
+
+    entl_state_machine_t *stm = &edev->edev_stm;
+    entl_state_machine_init(stm, edev->edev_user_pid); // FIXME: huh?
+
+    // FIXME: name not set until register_netdev ??
+    // size_t elen = strlcpy(edev->edev_name, edev->name, sizeof(edev->edev_name));
+    // size_t slen = strlcpy(stm->name, edev->edev_name, sizeof(stm->name));
+
+    // watchdog timer & task setup
+#ifndef BIONIC_3421
+    init_timer(&edev->edev_watchdog_timer);
+    edev->edev_watchdog_timer.function = entl_watchdog;
+    edev->edev_watchdog_timer.data = (unsigned long) edev;
+#else
+    timer_setup(&edev->edev_watchdog_timer, entl_watchdog, 0);
+#endif
+    INIT_WORK(&edev->edev_watchdog_task, entl_watchdog_task);
+    ENTL_skb_queue_init(&edev->edev_tx_skb_queue);
+    edev->edev_queue_stopped = 0;
+}
+
+static void entl_device_link_down(entl_device_t *dev) {
+    entl_state_machine_t *stm = &dev->edev_stm;
+
+    dev->edev_flag = ENTL_DEVICE_FLAG_SIGNAL;
+    mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+}
+
+static void entl_device_link_up(entl_device_t *dev) {
+    entl_state_machine_t *stm = &dev->edev_stm;
+    entl_link_up(stm);
+    dev->edev_flag |= ENTL_DEVICE_FLAG_SIGNAL;
+    mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+
+    // FIXME: why redundant watchdog ??
+    // mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+
+    uint32_t entl_state = FETCH_STATE(stm);
+    if (entl_state == ENTL_STATE_HELLO) {
+        dev->edev_flag |= ENTL_DEVICE_FLAG_HELLO;
+        mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+    }
+}
+
+// tx queue empty, inject a new packet
+void entl_send_inject(entl_device_t *dev, struct e1000_adapter *adapter, entl_state_machine_t *stm) {
+    uint16_t emsg_raw = (uint16_t) -1; uint32_t seqno = (uint32_t) -1;
+    int send_action = entl_next_send(stm, &emsg_raw, &seqno);
+    if (get_entl_msg(emsg_raw) == ENTL_MESSAGE_NOP_U) return;
+
+    int inject_action = locked_inject(dev, adapter, emsg_raw, seqno, send_action);
+    if (inject_action == 0) {
+        dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_WAITING;
+        return;
+    }
+
+    // failed inject, invoke task
+    // __E1000_DOWN or (e1000_desc_unused(tx_ring) < 3) ??
+    if (inject_action == 1) {
+        // resource error, retry
+        dev->edev_u_addr = emsg_raw;
+        dev->edev_l_addr = seqno;
+        dev->edev_action = send_action;
+        dev->edev_flag |= ENTL_DEVICE_FLAG_RETRY;
+        mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+    }
+    // some kind of skb issue?
+    else if (inject_action == -1) {
+        entl_state_error(stm, ENTL_ERROR_FATAL);
+        dev->edev_flag |= ENTL_DEVICE_FLAG_SIGNAL;
+        mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+    }
+}
+
+// ISR context
+// returns
+// true when netdev.c should continue to process packet
+// false when packet has been consumed
+static bool entl_device_process_rx_packet(entl_device_t *dev, struct sk_buff *skb) {
+    struct e1000_adapter *adapter = container_of(dev, struct e1000_adapter, entl_dev);
+    struct ethhdr *eth = (struct ethhdr *) skb->data;
+
+    unsigned int len = skb->len;
+    if (len <= sizeof(struct ethhdr)) {
+        // FIXME
+        ENTL_DEBUG_NAME(dev->edev_stm.name, "process_rx - runt len %d", len);
+    }
+
+    uint16_t smac_hi; uint32_t smac_lo; unpack_eth(eth->h_source, &smac_hi, &smac_lo);
+    uint16_t emsg_raw; uint32_t seqno; unpack_eth(eth->h_dest, &emsg_raw, &seqno);
+
+    bool retval = true;
+    if (emsg_raw & ENTL_MESSAGE_ONLY_U) retval = false;
+
+    entl_state_machine_t *stm = &dev->edev_stm;
+    int recv_action = entl_received(stm, smac_hi, smac_lo, emsg_raw, seqno);
+
+    if (recv_action == ENTL_ACTION_ERROR) {
+        dev->edev_flag |= (ENTL_DEVICE_FLAG_HELLO | ENTL_DEVICE_FLAG_SIGNAL);
+        mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+        return retval;
+    }
+
+    if (recv_action == ENTL_ACTION_SIG_ERR) {
+        dev->edev_flag |= ENTL_DEVICE_FLAG_SIGNAL;
+        mod_timer(&dev->edev_watchdog_timer, jiffies + 1);
+        return retval;
+    }
+
+    if (recv_action & ENTL_ACTION_PROC_AIT) {
+        unsigned int len = skb->len;
+        if (len <= sizeof(struct ethhdr)) {
+            // FIXME
+        }
+        else {
+            // level0 protocol layout: { ethhdr nbytes payload }
+            unsigned char *level0 = skb->data + sizeof(struct ethhdr);
+            unsigned char *payload = level0 + sizeof(uint32_t);
+
+            uint32_t nbytes;
+            memcpy(&nbytes, level0, sizeof(uint32_t));
+
+            // FIXME: MAX_AIT_MESSAGE_SIZE 256
+            // entt_ioctl_ait_data_t should really be dynamically sized (kzalloc'ed)
+            // NOTE: invariant: MAX_AIT_MESSAGE_SIZE <= sizeof(ait_data->data)
+            struct entt_ioctl_ait_data *ait_data = kzalloc(sizeof(struct entt_ioctl_ait_data), GFP_ATOMIC);
+            if ((nbytes > 0) && (nbytes <= MAX_AIT_MESSAGE_SIZE)) {
+                ait_data->message_len = nbytes;
+                memcpy(ait_data->data, payload, nbytes);
+                // ait_data->num_messages = 0;
+                // ait_data->num_queued = 0;
+                dump_ait_data(stm, "process_rx - recv", ait_data);
+            }
+            else {
+                ait_data->message_len = 0;
+                // ait_data->num_messages = 0;
+                // ait_data->num_queued = 0;
+            }
+            ENTL_DEBUG_NAME(stm->name, "process_rx - message 0x%04x (%s) seqno %d payload len %d", emsg_raw, emsg_op(emsg_raw), seqno, ait_data->message_len);
+            entl_new_AIT_message(stm, ait_data);
+        }
+    }
+
+    // FIXME: rather than a 'level', should have an event obj queue, that includes 'seqno'
+    if (recv_action & ENTL_ACTION_SIG_AIT) {
+        ENTL_DEBUG_NAME(stm->name, "process_tx - signal seqno %d", seqno);
+        dev->edev_flag |= ENTL_DEVICE_FLAG_SIGNAL2;
+    }
+
+    if (recv_action & ENTL_ACTION_SEND) {
+        // SEND_DAT - check if TX queue has data
+        if (recv_action & ENTL_ACTION_SEND_DAT && ENTL_skb_queue_has_data(&dev->edev_tx_skb_queue)) {
+            // TX queue has data, so transfer with data
+            struct sk_buff *dt = ENTL_skb_queue_front_pop(&dev->edev_tx_skb_queue);
+
+            // skip: GSO can't be used for ENTL
+            while (NULL != dt && skb_is_gso(dt)) {
+                e1000_xmit_frame(dt, adapter->netdev);
+                dt = ENTL_skb_queue_front_pop(&dev->edev_tx_skb_queue);
+            }
+
+            if (dt) {
+                e1000_xmit_frame(dt, adapter->netdev);
+            }
+            else {
+                entl_send_inject(dev, adapter, stm);
+            }
+
+            // netif queue handling for flow control
+            if (dev->edev_queue_stopped && ENTL_skb_queue_unused(&dev->edev_tx_skb_queue) > 2) {
+                netif_start_queue(adapter->netdev);
+                dev->edev_queue_stopped = 0;
+            }
+        }
+        else {
+            entl_send_inject(dev, adapter, stm);
+        }
+    }
+
+    return retval;
+}
+
+// from patch/netdev.c:
+// Assumes NOT interrupt context
+// process packet being sent. The ENTL message can only be sent over the single (non MSS) packet
+static void entl_device_process_tx_packet(entl_device_t *dev, struct sk_buff *skb) {
+    struct ethhdr *eth = (struct ethhdr *) skb->data;
+
+    // maximum segment size (MSS) packet can't be used for ENTL message (will use a header over multiple packets)
+    if (skb_is_gso(skb)) {
+        encode_dest(eth->h_dest, ENTL_MESSAGE_NOP_U, 0);
+    }
+    else {
+        entl_state_machine_t *stm = &dev->edev_stm;
+
+// might be offline(no carrier), or be newly online after offline ??
+        uint16_t emsg_raw = (uint16_t) -1; uint32_t seqno = (uint32_t) -1;
+        int send_action = entl_next_send_tx(stm, &emsg_raw, &seqno);
+        encode_dest(eth->h_dest, emsg_raw, seqno);
+
+        // FIXME: rather than a 'level', should have an event obj queue, that includes 'seqno'
+        if (send_action & ENTL_ACTION_SIG_AIT) {
+            ENTL_DEBUG_NAME(stm->name, "process_tx - signal seqno %d", seqno);
+            dev->edev_flag |= ENTL_DEVICE_FLAG_SIGNAL2; // AIT send completion signal
+        }
+
+        if (emsg_raw != ENTL_MESSAGE_NOP_U) {
+ENTL_DEBUG_NAME(stm->name, "process_tx - message 0x%04x (%s) seqno %d", emsg_raw, emsg_op(emsg_raw), seqno);
+            dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_WAITING;
+        }
+    }
+}
+
+static int entl_do_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd) {
+    struct e1000_adapter *adapter = netdev_priv(netdev);
+    entl_device_t *dev = &adapter->entl_dev;
+    struct e1000_hw *hw = &adapter->hw;
+    entl_state_machine_t *stm = &dev->edev_stm;
+
+    switch (cmd) {
+#if 0
+// could combine these into:
+    case SIOCDEVPRIVATE_ENTL_RD_CURRENT:
+    case SIOCDEVPRIVATE_ENTL_RD_ERROR: {
+        struct e1000_hw *hw = &adapter->hw;
+        uint32_t link = !hw->mac.get_link_status;
+        struct entl_ioctl_data entl_data;
+        entl_data.link_state = link;
+
+        if (cmd == SIOCDEVPRIVATE_ENTL_RD_CURRENT) {
+            entl_read_current_state(stm, &entl_data.state, &entl_data.error_state);
+        }
+        else {
+            // FIXME: CLEARS error_state!!
+            entl_read_error_state(stm, &entl_data.state, &entl_data.error_state);
+        }
+        entl_data.num_queued = entl_num_queued(stm);
+        copy_to_user(ifr->ifr_data, &entl_data, sizeof(struct entl_ioctl_data));
+    }
+    break;
+#endif
+    case SIOCDEVPRIVATE_ENTL_RD_CURRENT: {
+        // FIXME: switch to public API ?
+        // e1000e_has_link(struct e1000_adapter *adapter)
+        // or netif_carrier_ok(struct net_device *netdev = adapter->netdev;)
+        struct e1000_hw *hw = &adapter->hw;
+        uint32_t link = !hw->mac.get_link_status;
+        struct entl_ioctl_data entl_data;
+        entl_data.link_state = link;
+
+        entl_read_current_state(stm, &entl_data.state, &entl_data.error_state);
+        entl_data.num_queued = entl_num_queued(stm);
+        copy_to_user(ifr->ifr_data, &entl_data, sizeof(struct entl_ioctl_data));
+    }
+    break;
+
+    case SIOCDEVPRIVATE_ENTL_RD_ERROR: {
+        struct e1000_hw *hw = &adapter->hw;
+        uint32_t link = !hw->mac.get_link_status;
+        struct entl_ioctl_data entl_data;
+        entl_data.link_state = link;
+        // FIXME: CLEARS error_state!!
+        entl_read_error_state(stm, &entl_data.state, &entl_data.error_state);
+        entl_data.num_queued = entl_num_queued(stm);
+        copy_to_user(ifr->ifr_data, &entl_data, sizeof(struct entl_ioctl_data));
+        // dump_state("current", &entl_data.state, 1);
+        // dump_state("error", &entl_data.error_state, 0);
+    }
+    break;
+
+    case SIOCDEVPRIVATE_ENTL_SET_SIGRCVR: {
+        struct entl_ioctl_data entl_data;
+        copy_from_user(&entl_data, ifr->ifr_data, sizeof(struct entl_ioctl_data));
+        dev->edev_user_pid = entl_data.pid;
+    }
+    break;
+
+    case SIOCDEVPRIVATE_ENTL_GEN_SIGNAL:
+    break;
+
+    case SIOCDEVPRIVATE_ENTL_DO_INIT: {
+        adapter->entl_flag = 1;
+        entl_e1000_configure(adapter);
+        uint32_t icr = er32(ICR);
+        uint32_t ctrl = er32(CTRL);
+        uint32_t ims = er32(IMS);
+    }
+    break;
+
+    case SIOCDEVPRIVATE_ENTT_SEND_AIT: {
+        struct entt_ioctl_ait_data *ait_data = kzalloc(sizeof(struct entt_ioctl_ait_data), GFP_ATOMIC);
+        copy_from_user(ait_data, ifr->ifr_data, sizeof(struct entt_ioctl_ait_data));
+
+ENTL_DEBUG_NAME(stm->name, "ioctl - entl_send_AIT_message");
+        int q_space = entl_send_AIT_message(stm, ait_data);
+        int pending = sendq_count(stm); // cheat
+        // result data
+        ait_data->num_queued = pending;
+        ait_data->num_messages = q_space;
+        dump_ait_data(stm, "ioctl - sendq_push", ait_data);
+        copy_to_user(ifr->ifr_data, ait_data, sizeof(struct entt_ioctl_ait_data));
+
+        if (q_space < 0) {
+            kfree(ait_data); // FIXME: check for memory leak?
+        }
+    }
+    break;
+
+    case SIOCDEVPRIVATE_ENTT_READ_AIT: {
+ENTL_DEBUG_NAME(stm->name, "ioctl - entl_read_AIT_message");
+        struct entt_ioctl_ait_data *ait_data = entl_read_AIT_message(stm); // recvq_pop
+        if (ait_data) {
+            dump_ait_data(stm, "ioctl - recvq_pop", ait_data);
+            copy_to_user(ifr->ifr_data, ait_data, sizeof(struct entt_ioctl_ait_data));
+            kfree(ait_data);
+        }
+        else {
+            struct entt_ioctl_ait_data dt;
+            dt.message_len = 0;
+            dt.num_messages = 0;
+            dt.num_queued = entl_num_queued(stm);
+            copy_to_user(ifr->ifr_data, &dt, sizeof(struct entt_ioctl_ait_data));
+        }
+    }
+    break;
+
+    default:
+        ENTL_DEBUG_NAME(netdev->name, "ioctl error: undefined cmd %d", cmd);
+        break;
+    }
+
+    return 0;
+}
+
+// name here is problematical - called before register_netdev (but after netdev->name = pci_name)
+// called both from netdev (patch) and edev_init
+static void entl_e1000_set_my_addr(struct e1000_adapter *adapter, const uint8_t *addr) {
+    ENTL_DEBUG_NAME(adapter->netdev->name, "init - macaddr %02x:%02x:%02x:%02x:%02x:%02x", addr[0], addr[1], addr[2], addr[3], addr[4], addr[5]);
+    uint16_t u_addr; uint32_t l_addr; unpack_eth(addr, &u_addr, &l_addr);
+
+    // FIXME: mcn name not set up ??
+    entl_device_t *edev = &adapter->entl_dev;
+    entl_state_machine_t *stm = &edev->edev_stm;
+    entl_set_my_adder(stm, u_addr, l_addr);
+}
+
+// netdev entry points: (from entl_e1000_configure)
+static void edev_init(struct e1000_adapter *adapter) {
+    struct net_device *netdev = adapter->netdev;
+    entl_device_t *edev = &adapter->entl_dev;
+    entl_state_machine_t *stm = &edev->edev_stm;
+
+    ENTL_DEBUG_NAME(netdev->name, "edev_init");
+
+    // update name(s) to match adapter->netdev->name
+    size_t elen = strlcpy(edev->edev_name, netdev->name, sizeof(edev->edev_name));
+    size_t slen = strlcpy(stm->name, edev->edev_name, sizeof(stm->name));
+
+    entl_e1000_set_my_addr(adapter, netdev->dev_addr);
+#if 0
+    // force link status check by watchdog (taken care of elsewhere)
+    struct e1000_hw *hw = &adapter->hw;
+    hw->mac.get_link_status = true;
+#endif
+}
+
+#ifdef ENTL_TX_ON_ENTL_ENABLE
+static netdev_tx_t entl_tx_transmit(struct sk_buff *skb, struct net_device *netdev) {
+    struct e1000_adapter *adapter = netdev_priv(netdev);
+    entl_device_t *dev = &adapter->entl_dev;
+    ENTL_skb_queue_t *q = &dev->edev_tx_skb_queue;
+
+    if (ENTL_skb_queue_full(q)) {
+        BUG_ON(q->count >= q->size);
+        return NETDEV_TX_BUSY;
+    }
+
+    struct ethhdr *eth = (struct ethhdr *) skb->data;
+    if ((eth->h_proto != ETH_P_ECLP) && (eth->h_proto != ETH_P_ECLD)) {
+        dev_kfree_skb_any(skb);
+        return NETDEV_TX_OK;
+    }
+
+    ENTL_skb_queue_back_push(q, skb);
+
+    int avail = ENTL_skb_queue_unused(q);
+    if (avail < 2) {
+        netif_stop_queue(netdev);
+        dev->edev_queue_stopped = 1;
+        return NETDEV_TX_BUSY;
+    }
+
+    return NETDEV_TX_OK;
+}
+#endif
+
+// internal
+
+// FIXME - I hate to do debug printf's here but:
+// https://github.com/torvalds/linux/blob/master/kernel/printk/printk.c#L2023
+// process_tx - message 0x0000 seqno 0
+
+// emsg_raw, seqno
+static int inject_message(entl_device_t *dev, uint16_t emsg_raw, uint32_t seqno, int send_action) {
+    struct e1000_adapter *adapter = container_of(dev, struct e1000_adapter, entl_dev);
+    if (test_bit(__E1000_DOWN, &adapter->state)) return 1;
+
+    struct net_device *netdev = adapter->netdev;
+    struct pci_dev *pdev = adapter->pdev;
+    struct e1000_ring *tx_ring = adapter->tx_ring;
+    if (e1000_desc_unused(tx_ring) < 3) return 1;
+
+    entl_state_machine_t *stm = &dev->edev_stm;
+
+    struct entt_ioctl_ait_data *ait_data;
+    int len;
+    if (send_action & ENTL_ACTION_SEND_AIT) {
+ENTL_DEBUG_NAME(stm->name, "inject - entl_next_AIT_message (%s)", emsg_op(emsg_raw));
+        ait_data = entl_next_AIT_message(stm); // fetch payload data
+        len = ETH_HLEN + ait_data->message_len + sizeof(uint32_t);
+// FIXME: here we know how bit the actual frame will be ; last chance to discard it
+        if (len < ETH_ZLEN) len = ETH_ZLEN; // min 60 - include/uapi/linux/if_ether.h
+    }
+    else {
+        ait_data = NULL;
+        len = ETH_ZLEN;
+    }
+
+    len += ETH_FCS_LEN;
+    struct sk_buff *skb = __netdev_alloc_skb(netdev, len, GFP_ATOMIC);
+    if (!skb) {
+        return -1;
+    }
+
+    skb->len = len;
+
+    struct ethhdr *eth = (struct ethhdr *) skb->data;
+    memcpy(eth->h_source, netdev->dev_addr, ETH_ALEN);
+    emsg_raw |= 0x8000; // message only
+    encode_dest(eth->h_dest, emsg_raw, seqno);
+    eth->h_proto = 0; // ETH_P_ECLP : protocol type is not used anyway
+
+    if (ait_data) {
+        // copy ait_data payload into skb
+        if (send_action & ENTL_ACTION_SEND_AIT) {
+            unsigned char *level0 = skb->data + sizeof(struct ethhdr);
+            unsigned char *payload = level0 + sizeof(uint32_t);
+            memcpy(level0, &ait_data->message_len, sizeof(uint32_t));
+            memcpy(payload, ait_data->data, ait_data->message_len);
+            dump_ait_data(stm, "tx_ring - inject", ait_data);
+        }
+    }
+
+    int i = adapter->tx_ring->next_to_use;
+    struct e1000_buffer *buffer_info = &tx_ring->buffer_info[i];
+    buffer_info->length = skb->len;
+    buffer_info->time_stamp = jiffies;
+    buffer_info->next_to_watch = i;
+    buffer_info->dma = dma_map_single(&pdev->dev, skb->data, skb->len, DMA_TO_DEVICE);
+    buffer_info->mapped_as_page = false;
+    if (dma_mapping_error(&pdev->dev, buffer_info->dma)) {
+        buffer_info->dma = 0;
+        dev_kfree_skb_any(skb);
+        return -1;
+    }
+
+    buffer_info->skb = skb;
+    // report number of byte queued for sending to the device hardware queue
+    netdev_sent_queue(netdev, skb->len);
+
+    // process e1000_tx_queue
+    uint32_t txd_upper = 0;
+    uint32_t txd_lower = E1000_TXD_CMD_IFCS;
+    struct e1000_tx_desc *tx_desc = E1000_TX_DESC(*tx_ring, i);
+    tx_desc->buffer_addr = cpu_to_le64(buffer_info->dma);
+    tx_desc->upper.data = cpu_to_le32(txd_upper);
+    tx_desc->lower.data = cpu_to_le32(txd_lower | buffer_info->length);
+    tx_desc->lower.data |= cpu_to_le32(adapter->txd_cmd);
+
+    i++;
+    if (i == tx_ring->count) i = 0;
+
+    /* Force memory writes to complete before letting h/w know there are new descriptors to fetch.
+     * (Only applicable for weak-ordered memory model archs, such as IA-64).
+     */
+    wmb();
+
+    tx_ring->next_to_use = i;
+
+    // Update TDT register in the NIC
+    if (adapter->flags2 & FLAG2_PCIM2PCI_ARBITER_WA)
+        e1000e_update_tdt_wa(tx_ring, tx_ring->next_to_use);
+    else
+        writel(tx_ring->next_to_use, tx_ring->tail);
+
+    /* we need this if more than one processor can write to our tail at a time,
+     * it synchronizes IO on IA64/Altix systems
+     */
+    mmiowb();
+    return 0;
+}
+
+#ifndef BIONIC_3421
+static void entl_watchdog(unsigned long data) {
+    entl_device_t *dev = (entl_device_t *)data;
+    schedule_work(&dev->edev_watchdog_task); // use global kernel work queue
+}
+#else
+static void entl_watchdog(struct timer_list *t) {
+    entl_device_t *dev = from_timer(dev, t, edev_watchdog_timer);
+    schedule_work(&dev->edev_watchdog_task); // use global kernel work queue
+}
+#endif
+
+// ref: sigaction.2 - void handler(int sig, siginfo_t *info, void *ucontext)
+static inline void notify_listener(int subscriber, int sigusr) {
+    // POSIX.1b signals - siginfo_t.h
+    struct siginfo info = {
+        .si_signo = SIGIO,
+        .si_int = 1,
+        .si_code = SI_QUEUE
+    };
+    struct task_struct *task = pid_task(find_vpid(subscriber), PIDTYPE_PID);
+    if (task != NULL) send_sig_info(sigusr, &info, task);
+}
+
+static inline void notify_manager(entl_mgr_t *mgr, int sigusr) {
+    mgr->emf_event(mgr, sigusr);
+}
+
+static void entl_watchdog_task(struct work_struct *work) {
+    unsigned long wakeup = 1 * HZ;  // one second
+
+    entl_device_t *dev = container_of(work, entl_device_t, edev_watchdog_task); // get the struct pointer from a member
+    struct e1000_adapter *adapter = container_of(dev, struct e1000_adapter, entl_dev);
+
+    if (!dev->edev_flag) {
+        dev->edev_flag |= ENTL_DEVICE_FLAG_WAITING;
+        goto restart_watchdog;
+    }
+
+    entl_mgr_t *manager = dev->edev_mgr;
+    int subscriber = dev->edev_user_pid;
+    if (subscriber || manager) {
+        if (dev->edev_flag & ENTL_DEVICE_FLAG_SIGNAL) {
+            dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_SIGNAL;
+            if (subscriber) notify_listener(subscriber, SIGUSR1);
+            if (manager) notify_manager(manager, SIGUSR1);
+        }
+        else if (dev->edev_flag & ENTL_DEVICE_FLAG_SIGNAL2) {
+            dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_SIGNAL2;
+            if (subscriber) notify_listener(subscriber, SIGUSR2);
+            if (manager) notify_manager(manager, SIGUSR2);
+        }
+    }
+
+    // notice carrier (i.e. link up)
+    if (netif_carrier_ok(adapter->netdev) && (dev->edev_flag & ENTL_DEVICE_FLAG_HELLO)) {
+        struct e1000_ring *tx_ring = adapter->tx_ring;
+        if (test_bit(__E1000_DOWN, &adapter->state)) {
+            goto restart_watchdog;
+        }
+
+        int t;
+        if ((t = e1000_desc_unused(tx_ring)) < 3) {
+            goto restart_watchdog;
+        }
+
+        entl_state_machine_t *stm = &dev->edev_stm;
+        uint32_t entl_state = FETCH_STATE(stm);
+
+        if ((entl_state == ENTL_STATE_HELLO)
+        ||  (entl_state == ENTL_STATE_WAIT)
+        ||  (entl_state == ENTL_STATE_RECEIVE)
+        ||  (entl_state == ENTL_STATE_AM)
+        ||  (entl_state == ENTL_STATE_BH)) {
+            uint16_t emsg_raw = (uint16_t) -1; uint32_t seqno = (uint32_t) -1;
+            int send_action = entl_get_hello(stm, &emsg_raw, &seqno);
+            if (send_action) {
+                int inject_action = locked_inject(dev, adapter, emsg_raw, seqno, send_action);
+                if (inject_action == 0) {
+                    dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_HELLO;
+                }
+            }
+        }
+        else {
+            dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_HELLO;
+        }
+    }
+    else if (dev->edev_flag & ENTL_DEVICE_FLAG_RETRY) {
+        struct e1000_adapter *adapter = container_of(dev, struct e1000_adapter, entl_dev);
+        if (test_bit(__E1000_DOWN, &adapter->state)) goto restart_watchdog;
+
+        struct e1000_ring *tx_ring = adapter->tx_ring;
+        if (e1000_desc_unused(tx_ring) < 3) goto restart_watchdog;
+
+        // uint16_t emsg_raw = dev->edev_u_addr;
+        // uint32_t seqno = dev->edev_l_addr;
+        // int action = dev->edev_action;
+        int inject_action = locked_inject(dev, adapter, dev->edev_u_addr, dev->edev_l_addr, dev->edev_action);
+
+        if (inject_action == 0) {
+            dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_RETRY;
+            dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_WAITING;
+        }
+    }
+    else if (dev->edev_flag & ENTL_DEVICE_FLAG_WAITING) {
+        dev->edev_flag &= ~(uint32_t) ENTL_DEVICE_FLAG_WAITING;
+        uint32_t entl_state = FETCH_STATE(stm);
+        if ((entl_state == ENTL_STATE_HELLO)
+        ||  (entl_state == ENTL_STATE_WAIT)
+        ||  (entl_state == ENTL_STATE_RECEIVE)
+        ||  (entl_state == ENTL_STATE_AM)
+        ||  (entl_state == ENTL_STATE_BH)) {
+            dev->edev_flag |= ENTL_DEVICE_FLAG_HELLO;
+        }
+    }
+
+restart_watchdog:
+    mod_timer(&dev->edev_watchdog_timer, round_jiffies(jiffies + wakeup));
+}
+
+// unused, debug
+#if 0
+static void dump_state(char *type, entl_state_t *st, int flag) {
+    ENTL_DEBUG(
+        "%s"
+        " event_i_know: %d"
+        " event_i_sent: %d"
+        " event_send_next: %d"
+        " current_state: %d"
+        " error_flag %x"
+        " p_error %x"
+        " error_count %d"
+        " @ %ld.%ld",
+
+        type,
+        st->event_i_know,
+        st->event_i_sent,
+        st->event_send_next,
+        st->current_state,
+        st->error_flag,
+        st->p_error_flag,
+        st->error_count,
+        st->update_time.tv_sec, st->update_time.tv_nsec
+    );
+
+    if (st->error_flag) {
+        ENTL_DEBUG("  Error time: %ld.%ld", st->error_time.tv_sec, st->error_time.tv_nsec);
+    }
+#ifdef ENTL_SPEED_CHECK
+    if (flag) {
+        ENTL_DEBUG("  interval_time    : %ld.%ld", st->interval_time.tv_sec, st->interval_time.tv_nsec);
+        ENTL_DEBUG("  max_interval_time: %ld.%ld", st->max_interval_time.tv_sec, st->max_interval_time.tv_nsec);
+        ENTL_DEBUG("  min_interval_time: %ld.%ld", st->min_interval_time.tv_sec, st->min_interval_time.tv_nsec);
+    }
+#endif
+}
+#endif
+
+// derivative work - ref: orig-frag-netdev.c, copied-frag-entl_device.c
+
+// entl version of e1000_configure
+/**
+ * e1000_configure - configure the hardware for Rx and Tx
+ * @adapter: private board structure
+ **/
+static void entl_e1000_configure(struct e1000_adapter *adapter) {
+        struct e1000_ring *rx_ring = adapter->rx_ring;
+
+        entl_e1000e_set_rx_mode(adapter->netdev);
+#if defined(NETIF_F_HW_VLAN_TX) || defined(NETIF_F_HW_VLAN_CTAG_TX)
+        e1000_restore_vlan(adapter);
+#endif
+        e1000_init_manageability_pt(adapter);
+
+        // We don’t need immediate interrupt on Tx completion.
+        // (unless buffer was full and quick responce is required, but that’s not likely)
+        e1000_configure_tx(adapter);
+
+#ifdef NETIF_F_RXHASH
+        if (adapter->netdev->features & NETIF_F_RXHASH)
+                e1000e_setup_rss_hash(adapter);
+#endif
+        entl_e1000_setup_rctl(adapter);
+        entl_e1000_configure_rx(adapter);
+        adapter->alloc_rx_buf(rx_ring, e1000_desc_unused(rx_ring), GFP_KERNEL);
+#ifdef ENTL
+        edev_init(adapter);
+#endif
+}
+
+/**
+ * entl_e1000e_set_rx_mode - ENTL versin, always set Promiscuous mode
+ * @netdev: network interface device structure
+ *
+ * The ndo_set_rx_mode entry point is called whenever the unicast or multicast
+ * address list or the network interface flags are updated.  This routine is
+ * responsible for configuring the hardware for proper unicast, multicast,
+ * promiscuous mode, and all-multi behavior.
+ **/
+static void entl_e1000e_set_rx_mode(struct net_device *netdev)
+{
+	struct e1000_adapter *adapter = netdev_priv(netdev);
+	struct e1000_hw *hw = &adapter->hw;
+	u32 rctl;
+
+	if (pm_runtime_suspended(netdev->dev.parent))
+		return;
+
+	/* Check for Promiscuous and All Multicast modes */
+	rctl = er32(RCTL);                                           
+
+#ifdef ENTL
+	// behave as if IFF_PROMISC is always set
+	rctl |= (E1000_RCTL_UPE | E1000_RCTL_MPE);
+#ifdef HAVE_VLAN_RX_REGISTER
+	rctl &= ~E1000_RCTL_VFE;
+#else
+	/* Do not hardware filter VLANs in promisc mode */
+	e1000e_vlan_filter_disable(adapter);
+#endif /* HAVE_VLAN_RX_REGISTER */
+
+        ADAPTER_DEBUG(adapter, "entl_e1000e_set_rx_mode RCTL = %08x", rctl);
+#else
+	/* clear the affected bits */
+	rctl &= ~(E1000_RCTL_UPE | E1000_RCTL_MPE);
+
+	if (netdev->flags & IFF_PROMISC) {
+		rctl |= (E1000_RCTL_UPE | E1000_RCTL_MPE);
+#ifdef HAVE_VLAN_RX_REGISTER
+		rctl &= ~E1000_RCTL_VFE;
+#else
+		/* Do not hardware filter VLANs in promisc mode */
+		e1000e_vlan_filter_disable(adapter);
+#endif /* HAVE_VLAN_RX_REGISTER */
+	} else {
+		int count;
+
+		if (netdev->flags & IFF_ALLMULTI) {
+			rctl |= E1000_RCTL_MPE;
+		} else {
+			/* Write addresses to the MTA, if the attempt fails
+			 * then we should just turn on promiscuous mode so
+			 * that we can at least receive multicast traffic
+			 */
+			count = e1000e_write_mc_addr_list(netdev);
+			if (count < 0)
+				rctl |= E1000_RCTL_MPE;
+		}
+#ifdef HAVE_VLAN_RX_REGISTER
+		if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER)
+			rctl |= E1000_RCTL_VFE;
+#else
+		e1000e_vlan_filter_enable(adapter);
+#endif
+#ifdef HAVE_SET_RX_MODE
+		/* Write addresses to available RAR registers, if there is not
+		 * sufficient space to store all the addresses then enable
+		 * unicast promiscuous mode
+		 */
+		count = e1000e_write_uc_addr_list(netdev);
+		if (count < 0)
+			rctl |= E1000_RCTL_UPE;
+#endif /* HAVE_SET_RX_MODE */
+	}
+#endif /* ENTL */
+
+	ew32(RCTL, rctl);
+#ifndef HAVE_VLAN_RX_REGISTER
+
+#ifdef NETIF_F_HW_VLAN_CTAG_RX
+	if (netdev->features & NETIF_F_HW_VLAN_CTAG_RX)
+#else
+	if (netdev->features & NETIF_F_HW_VLAN_RX)
+#endif
+		e1000e_vlan_strip_enable(adapter);
+	else
+		e1000e_vlan_strip_disable(adapter);
+#endif /* HAVE_VLAN_RX_REGISTER */
+}
+
+/**
+ * entl_e1000_setup_rctl - ENTL version of configure the receive control registers
+ * @adapter: Board private structure
+ **/
+static void entl_e1000_setup_rctl(struct e1000_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	u32 rctl, rfctl;
+	u32 pages = 0;
+
+	/* Workaround Si errata on PCHx - configure jumbo frame flow.
+	 * If jumbo frames not set, program related MAC/PHY registers
+	 * to h/w defaults
+	 */
+	if (hw->mac.type >= e1000_pch2lan) {
+		s32 ret_val;
+
+		if (adapter->netdev->mtu > ETH_DATA_LEN)
+			ret_val = e1000_lv_jumbo_workaround_ich8lan(hw, true);
+		else
+			ret_val = e1000_lv_jumbo_workaround_ich8lan(hw, false);
+
+		if (ret_val)
+			e_dbg("failed to enable|disable jumbo frame workaround mode\n");
+	}
+
+	/* Program MC offset vector base */
+	rctl = er32(RCTL);
+	rctl &= ~(3 << E1000_RCTL_MO_SHIFT);
+	rctl |= E1000_RCTL_EN | E1000_RCTL_BAM |
+	    E1000_RCTL_LBM_NO | E1000_RCTL_RDMTS_HALF |
+	    (adapter->hw.mac.mc_filter_type << E1000_RCTL_MO_SHIFT);
+
+	/* Do not Store bad packets */
+	rctl &= ~E1000_RCTL_SBP;
+
+	/* Enable Long Packet receive */
+	if (adapter->netdev->mtu <= ETH_DATA_LEN) {
+		ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl %d <= %d", adapter->netdev->mtu, ETH_DATA_LEN);
+		rctl &= ~E1000_RCTL_LPE;
+	}
+	else {
+		ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl %d > %d", adapter->netdev->mtu, ETH_DATA_LEN);
+		rctl |= E1000_RCTL_LPE;
+	}
+
+	/* Some systems expect that the CRC is included in SMBUS traffic. The
+	 * hardware strips the CRC before sending to both SMBUS (BMC) and to
+	 * host memory when this is enabled
+	 */
+	if (adapter->flags2 & FLAG2_CRC_STRIPPING)
+		rctl |= E1000_RCTL_SECRC;
+
+	/* Workaround Si errata on 82577/82578 - configure IPG for jumbos */
+	if ((hw->mac.type == e1000_pchlan) && (rctl & E1000_RCTL_LPE)) {
+		u32 mac_data;
+		u16 phy_data;
+
+		ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl Workaround Si errata on 82577/82578 - configure IPG for jumbos");
+
+		e1e_rphy(hw, PHY_REG(770, 26), &phy_data);
+		phy_data &= 0xfff8;
+		phy_data |= (1 << 2);
+		e1e_wphy(hw, PHY_REG(770, 26), phy_data);
+
+		mac_data = er32(FFLT_DBG);
+		mac_data |= (1 << 17);
+		ew32(FFLT_DBG, mac_data);
+
+		if (hw->phy.type == e1000_phy_82577) {
+			e1e_rphy(hw, 22, &phy_data);
+			phy_data &= 0x0fff;
+			phy_data |= (1 << 14);
+			e1e_wphy(hw, 0x10, 0x2823);
+			e1e_wphy(hw, 0x11, 0x0003);
+			e1e_wphy(hw, 22, phy_data);
+		}
+	}
+
+	/* Setup buffer sizes */
+	rctl &= ~E1000_RCTL_SZ_4096;
+	rctl |= E1000_RCTL_BSEX;
+	switch (adapter->rx_buffer_len) {
+	case 2048:
+	default:
+		ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl E1000_RCTL_SZ_2048");
+		rctl |= E1000_RCTL_SZ_2048;
+		rctl &= ~E1000_RCTL_BSEX;
+		break;
+	case 4096:
+		ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl E1000_RCTL_SZ_4096");
+		rctl |= E1000_RCTL_SZ_4096;
+		break;
+	case 8192:
+		ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl E1000_RCTL_SZ_8192");
+		rctl |= E1000_RCTL_SZ_8192;
+		break;
+	case 16384:
+		ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl E1000_RCTL_SZ_16384");
+		rctl |= E1000_RCTL_SZ_16384;
+		break;
+	}
+
+	/* Enable Extended Status in all Receive Descriptors */
+	rfctl = er32(RFCTL);
+	rfctl |= E1000_RFCTL_EXTEN;
+	ew32(RFCTL, rfctl);
+
+	/* 82571 and greater support packet-split where the protocol
+	 * header is placed in skb->data and the packet data is
+	 * placed in pages hanging off of skb_shinfo(skb)->nr_frags.
+	 * In the case of a non-split, skb->data is linearly filled,
+	 * followed by the page buffers.  Therefore, skb->data is
+	 * sized to hold the largest protocol header.
+	 *
+	 * allocations using alloc_page take too long for regular MTU
+	 * so only enable packet split for jumbo frames
+	 *
+	 * Using pages when the page size is greater than 16k wastes
+	 * a lot of memory, since we allocate 3 pages at all times
+	 * per packet.
+	 */
+	pages = PAGE_USE_COUNT(adapter->netdev->mtu);
+	if ((pages <= 3) && (PAGE_SIZE <= 16384) && (rctl & E1000_RCTL_LPE))
+		adapter->rx_ps_pages = pages;
+	else
+		adapter->rx_ps_pages = 0;
+
+	ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl rx_ps_pages = %d", adapter->rx_ps_pages);
+
+	if (adapter->rx_ps_pages) {
+		u32 psrctl = 0;
+
+		/* Enable Packet split descriptors */
+		rctl |= E1000_RCTL_DTYP_PS;
+
+		psrctl |= adapter->rx_ps_bsize0 >> E1000_PSRCTL_BSIZE0_SHIFT;
+
+		switch (adapter->rx_ps_pages) {
+		case 3:
+			psrctl |= PAGE_SIZE << E1000_PSRCTL_BSIZE3_SHIFT;
+			/* fall-through */
+		case 2:
+			psrctl |= PAGE_SIZE << E1000_PSRCTL_BSIZE2_SHIFT;
+			/* fall-through */
+		case 1:
+			psrctl |= PAGE_SIZE >> E1000_PSRCTL_BSIZE1_SHIFT;
+			break;
+		}
+
+		ew32(PSRCTL, psrctl);
+	}
+
+	/* This is useful for sniffing bad packets. */
+	if (adapter->netdev->features & NETIF_F_RXALL) {
+		/* UPE and MPE will be handled by normal PROMISC logic
+		 * in e1000e_set_rx_mode
+		 */
+		rctl |= (E1000_RCTL_SBP |	/* Receive bad packets */
+			 E1000_RCTL_BAM |	/* RX All Bcast Pkts */
+			 E1000_RCTL_PMCF);	/* RX All MAC Ctrl Pkts */
+
+		rctl &= ~(E1000_RCTL_VFE |	/* Disable VLAN filter */
+			  E1000_RCTL_DPF |	/* Allow filtered pause */
+			  E1000_RCTL_CFIEN);	/* Dis VLAN CFIEN Filter */
+		/* Do not mess with E1000_CTRL_VME, it affects transmit as well,
+		 * and that breaks VLANs.
+		 */
+	}
+        ADAPTER_DEBUG(adapter, "entl_e1000_setup_rctl RCTL = %08x", rctl);
+
+	ew32(RCTL, rctl);
+	/* just started the receive unit, no need to restart */
+	adapter->flags &= ~FLAG_RESTART_NOW;
+}
+
+/**
+ * entl_e1000_configure_rx - ENTL version of Configure Receive Unit after Reset
+ * @adapter: board private structure
+ *
+ * Configure the Rx unit of the MAC after a reset.
+ **/
+static void entl_e1000_configure_rx(struct e1000_adapter *adapter)
+{
+	struct e1000_hw *hw = &adapter->hw;
+	struct e1000_ring *rx_ring = adapter->rx_ring;
+	u64 rdba;
+	u32 rdlen, rctl, rxcsum, ctrl_ext;
+
+	if (adapter->rx_ps_pages) {
+		/* this is a 32 byte descriptor */
+		rdlen = rx_ring->count *
+		    sizeof(union e1000_rx_desc_packet_split);
+		adapter->clean_rx = e1000_clean_rx_irq_ps;
+		adapter->alloc_rx_buf = e1000_alloc_rx_buffers_ps;
+		ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx use e1000_alloc_rx_buffers_ps");
+#ifdef CONFIG_E1000E_NAPI
+	} else if (adapter->netdev->mtu > ETH_FRAME_LEN + ETH_FCS_LEN) {
+		rdlen = rx_ring->count * sizeof(union e1000_rx_desc_extended);
+		adapter->clean_rx = e1000_clean_jumbo_rx_irq;
+		adapter->alloc_rx_buf = e1000_alloc_jumbo_rx_buffers;
+		ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx use e1000_alloc_jumbo_rx_buffers");
+#endif
+	} else {
+		rdlen = rx_ring->count * sizeof(union e1000_rx_desc_extended);
+		adapter->clean_rx = e1000_clean_rx_irq;
+		adapter->alloc_rx_buf = e1000_alloc_rx_buffers;
+		ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx use e1000_alloc_rx_buffers");
+	}
+
+	/* disable receives while setting up the descriptors */
+	rctl = er32(RCTL);
+	if (!(adapter->flags2 & FLAG2_NO_DISABLE_RX))
+		ew32(RCTL, rctl & ~E1000_RCTL_EN);
+	e1e_flush();
+	usleep_range(10000, 20000);
+
+	if (adapter->flags2 & FLAG2_DMA_BURST) {
+		ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx set DMA burst");
+		/* set the writeback threshold (only takes effect if the RDTR
+		 * is set). set GRAN=1 and write back up to 0x4 worth, and
+		 * enable prefetching of 0x20 Rx descriptors
+		 * granularity = 01
+		 * wthresh = 04,
+		 * hthresh = 04,
+		 * pthresh = 0x20
+		 */
+		ew32(RXDCTL(0), E1000_RXDCTL_DMA_BURST_ENABLE);
+		ew32(RXDCTL(1), E1000_RXDCTL_DMA_BURST_ENABLE);
+
+		/* override the delay timers for enabling bursting, only if
+		 * the value was not set by the user via module options
+		 */
+		if (adapter->rx_int_delay == DEFAULT_RDTR)
+			adapter->rx_int_delay = BURST_RDTR;
+		if (adapter->rx_abs_int_delay == DEFAULT_RADV)
+			adapter->rx_abs_int_delay = BURST_RADV;
+	}
+
+	/* set the Receive Delay Timer Register */
+	ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx set Receive Delay Timer Register = %d", adapter->rx_int_delay);
+	ew32(RDTR, adapter->rx_int_delay);
+
+	/* irq moderation */
+	ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx set Abs Delay Timer Register = %d", adapter->rx_abs_int_delay);
+	ew32(RADV, adapter->rx_abs_int_delay);
+	if ((adapter->itr_setting != 0) && (adapter->itr != 0))
+		e1000e_write_itr(adapter, adapter->itr);
+
+	ctrl_ext = er32(CTRL_EXT);
+#ifdef CONFIG_E1000E_NAPI
+	/* Auto-Mask interrupts upon ICR access */
+	ctrl_ext |= E1000_CTRL_EXT_IAME;
+	ew32(IAM, 0xffffffff);
+#endif
+	ew32(CTRL_EXT, ctrl_ext);
+	e1e_flush();
+
+	/* Setup the HW Rx Head and Tail Descriptor Pointers and
+	 * the Base and Length of the Rx Descriptor Ring
+	 */
+	rdba = rx_ring->dma;
+	ew32(RDBAL(0), (rdba & DMA_BIT_MASK(32)));
+	ew32(RDBAH(0), (rdba >> 32));
+	ew32(RDLEN(0), rdlen);
+	ew32(RDH(0), 0);
+	ew32(RDT(0), 0);
+	rx_ring->head = adapter->hw.hw_addr + E1000_RDH(0);
+	rx_ring->tail = adapter->hw.hw_addr + E1000_RDT(0);
+
+	/* Enable Receive Checksum Offload for TCP and UDP */
+	rxcsum = er32(RXCSUM);
+#ifdef HAVE_NDO_SET_FEATURES
+	if (adapter->netdev->features & NETIF_F_RXCSUM)
+#else
+	if (adapter->flags & FLAG_RX_CSUM_ENABLED)
+#endif
+		rxcsum |= E1000_RXCSUM_TUOFL;
+	else
+		rxcsum &= ~E1000_RXCSUM_TUOFL;
+	ew32(RXCSUM, rxcsum);
+
+	/* With jumbo frames, excessive C-state transition latencies result
+	 * in dropped transactions.
+	 */
+	if (adapter->netdev->mtu > ETH_DATA_LEN) {
+		u32 lat =
+		    ((er32(PBA) & E1000_PBA_RXA_MASK) * 1024 -
+		     adapter->max_frame_size) * 8 / 1000;
+
+		ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx adapter->netdev->mtu %d > ETH_DATA_LEN %d lat = %d", adapter->netdev->mtu, ETH_DATA_LEN, lat);
+
+		if (adapter->flags & FLAG_IS_ICH) {
+			u32 rxdctl = er32(RXDCTL(0));
+
+			ew32(RXDCTL(0), rxdctl | 0x3);
+		}
+#ifdef HAVE_PM_QOS_REQUEST_LIST_NEW
+		pm_qos_update_request(&adapter->pm_qos_req, lat);
+#elif defined(HAVE_PM_QOS_REQUEST_LIST)
+		pm_qos_update_request(&adapter->pm_qos_req, lat);
+#else
+		pm_qos_update_requirement(PM_QOS_CPU_DMA_LATENCY,
+					  adapter->netdev->name, lat);
+#endif
+	} else {
+		ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx adapter->netdev->mtu %d <= ETH_DATA_LEN %d default qos = %d", adapter->netdev->mtu, ETH_DATA_LEN, PM_QOS_DEFAULT_VALUE);
+
+#ifdef HAVE_PM_QOS_REQUEST_LIST_NEW
+		pm_qos_update_request(&adapter->pm_qos_req,
+				      PM_QOS_DEFAULT_VALUE);
+#elif defined(HAVE_PM_QOS_REQUEST_LIST)
+		pm_qos_update_request(&adapter->pm_qos_req,
+				      PM_QOS_DEFAULT_VALUE);
+#else
+		pm_qos_update_requirement(PM_QOS_CPU_DMA_LATENCY,
+					  adapter->netdev->name,
+					  PM_QOS_DEFAULT_VALUE);
+#endif
+	}
+	ADAPTER_DEBUG(adapter, "entl_e1000_configure_rx RCTL = %08x", rctl);
+
+	/* Enable Receives */
+	ew32(RCTL, rctl);
+}
+
+// ENTL - ECNL linkage
+
+// minimal i/f compatibility
+static int adapt_validate(struct net_device *e1000e, int magic, entl_mgr_t *mgr) {
+    ADAPT_INFO_NAME(e1000e->name, "adapt_validate");
+
+    struct e1000_adapter *adapter = netdev_priv(e1000e);
+    if (adapter == NULL) return -1;
+    entl_device_t *entl_dev = &adapter->entl_dev;
+    if (entl_dev == NULL) return -1;
+    entl_state_machine_t *stm = &entl_dev->edev_stm;
+    if (stm == NULL) return -1;
+
+    int compat = (magic == ENCL_ENTL_MAGIC);
+    if (compat) entl_dev->edev_mgr = mgr;
+
+    return (compat) ? 1 : -1; // ENCL_ENTL_MAGIC 0x5affdead
+}
+
+// ref: linux/netdevice.h - enum netdev_tx
+static netdev_tx_t adapt_start_xmit(struct sk_buff *skb, struct net_device *e1000e) {
+    ADAPT_INFO_NAME(e1000e->name, "adapt_start_xmit");
+    if (skb == NULL) return -1;
+
+#if 0
+    return NETDEV_TX_OK; // 0x00
+#endif
+    return NETDEV_TX_BUSY; // 0x10
+}
+
+// edf_send_AIT
+static int adapt_send_AIT(struct sk_buff *skb, struct net_device *e1000e) {
+    ADAPT_INFO_NAME(e1000e->name, "adapt_send_AIT");
+    if (skb == NULL) return -1;
+
+    struct e1000_adapter *adapter = netdev_priv(e1000e);
+    if (adapter == NULL) return -1;
+    entl_device_t *entl_dev = &adapter->entl_dev;
+    if (entl_dev == NULL) return -1;
+    entl_state_machine_t *stm = &entl_dev->edev_stm;
+    if (stm == NULL) return -1;
+
+    // FIXME : nl_ecnl_send_ait_message and ecnl_forward_ait_message disagree about the type of 'skb' here.
+    struct ec_ait_data *dt = (struct ec_ait_data *) skb;
+
+    int nbytes = dt->ecad_message_len;
+    if (nbytes > MAX_AIT_MESSAGE_SIZE) {
+        nbytes = MAX_AIT_MESSAGE_SIZE;
+        ADAPT_INFO_NAME(e1000e->name, "adapt_send_AIT oversize frame: %d truncated (%d)", dt->ecad_message_len, nbytes);
+    }
+
+    struct entt_ioctl_ait_data *ait_data = kzalloc(sizeof(struct entt_ioctl_ait_data), GFP_ATOMIC);
+    memcpy(ait_data->data, dt->ecad_data, nbytes);
+    ait_data->message_len = nbytes; // inject_message : memcpy(payload, ait_data->data, ait_data->message_len);
+
+    int q_space = entl_send_AIT_message(stm, ait_data); // sendq_push
+    int pending = sendq_count(stm); // cheat
+    // result data
+    ait_data->num_queued = pending;
+    ait_data->num_messages = q_space;
+
+    // ADAPT_INFO_NAME(e1000e->name, "send_AIT skb: %px", skb);
+    dump_ait_data(stm, "adapt_send - sendq_push", ait_data);
+
+    // FIXME : return q_space to caller ??
+    if (q_space < 0) {
+        // kfree(ait_data); // FIXME: check for memory leak?
+        return -1;
+    }
+
+    return 0;
+}
+
+// edf_retrieve_AIT
+static int adapt_retrieve_AIT(struct net_device *e1000e, entt_ioctl_ait_data_t *data) {
+    ADAPT_INFO_NAME(e1000e->name, "adapt_retrieve_AIT");
+    if (data == NULL) return -1;
+
+    struct e1000_adapter *adapter = netdev_priv(e1000e);
+    if (adapter == NULL) return -1;
+    entl_device_t *entl_dev = &adapter->entl_dev;
+    if (entl_dev == NULL) return -1;
+    entl_state_machine_t *stm = &entl_dev->edev_stm;
+    if (stm == NULL) return -1;
+
+    // send/retr differ: egrep 'typedef struct (ec_ait_data|entt_ioctl_ait_data)'
+    struct entt_ioctl_ait_data *ait_data = entl_read_AIT_message(stm); // recvq_pop
+    if (ait_data) {
+        // ADAPT_INFO_NAME(e1000e->name, "retr_AIT skb: %px", data);
+        dump_ait_data(stm, "adapt_retr - recvq_pop", ait_data);
+        memcpy(data, ait_data, sizeof(struct entt_ioctl_ait_data));
+        kfree(ait_data);
+    }
+    else {
+        struct entt_ioctl_ait_data dt; memset(&dt, 0, sizeof(struct entt_ioctl_ait_data));
+        // dt.data[0..MAX_AIT_MESSAGE_SIZE] = 0;
+        // dt.message_len = 0;
+        // dt.num_messages = 0;
+        dt.num_queued = entl_num_queued(stm);
+        memcpy(data, &dt, sizeof(struct entt_ioctl_ait_data));
+    }
+
+    return 0;
+}
+
+static int adapt_write_reg(struct net_device *e1000e, ec_alo_reg_t *reg) {
+    ADAPT_INFO_NAME(e1000e->name, "adapt_write_reg");
+    if (reg == NULL) return -1;
+
+#if 0
+#endif
+    return 0;
+}
+
+static int adapt_read_regset(struct net_device *e1000e, ec_alo_regs_t *regs) {
+    ADAPT_INFO_NAME(e1000e->name, "adapt_read_regset");
+    if (regs == NULL) return -1;
+
+#if 0
+#endif
+    return 0;
+}
+
+static int adapt_get_state(struct net_device *e1000e, ec_state_t *state) {
+    ADAPT_INFO_NAME(e1000e->name, "adapt_get_state");
+
+    if (state == NULL) return -1;
+
+    // ref: e1000e-3.3.4/src/e1000.h
+    struct e1000_adapter *adapter = netdev_priv(e1000e);
+    // ADAPT_INFO("  adapter: %p", adapter);
+    if (adapter == NULL) return -1;
+
+    entl_device_t *entl_dev = &adapter->entl_dev;
+    // ADAPT_INFO("  entl_dev: %p", entl_dev);
+    if (entl_dev == NULL) return -1;
+
+    ADAPT_INFO("  entl_dev->edev_name: \"%s\"", entl_dev->edev_name);
+    ADAPT_INFO("  entl_dev->edev_queue_stopped: %d", entl_dev->edev_queue_stopped);
+
+    char *nic_name = adapter->netdev->name;
+    int link_state = netif_carrier_ok(e1000e);
+    state->ecs_link_state = link_state; // FIXME: raw data for now
+    if (link_state) {
+        int link_speed = adapter->link_speed;
+        int link_duplex = adapter->link_duplex;
+        ADAPT_INFO_NAME(nic_name, "NIC Link is Up %d Mbps %s Duplex", link_speed, (link_duplex == FULL_DUPLEX) ? "Full" : "Half");
+    }
+    else {
+        ADAPT_INFO_NAME(nic_name, "NIC Link is Down");
+    }
+
+    entl_state_machine_t *stm = &entl_dev->edev_stm;
+    if (stm == NULL) return -1;
+
+#if 0
+    // ref: add_link_state
+    state->ecs_link_state = link_state;
+    state->ecs_s_count = s_count;
+    state->ecs_r_count = r_count;
+    state->ecs_recover_count = recover_count;
+    state->ecs_recovered_count = recovered_count;
+    state->ecs_entt_count = entt_count;
+    state->ecs_aop_count = aop_count;
+    state->ecs_num_queued = num_queued;
+    // FIXME: update_time, interval_time, max_interval_time, min_interval_time
+#endif
+    return 0;
+}
diff -ruN intel-e1000e-3.8.4/src/entl_device.h e1000e-3.8.4/src/entl_device.h
--- intel-e1000e-3.8.4/src/entl_device.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_device.h	2021-04-25 17:41:46.970475221 -0700
@@ -0,0 +1,37 @@
+#ifndef _ENTL_DEVICE_H_
+#define _ENTL_DEVICE_H_
+
+#define ENTL_DEVICE_FLAG_HELLO   0x0001
+#define ENTL_DEVICE_FLAG_SIGNAL  0x0002
+#define ENTL_DEVICE_FLAG_RETRY   0x0004
+#define ENTL_DEVICE_FLAG_WAITING 0x0008
+#define ENTL_DEVICE_FLAG_SIGNAL2 0x0010
+#define ENTL_DEVICE_FLAG_FATAL   0x8000
+
+#include "entl_skb_queue.h"
+#include "entl_state_machine.h"
+
+typedef struct entl_mgr {
+    void (*emf_event)(struct entl_mgr *self, int sigusr); // called from watchdog, be careful
+    void *emf_private;
+} entl_mgr_t;
+
+typedef struct entl_device {
+    int edev_action;
+    uint32_t edev_flag; // ENTL_DEVICE_FLAG
+    uint32_t edev_l_addr;
+    char edev_name[ENTL_DEVICE_NAME_LEN]; // 15
+    int edev_queue_stopped;
+    entl_state_machine_t edev_stm;
+    ENTL_skb_queue_t edev_tx_skb_queue;
+    uint16_t edev_u_addr;
+    int edev_user_pid;
+    struct timer_list edev_watchdog_timer;
+    struct work_struct edev_watchdog_task;
+    entl_mgr_t *edev_mgr;
+} entl_device_t;
+
+#include "entl_user_api.h"
+#include "netdev_entl_if.h"
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entl_ioctl.h e1000e-3.8.4/src/entl_ioctl.h
--- intel-e1000e-3.8.4/src/entl_ioctl.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_ioctl.h	2021-04-25 17:41:46.970475221 -0700
@@ -0,0 +1,41 @@
+#ifndef _ENTL_IOCTL_H_
+#define _ENTL_IOCTL_H_
+
+// IOCTL cmd values
+// ref: netdev.c
+// #define SIOCDEVPRIVATE_ENTL 0x89F0 /* to 89FF */
+#define SIOCDEVPRIVATE_ENTL_RD_CURRENT  0x89F0
+#define SIOCDEVPRIVATE_ENTL_RD_ERROR    0x89F1
+#define SIOCDEVPRIVATE_ENTL_SET_SIGRCVR 0x89F2
+#define SIOCDEVPRIVATE_ENTL_GEN_SIGNAL  0x89F3
+#define SIOCDEVPRIVATE_ENTL_DO_INIT     0x89F4
+#define SIOCDEVPRIVATE_ENTT_SEND_AIT    0x89F5
+#define SIOCDEVPRIVATE_ENTT_READ_AIT    0x89F6
+
+// IOCTL signal event
+// ref: ENTL_ACTION_PROC_AIT, entl_device_process_rx_packet, entl_new_AIT_message
+#define MAX_AIT_MESSAGE_SIZE 256
+
+// FIXME: unused num_messages, num_queued?
+typedef struct entt_ioctl_ait_data {
+    uint32_t num_messages;
+    uint32_t message_len;
+    char data[MAX_AIT_MESSAGE_SIZE];
+    uint32_t num_queued;
+} entt_ioctl_ait_data_t;
+
+#include "entl_state.h"
+
+// FIXME: entl_state_t
+typedef struct entl_ioctl_data {
+    int pid;
+    int link_state; // 0: down, 1: up
+    entl_state_t state;
+    entl_state_t error_state;
+    uint32_t icr;
+    uint32_t ctrl;
+    uint32_t ims;
+    uint32_t num_queued;
+} entl_ioctl_data_t;
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entl_skb_queue.h e1000e-3.8.4/src/entl_skb_queue.h
--- intel-e1000e-3.8.4/src/entl_skb_queue.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_skb_queue.h	2021-04-25 17:41:46.970475221 -0700
@@ -0,0 +1,40 @@
+#ifndef _ENTL_SKB_QUEUE_H_
+#define _ENTL_SKB_QUEUE_H_
+
+#define ENTL_DEFAULT_TXD 256
+typedef struct ENTL_skb_queue {
+    uint16_t size;
+    uint16_t count;
+    uint16_t head;
+    uint16_t tail;
+    struct sk_buff *data[ENTL_DEFAULT_TXD];
+} ENTL_skb_queue_t;
+
+static inline void ENTL_skb_queue_init(ENTL_skb_queue_t *q) {
+    q->size = ENTL_DEFAULT_TXD;
+    q->count = 0;
+    q->head = q->tail = 0;
+}
+
+static inline int ENTL_skb_queue_has_data(ENTL_skb_queue_t *q) { return q->count; }
+static inline int ENTL_skb_queue_unused(ENTL_skb_queue_t *q) { return q->size - q->count - 1; }
+static inline int ENTL_skb_queue_full(ENTL_skb_queue_t *q) { return (q->size == q->count) ? 1 : 0; }
+
+static inline struct sk_buff *ENTL_skb_queue_front(ENTL_skb_queue_t *q) { return (q->count == 0) ? NULL : q->data[q->head]; }
+static inline struct sk_buff *ENTL_skb_queue_front_pop(ENTL_skb_queue_t *q) {
+    if (q->count == 0) return NULL;
+    struct sk_buff *dt = q->data[q->head];
+    q->head = (q->head + 1) % q->size;
+    q->count--;
+    return dt;
+}
+
+static inline int ENTL_skb_queue_back_push(ENTL_skb_queue_t *q, struct sk_buff *dt) {
+    if (q->size == q->count) return -1;
+    q->data[q->tail] = dt;
+    q->tail = (q->tail+1) % q->size;
+    q->count++;
+    return q->size - q->count;
+}
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entl_state.h e1000e-3.8.4/src/entl_state.h
--- intel-e1000e-3.8.4/src/entl_state.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_state.h	2021-04-25 17:41:46.971474847 -0700
@@ -0,0 +1,21 @@
+#ifndef _ENTL_STATE_H_
+#define _ENTL_STATE_H_
+
+typedef struct entl_state {
+    uint32_t event_i_know;       // last event received
+    uint32_t event_i_sent;       // last event sent
+    uint32_t event_send_next;    // next event sent
+    uint32_t current_state;      // 0:idle 1:H 2:W 3:S 4:R
+    struct timespec update_time; // last updated (usec)
+    uint32_t error_flag;         // first error
+    uint32_t p_error_flag;       // when multiple, union of error bits
+    uint32_t error_count;        // multiple errors
+    struct timespec error_time;  // first error detected (usec)
+#ifdef ENTL_SPEED_CHECK
+    struct timespec interval_time; // duration between S <-> R transition
+    struct timespec max_interval_time;
+    struct timespec min_interval_time;
+#endif
+} entl_state_t;
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entl_state_machine.c e1000e-3.8.4/src/entl_state_machine.c
--- intel-e1000e-3.8.4/src/entl_state_machine.c	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_state_machine.c	2021-04-25 17:41:46.971474847 -0700
@@ -0,0 +1,898 @@
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+
+#include "entl_state_machine.h"
+#include "entt_queue.h"
+#include "entl_stm_if.h"
+#include "entl_user_api.h"
+
+// newline should be unnecessary here - https://lwn.net/Articles/732420/
+#define MCN_DEBUG(_name, _time, fmt, args...) printk(KERN_ALERT "%ld STM: %s " fmt "\n", _time, _name, ## args)
+#define STM_TDEBUG(fmt, args...) MCN_DEBUG(mcn->name, ts.tv_sec, fmt, ## args)
+#define STM_TDEBUG_ERROR(mcn, fmt, args...) STM_TDEBUG("error pending: flag %d (%s) count %d " fmt, mcn->error_state.error_flag, mcn_flag2name(mcn->error_state.error_flag), mcn->error_state.error_count, ## args)
+// FIXME: add STM_TDEBUG_STATE, STM_TDEBUG_TRANSITION
+
+#define STM_LOCK unsigned long flags; spin_lock_irqsave(&mcn->state_lock, flags)
+#define STM_UNLOCK spin_unlock_irqrestore(&mcn->state_lock, flags)
+#define OOPS_STM_UNLOCK spin_unlock(&mcn->state_lock)
+
+#define respond_with(hi, lo, action) { *emsg_raw = hi; *seqno = lo; ret_action = action; }
+
+static inline int cmp_addr(uint16_t l_high, uint32_t l_low, uint16_t r_high, uint32_t r_low) {
+    if (l_high > r_high) return 1;
+    if (l_high < r_high) return -1;
+    return l_low - r_low;
+}
+
+void entl_set_my_adder(entl_state_machine_t *mcn, uint16_t mac_hi, uint32_t mac_lo) {
+    struct timespec ts = current_kernel_time();
+    STM_TDEBUG("set-id - macaddr %04x %08x", mac_hi, mac_lo); // FIXME: mcn name not set up ??
+    STM_LOCK;
+        mcn->mac_hi = mac_hi;
+        mcn->mac_lo = mac_lo;
+        mcn->mac_valid = 1;
+        mcn->hello_valid = 0;
+    STM_UNLOCK;
+}
+
+// unused ??
+uint32_t get_entl_state(entl_state_machine_t *mcn) {
+    STM_LOCK;
+        uint16_t ret_state = (current_error_pending(mcn)) ? ENTL_STATE_ERROR : get_atomic_state(mcn);
+    STM_UNLOCK; // OOPS_STM_UNLOCK;
+    return ret_state;
+}
+
+// https://www.kernel.org/doc/html/latest/core-api/printk-formats.html
+static char *error_bits[] = {
+    "SEQUENCE",      // 0x0001 1 << 0
+    "LINKDONW",      // 0x0002 1 << 1
+    "TIMEOUT",       // 0x0004 1 << 2
+    "SAME_ADDRESS",  // 0x0008 1 << 3
+    "UNKOWN_CMD",    // 0x0010 1 << 4
+    "UNKOWN_STATE",  // 0x0020 1 << 5
+    "UNEXPECTED_LU", // 0x0040 1 << 6
+    "FATAL"          // 0x8000 1 << 15
+};
+
+static inline char *mcn_flag2name(uint32_t s) {
+    for (int i = 0; i < 7; i++) {
+        if (s == (1 << i)) return error_bits[i];
+    }
+    if (s == 0x8000) return error_bits[7];
+    return "??";
+}
+
+static char *mcn_names[] = {
+    "IDLE",     // 0
+    "HELLO",    // 1
+    "WAIT",     // 2
+    "SEND",     // 3
+    "RECEIVE",  // 4
+    "AM",       // 5
+    "BM",       // 6
+    "AH",       // 7
+    "BH",       // 8
+    "ERROR"     // 9
+};
+
+static inline char *mcn_state2name(uint32_t s) {
+    return (s < 10) ? mcn_names[s] : "??";
+}
+
+// FIXME: emsg_op(emsg_type)
+static inline char *msg_nick(int emsg_type) { return (emsg_type == ENTL_MESSAGE_EVENT_U) ? "EVENT" : (emsg_type == ENTL_MESSAGE_ACK_U) ? "ACK" : "??"; }
+
+#define seqno_error(mcn, _action) { set_error(mcn, ENTL_ERROR_FLAG_SEQUENCE); unicorn(mcn, ENTL_STATE_HELLO); set_update_time(mcn, ts); ret_action = _action;  }
+
+// behavior : get_atomic_state(mcn) X emsg_type
+int entl_received(entl_state_machine_t *mcn, uint16_t from_hi, uint32_t from_lo, uint16_t emsg_raw, uint32_t seqno) {
+    struct timespec ts = current_kernel_time();
+    uint16_t emsg_type = get_entl_msg(emsg_raw);
+
+    if (emsg_type == ENTL_MESSAGE_NOP_U) return ENTL_ACTION_NOP;
+
+    if (mcn->mac_valid == 0) {
+        STM_TDEBUG("invalid macaddr %04x %08x", mcn->mac_hi, mcn->mac_lo);
+        return ENTL_ACTION_NOP;
+    }
+
+    if (current_error_pending(mcn)) {
+        uint32_t was_state = get_atomic_state(mcn); // cheat - no locking
+        STM_TDEBUG_ERROR(mcn, "%s message %s (0x%04x) neighbor %04x %08x seqno %d", mcn_state2name(was_state), msg_nick(emsg_type), emsg_raw, from_hi, from_lo, seqno);
+        return ENTL_ACTION_SIG_ERR;
+    }
+
+    int ret_action = ENTL_ACTION_NOP;
+    STM_LOCK;
+        uint32_t was_state = get_atomic_state(mcn);
+        switch (was_state) {
+        case ENTL_STATE_IDLE: {
+            STM_TDEBUG("%s message %s (0x%04x) neighbor %04x %08x seqno %d", mcn_state2name(was_state), msg_nick(emsg_type), emsg_raw, from_hi, from_lo, seqno);
+        }
+        ret_action = ENTL_ACTION_NOP;
+        break;
+
+        case ENTL_STATE_HELLO: {
+            if (emsg_type == ENTL_MESSAGE_HELLO_U) {
+                // establish neighbor identity:
+                mcn->hello_hi = from_hi;
+                mcn->hello_lo = from_lo;
+                mcn->hello_valid = 1;
+
+                STM_TDEBUG("%04x %08x greeting - neighbor %04x %08x", mcn->mac_hi, mcn->mac_lo, from_hi, from_lo);
+
+                // symmetry breaking : master / slave
+                int ordering = cmp_addr(mcn->mac_hi, mcn->mac_lo, from_hi, from_lo);
+                if (ordering > 0) {
+                // if ((mcn->mac_hi > from_hi) ||  ((mcn->mac_hi == from_hi) && (mcn->mac_lo > from_lo))) { // }
+                    STM_TDEBUG("%s (master) -> WAIT", mcn_state2name(was_state));
+                    unicorn(mcn, ENTL_STATE_WAIT); set_update_time(mcn, ts);
+                    clear_intervals(mcn);
+                    mcn->state_count = 0;
+                    ret_action = ENTL_ACTION_SEND;
+                }
+                else if (ordering == 0) {
+                // else if ((mcn->mac_hi == from_hi) && (mcn->mac_lo == from_lo)) { // }
+                    // say error as Alan's 1990s problem again
+                    STM_TDEBUG("%s -> IDLE - Fatal Error: SAME ADDRESS", mcn_state2name(was_state));
+                    set_error(mcn, ENTL_ERROR_SAME_ADDRESS);
+                    set_atomic_state(mcn, ENTL_STATE_IDLE);
+                    set_update_time(mcn, ts);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    STM_TDEBUG("%s (slave)", mcn_state2name(was_state));
+                    ret_action = ENTL_ACTION_NOP;
+                }
+            }
+            else if (emsg_type == ENTL_MESSAGE_EVENT_U) {
+                if (seqno != 0) {
+                    STM_TDEBUG("%s EVENT(in): Out of Sequence - seqno %d", mcn_state2name(was_state), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    STM_TDEBUG("%s (slave) -> SEND EVENT: advance - seqno %d", mcn_state2name(was_state), seqno);
+                    set_i_know(mcn, seqno); set_send_next(mcn, seqno + 1);
+                    set_atomic_state(mcn, ENTL_STATE_SEND);
+                    calc_intervals(mcn);
+                    set_update_time(mcn, ts);
+                    ret_action = ENTL_ACTION_SEND;
+                }
+            }
+            else {
+                STM_TDEBUG("%s WTF? message %s (0x%04x) neighbor %04x %08x seqno %d (0x%08x)", mcn_state2name(was_state), msg_nick(emsg_type), emsg_raw, from_hi, from_lo, seqno, seqno);
+                // FIXME: dump whole packet here?
+                ret_action = ENTL_ACTION_NOP;
+            }
+        }
+        break;
+
+        case ENTL_STATE_WAIT: {
+            if (emsg_type == ENTL_MESSAGE_HELLO_U) {
+                mcn->state_count++; // private to this logic
+                if (mcn->state_count > ENTL_COUNT_MAX) {
+                    STM_TDEBUG("%s -> HELLO - overflow %d", mcn_state2name(was_state), mcn->state_count);
+                    unicorn(mcn, ENTL_STATE_HELLO); set_update_time(mcn, ts);
+                }
+                ret_action = ENTL_ACTION_NOP;
+            }
+            else if (emsg_type == ENTL_MESSAGE_EVENT_U) {
+                // hmm, this should be exactly 1, not sent+1
+                if (seqno == get_i_sent(mcn) + 1) {
+                    STM_TDEBUG("%s (master) -> SEND EVENT(in): advance - seqno %d", mcn_state2name(was_state), seqno);
+                    set_i_know(mcn, seqno); set_send_next(mcn, seqno + 1);
+                    set_atomic_state(mcn, ENTL_STATE_SEND);
+                    set_update_time(mcn, ts);
+                    clear_intervals(mcn);
+                    ret_action = ENTL_ACTION_SEND;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO EVENT(in): wrong seqno %d", mcn_state2name(was_state), seqno);
+                    unicorn(mcn, ENTL_STATE_HELLO); set_update_time(mcn, ts);
+                    clear_intervals(mcn);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+            }
+            else {
+                // Received non hello message on Wait state
+                STM_TDEBUG("%s -> HELLO wrong message 0x%04x", mcn_state2name(was_state), emsg_raw);
+                seqno_error(mcn, ENTL_ACTION_NOP);
+            }
+        }
+        break;
+
+// FIXME: study this ??
+        case ENTL_STATE_SEND: {
+            if (emsg_type == ENTL_MESSAGE_EVENT_U
+            ||  emsg_type == ENTL_MESSAGE_ACK_U) {
+                if (seqno == get_i_know(mcn)) {
+                    STM_TDEBUG("%s(in) same seqno %d, SEND", msg_nick(emsg_type), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO %s(in): Out of Sequence - seqno %d", mcn_state2name(was_state), msg_nick(emsg_type), seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else {
+                STM_TDEBUG("%s -> HELLO wrong message 0x%04x", mcn_state2name(was_state), emsg_raw);
+                seqno_error(mcn, ENTL_ACTION_ERROR);
+            }
+        }
+        break;
+
+        case ENTL_STATE_RECEIVE: {
+            if (emsg_type == ENTL_MESSAGE_EVENT_U) {
+                if (get_i_know(mcn) + 2 == seqno) {
+                    set_i_know(mcn, seqno); set_send_next(mcn, seqno + 1);
+                    set_atomic_state(mcn, ENTL_STATE_SEND);
+                    ret_action = ENTL_ACTION_SEND;
+
+                    int pending = sendq_count(mcn);
+                    // int nfree = sendq_space(mcn);
+                    // int avail = recvq_space(mcn);
+                    // int delivered = recvq_count(mcn);
+
+                    // send queue empty
+                    if (pending == 0) { // AIT has priority
+                        // way too noisy to log!
+                        // STM_TDEBUG("%s -> SEND (data) EVENT(in): advance - seqno %d", mcn_state2name(was_state), seqno);
+                        ret_action |= ENTL_ACTION_SEND_DAT; // data send as optional
+                    }
+                    set_update_time(mcn, ts);
+                }
+                else if (get_i_know(mcn) == seqno) {
+                    STM_TDEBUG("%s EVENT(in): unchanged - seqno %d", mcn_state2name(was_state), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO EVENT(in): Out of Sequence - seqno %d", mcn_state2name(was_state), seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else if (emsg_type == ENTL_MESSAGE_AIT_U) {
+                if (get_i_know(mcn) + 2 == seqno) {
+                    set_i_know(mcn, seqno); set_send_next(mcn, seqno + 1);
+                    set_atomic_state(mcn, ENTL_STATE_AH);
+                    ret_action = ENTL_ACTION_PROC_AIT;
+
+                    // int pending = sendq_count(mcn);
+                    // int nfree = sendq_space(mcn);
+                    int avail = recvq_space(mcn);
+                    int delivered = recvq_count(mcn);
+                    // recv queue space avail
+                    if (avail > 0) {
+                        STM_TDEBUG("%s -> AH (delivered %d avail %d) AIT(in): advance - seqno %d", mcn_state2name(was_state), delivered, avail, seqno);
+                        ret_action |= ENTL_ACTION_SEND;
+                    }
+                    else {
+                        STM_TDEBUG("%s -> AH (hold) AIT(in): queue full - seqno %d", mcn_state2name(was_state), seqno);
+                    }
+                    set_update_time(mcn, ts);
+                }
+                else if (get_i_know(mcn) == seqno) {
+                    STM_TDEBUG("%s AIT(in): unchanged - seqno %d", mcn_state2name(was_state), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO AIT(in): Out of Sequence - seqno %d", mcn_state2name(was_state), seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else {
+                STM_TDEBUG("%s -> HELLO wrong message 0x%04x", mcn_state2name(was_state), emsg_raw);
+                seqno_error(mcn, ENTL_ACTION_ERROR);
+            }
+        }
+        break;
+
+        // AIT message sent, waiting for ack
+        case ENTL_STATE_AM: {
+            if (emsg_type == ENTL_MESSAGE_ACK_U) {
+                if (get_i_know(mcn) + 2 == seqno) {
+                    STM_TDEBUG("%s -> BM ACK(in): advance - seqno %d", mcn_state2name(was_state), seqno);
+                    set_i_know(mcn, seqno); set_send_next(mcn, seqno + 1);
+                    set_atomic_state(mcn, ENTL_STATE_BM);
+                    set_update_time(mcn, ts);
+                    ret_action = ENTL_ACTION_SEND;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO ACK(in): Out of Sequence - seqno %d", mcn_state2name(was_state), seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else if (emsg_type == ENTL_MESSAGE_EVENT_U) {
+                if (get_i_know(mcn) == seqno) {
+                    STM_TDEBUG("%s EVENT(in): unchanged - seqno %d", mcn_state2name(was_state), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    // FIXME: is this just a 'normal' Out of Sequence ??
+                    STM_TDEBUG("%s -> HELLO EVENT(in): wrong message 0x%04x - seqno %d", mcn_state2name(was_state), emsg_raw, seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else {
+                STM_TDEBUG("%s -> HELLO wrong message 0x%04x", mcn_state2name(was_state), emsg_raw);
+                seqno_error(mcn, ENTL_ACTION_ERROR);
+            }
+        }
+        break;
+
+        // AIT sent, Ack received, sending Ack
+        case ENTL_STATE_BM: {
+            if (emsg_type == ENTL_MESSAGE_ACK_U) {
+                if (get_i_know(mcn) == seqno) {
+                    STM_TDEBUG("%s ACK(in): unchanged - seqno %d", mcn_state2name(was_state), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    // FIXME: is this just a 'normal' Out of Sequence ??
+                    STM_TDEBUG("%s -> HELLO ACK(in): wrong message 0x%04x - seqno %d", mcn_state2name(was_state), emsg_raw, seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else {
+                STM_TDEBUG("%s -> HELLO wrong message 0x%04x", mcn_state2name(was_state), emsg_raw);
+                seqno_error(mcn, ENTL_ACTION_ERROR);
+            }
+        }
+        break;
+
+        // AIT message received, sending Ack
+        case ENTL_STATE_AH: {
+            if (emsg_type == ENTL_MESSAGE_AIT_U) {
+                if (get_i_know(mcn) == seqno) {
+                    STM_TDEBUG("%s AIT(in): unchanged - seqno %d", mcn_state2name(was_state), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO AIT(in): Out of Sequence - seqno %d", mcn_state2name(was_state), seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else {
+                STM_TDEBUG("%s -> HELLO wrong message 0x%04x", mcn_state2name(was_state), emsg_raw);
+                seqno_error(mcn, ENTL_ACTION_ERROR);
+            }
+        }
+        break;
+// bj wrong message xx(emsg_type)
+// bj move emsg_type out of fmt
+
+        // got AIT, Ack sent, waiting for ack
+        case ENTL_STATE_BH: {
+            if (emsg_type == ENTL_MESSAGE_ACK_U) {
+                if (get_i_know(mcn) + 2 == seqno) {
+                    STM_TDEBUG("%s -> SEND ACK(in): advance - seqno %d", mcn_state2name(was_state), seqno);
+                    set_i_know(mcn, seqno); set_send_next(mcn, seqno + 1);
+                    set_atomic_state(mcn, ENTL_STATE_SEND);
+                    set_update_time(mcn, ts);
+                    {
+                        struct entt_ioctl_ait_data *dt = mcn->receive_buffer;
+                        dump_ait_data(mcn, "stm - recvq_push", dt);
+                    }
+
+                    // int pending = sendq_count(mcn);
+                    // int nfree = sendq_space(mcn);
+                    int avail = recvq_space(mcn);
+                    int delivered = recvq_count(mcn);
+
+                    // FIXME: what about when q is full?
+                    // add to recvq
+                    int recv_space = recvq_push(mcn);
+                    STM_TDEBUG("recvq_push - delivered %d avail: before %d after %d", delivered, avail, recv_space);
+                    mcn->receive_buffer = NULL;
+                    ret_action = ENTL_ACTION_SEND | ENTL_ACTION_SIG_AIT;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO ACK(in): Out of Sequence - seqno %d", mcn_state2name(was_state), seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else if (emsg_type == ENTL_MESSAGE_AIT_U) {
+                if (get_i_know(mcn) == seqno) {
+                    STM_TDEBUG("%s AIT(in): unchanged - seqno %d", mcn_state2name(was_state), seqno);
+                    ret_action = ENTL_ACTION_NOP;
+                }
+                else {
+                    STM_TDEBUG("%s -> HELLO AIT(in): Out of Sequence - seqno %d", mcn_state2name(was_state), seqno);
+                    seqno_error(mcn, ENTL_ACTION_ERROR);
+                }
+            }
+            else {
+                STM_TDEBUG("%s -> HELLO wrong message 0x%04x", mcn_state2name(was_state), emsg_raw);
+                seqno_error(mcn, ENTL_ACTION_ERROR);
+            }
+        }
+        break;
+
+        default: {
+            STM_TDEBUG("%s -> IDLE wrong state", mcn_state2name(was_state));
+            set_error(mcn, ENTL_ERROR_UNKOWN_STATE);
+            unicorn(mcn, ENTL_STATE_IDLE); set_update_time(mcn, ts);
+        }
+        ret_action = ENTL_ACTION_NOP;
+        break;
+    }
+    STM_UNLOCK;
+    return ret_action;
+}
+
+int entl_get_hello(entl_state_machine_t *mcn, uint16_t *emsg_raw, uint32_t *seqno) {
+    struct timespec ts = current_kernel_time();
+
+    if (current_error_pending(mcn)) {
+        STM_TDEBUG_ERROR(mcn, "entl_get_hello");
+        return ENTL_ACTION_NOP;
+    }
+
+    int ret_action = ENTL_ACTION_NOP;
+    STM_LOCK;
+        uint32_t was_state = get_atomic_state(mcn);
+        switch (was_state) {
+        case ENTL_STATE_HELLO:
+            respond_with(ENTL_MESSAGE_HELLO_U, ENTL_MESSAGE_HELLO_L, ENTL_ACTION_SEND);
+            // STM_TDEBUG("%s HELLO(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_WAIT:
+            respond_with(ENTL_MESSAGE_EVENT_U, 0, ENTL_ACTION_SEND);
+            // STM_TDEBUG("%s EVENT(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+// bj - should we display : get_i_sent(mcn), i.e. seqno
+        case ENTL_STATE_RECEIVE:
+            respond_with(ENTL_MESSAGE_EVENT_U, get_i_sent(mcn), ENTL_ACTION_SEND);
+            STM_TDEBUG("%s EVENT(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_AM:
+            respond_with(ENTL_MESSAGE_AIT_U, get_i_sent(mcn), ENTL_ACTION_SEND | ENTL_ACTION_SEND_AIT);
+            STM_TDEBUG("%s AIT(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_BH: {
+            // int pending = sendq_count(mcn);
+            // int nfree = sendq_space(mcn);
+            int avail = recvq_space(mcn);
+            int delivered = recvq_count(mcn);
+            // recv queue space avail
+            if (avail > 0) {
+                respond_with(ENTL_MESSAGE_ACK_U, get_i_sent(mcn), ENTL_ACTION_SEND);
+                STM_TDEBUG("%s ACK(out) - delivered %d avail %d seqno %d", mcn_state2name(was_state), delivered, avail, *seqno);
+            }
+            else {
+                ret_action = ENTL_ACTION_NOP;
+            }
+        }
+        break;
+
+// FIXME: what state?
+        default:
+            ret_action = ENTL_ACTION_NOP;
+        break;
+        }
+    STM_UNLOCK;
+    return ret_action;
+}
+
+int entl_next_send(entl_state_machine_t *mcn, uint16_t *emsg_raw, uint32_t *seqno) {
+    struct timespec ts = current_kernel_time();
+
+    if (current_error_pending(mcn)) {
+        int ret_action;
+        uint32_t was_state = get_atomic_state(mcn);
+        respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+        STM_TDEBUG_ERROR(mcn, "%s entl_next_send", mcn_state2name(was_state));
+        return ret_action;
+    }
+
+    int ret_action = ENTL_ACTION_NOP;
+    STM_LOCK;
+        uint32_t was_state = get_atomic_state(mcn);
+        switch (was_state) {
+        case ENTL_STATE_IDLE:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_HELLO:
+            respond_with(ENTL_MESSAGE_HELLO_U, ENTL_MESSAGE_HELLO_L, ENTL_ACTION_SEND);
+            STM_TDEBUG("%s HELLO(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_WAIT:
+            respond_with(ENTL_MESSAGE_EVENT_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s EVENT(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_SEND: {
+            uint32_t event_i_know = get_i_know(mcn); // last received event number
+            uint32_t event_i_sent = get_i_sent(mcn);
+            zebra(mcn); advance_send_next(mcn);
+            calc_intervals(mcn);
+            set_update_time(mcn, ts);
+
+            int pending; // = sendq_count(mcn);
+            int nfree = sendq_space(mcn);
+            // int avail = recvq_space(mcn);
+            // int delivered = recvq_count(mcn);
+            // Avoid sending AIT on first exchange, neighbor will be in Hello state
+            // send queue non-empty
+            if (event_i_know && event_i_sent && (pending = sendq_count(mcn))) {
+                set_atomic_state(mcn, ENTL_STATE_AM);
+                respond_with(ENTL_MESSAGE_AIT_U, get_i_sent(mcn), ENTL_ACTION_SEND | ENTL_ACTION_SEND_AIT);
+                STM_TDEBUG("%s -> AM AIT(out) - pending %d nfree %d seqno %d", mcn_state2name(was_state), pending, nfree, *seqno);
+            }
+            else {
+                set_atomic_state(mcn, ENTL_STATE_RECEIVE);
+                respond_with(ENTL_MESSAGE_EVENT_U, get_i_sent(mcn), ENTL_ACTION_SEND | ENTL_ACTION_SEND_DAT); // data send as optional
+                // STM_TDEBUG("%s -> RECEIVE EVENT(out) - seqno %d", mcn_state2name(was_state), *seqno);
+            }
+        }
+        break;
+
+        case ENTL_STATE_RECEIVE:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        // AIT
+        case ENTL_STATE_AM:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_BM: {
+            zebra(mcn); advance_send_next(mcn);
+            respond_with(ENTL_MESSAGE_ACK_U, get_i_sent(mcn), ENTL_ACTION_SEND | ENTL_ACTION_SIG_AIT);
+            STM_TDEBUG("%s -> RECEIVE ACK(out) - seqno %d", mcn_state2name(was_state), *seqno);
+            set_atomic_state(mcn, ENTL_STATE_RECEIVE);
+            calc_intervals(mcn);
+            set_update_time(mcn, ts);
+
+            // discard off send queue
+            struct entt_ioctl_ait_data *ait_data = sendq_pop(mcn);
+            if (ait_data) {
+                // int pending = sendq_count(mcn);
+                // int nfree = sendq_space(mcn);
+                // int avail = recvq_space(mcn);
+                // int delivered = recvq_count(mcn);
+                int delivered = ait_data->num_messages;
+                int pending = ait_data->num_queued;
+                STM_TDEBUG("sendq_pop - pending %d", pending); //  recvq delivered %d", delivered);
+                kfree(ait_data);
+            }
+            else {
+                STM_TDEBUG("sendq_pop - empty");
+            }
+        }
+        break;
+
+        case ENTL_STATE_AH: {
+            // int pending = sendq_count(mcn);
+            // int nfree = sendq_space(mcn);
+            int avail = recvq_space(mcn);
+            int delivered = recvq_count(mcn);
+            // recv queue space avail
+            if (avail > 0) {
+                zebra(mcn); advance_send_next(mcn);
+                respond_with(ENTL_MESSAGE_ACK_U, get_i_sent(mcn), ENTL_ACTION_SEND);
+                STM_TDEBUG("%s -> BH ACK(out) - delivered %d avail %d seqno %d", mcn_state2name(was_state), delivered, avail, *seqno);
+                set_atomic_state(mcn, ENTL_STATE_BH);
+                calc_intervals(mcn);
+                set_update_time(mcn, ts);
+            }
+            else {
+                respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+                // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+            }
+        }
+        break;
+
+        case ENTL_STATE_BH:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+// FIXME: what state?
+        default:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+        }
+    STM_UNLOCK;
+    return ret_action;
+}
+
+// For TX, it can't send AIT, so just keep ENTL state on Send state
+int entl_next_send_tx(entl_state_machine_t *mcn, uint16_t *emsg_raw, uint32_t *seqno) {
+    struct timespec ts = current_kernel_time();
+
+    // might be offline(no carrier), or be newly online after offline ??
+    if (current_error_pending(mcn)) {
+        int ret_action;
+        uint32_t was_state = get_atomic_state(mcn);
+        respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+        STM_TDEBUG_ERROR(mcn, "%s entl_next_send_tx", mcn_state2name(was_state));
+        return ret_action;
+    }
+
+    int ret_action = ENTL_ACTION_NOP;
+    STM_LOCK;
+        uint32_t was_state = get_atomic_state(mcn);
+        switch (was_state) {
+        case ENTL_STATE_IDLE:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_HELLO:
+            respond_with(ENTL_MESSAGE_HELLO_U, ENTL_MESSAGE_HELLO_L, ENTL_ACTION_SEND);
+            // STM_TDEBUG("%s HELLO(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_WAIT:
+            respond_with(ENTL_MESSAGE_EVENT_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s EVENT(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_SEND: {
+            zebra(mcn); advance_send_next(mcn);
+            calc_intervals(mcn);
+            set_update_time(mcn, ts);
+            set_atomic_state(mcn, ENTL_STATE_RECEIVE);
+            respond_with(ENTL_MESSAGE_EVENT_U, get_i_sent(mcn), ENTL_ACTION_SEND);
+            // STM_TDEBUG("%s EVENT(out) - seqno %d", mcn_state2name(was_state), *seqno);
+            // For TX, it can't send AIT, so just keep ENTL state on Send state
+        }
+        break;
+
+        case ENTL_STATE_RECEIVE:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        // AIT
+        case ENTL_STATE_AM:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+        case ENTL_STATE_BM: {
+            zebra(mcn); advance_send_next(mcn);
+            respond_with(ENTL_MESSAGE_ACK_U, get_i_sent(mcn), ENTL_ACTION_SEND | ENTL_ACTION_SIG_AIT);
+            STM_TDEBUG("%s -> RECEIVE ACK(out) - seqno %d", mcn_state2name(was_state), *seqno);
+            set_atomic_state(mcn, ENTL_STATE_RECEIVE);
+            calc_intervals(mcn);
+            set_update_time(mcn, ts);
+
+            // discard off send queue
+            struct entt_ioctl_ait_data *ait_data = sendq_pop(mcn);
+            if (ait_data) {
+                // int pending = sendq_count(mcn);
+                // int nfree = sendq_space(mcn);
+                // int avail = recvq_space(mcn);
+                // int delivered = recvq_count(mcn);
+                int pending = ait_data->num_queued;
+                int delivered = ait_data->num_messages;
+                STM_TDEBUG("sendq_pop - pending %d", pending); //  recvq delivered %d", delivered);
+                // FIXME: memory leak?
+                // kfree(ait_data);
+            }
+            else {
+                STM_TDEBUG("sendq_pop - empty");
+            }
+        }
+        break;
+
+        case ENTL_STATE_AH: {
+            // int pending = sendq_count(mcn);
+            // int nfree = sendq_space(mcn);
+            int avail = recvq_space(mcn);
+            int delivered = recvq_count(mcn);
+            // recv queue space avail
+            if (avail > 0) {
+                zebra(mcn); advance_send_next(mcn);
+                respond_with(ENTL_MESSAGE_ACK_U, get_i_sent(mcn), ENTL_ACTION_SEND);
+                STM_TDEBUG("%s -> BH ACK(out) - delivered %d avail %d seqno %d", mcn_state2name(was_state), delivered, avail, *seqno);
+                set_atomic_state(mcn, ENTL_STATE_BH);
+                calc_intervals(mcn);
+                set_update_time(mcn, ts);
+            }
+            else {
+                respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+                // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+            }
+        }
+        break;
+
+        case ENTL_STATE_BH:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+
+// FIXME: what state?
+        default:
+            respond_with(ENTL_MESSAGE_NOP_U, 0, ENTL_ACTION_NOP);
+            // STM_TDEBUG("%s NOP(out) - seqno %d", mcn_state2name(was_state), *seqno);
+        break;
+        }
+    STM_UNLOCK;
+    return ret_action;
+}
+
+void entl_state_error(entl_state_machine_t *mcn, uint32_t error_flag) {
+    struct timespec ts = current_kernel_time();
+
+    uint32_t was_state = get_atomic_state(mcn);
+
+    STM_LOCK;
+        set_error(mcn, error_flag);
+        if (error_flag == ENTL_ERROR_FLAG_SEQUENCE) {
+            // FIXME : seems redundant ?
+            unicorn(mcn, ENTL_STATE_HELLO); set_update_time(mcn, ts);
+            clear_error(mcn);
+            clear_intervals(mcn);
+        }
+        // FIXME: what about other values for error_flag ??
+        uint32_t now = get_atomic_state(mcn);
+    STM_UNLOCK;
+    STM_TDEBUG("%s -> %s entl_state_error - flag %s (%d)", mcn_state2name(was_state), mcn_state2name(now), mcn_flag2name(error_flag), error_flag);
+}
+
+void entl_read_current_state(entl_state_machine_t *mcn, entl_state_t *st, entl_state_t *err) {
+    struct timespec ts = current_kernel_time();
+    STM_LOCK;
+        memcpy(st, &mcn->current_state, sizeof(entl_state_t));
+        memcpy(err, &mcn->error_state, sizeof(entl_state_t));
+    STM_UNLOCK;
+}
+
+void entl_clear_error_state(entl_state_machine_t *mcn, entl_state_t *st, entl_state_t *err) {
+    struct timespec ts = current_kernel_time();
+    STM_LOCK;
+        memset(&mcn->error_state, 0, sizeof(entl_state_t));
+    STM_UNLOCK;
+    uint32_t was_state = st->current_state;
+    uint32_t count = err->error_count;
+    uint32_t error_flag = err->error_flag;
+    uint32_t mask = err->p_error_flag;
+    STM_TDEBUG("state %s (%d) set error_state -"
+        " flag %s (0x%04x)"
+        " count %d"
+        " mask 0x%04x",
+        mcn_state2name(was_state), was_state,
+        mcn_flag2name(error_flag), error_flag,
+        count,
+        mask
+    );
+}
+
+void entl_read_error_state(entl_state_machine_t *mcn, entl_state_t *st, entl_state_t *err) {
+    struct timespec ts = current_kernel_time();
+    STM_LOCK;
+        memcpy(st, &mcn->current_state, sizeof(entl_state_t));
+        memcpy(err, &mcn->error_state, sizeof(entl_state_t));
+    STM_UNLOCK;
+    uint32_t was_state = st->current_state;
+    uint32_t count = err->error_count;
+    uint32_t error_flag = err->error_flag;
+    uint32_t mask = err->p_error_flag;
+    STM_TDEBUG("state %s (%d) read error_state -"
+        " flag %s (0x%04x)"
+        " count %d"
+        " mask 0x%04x",
+        mcn_state2name(was_state), was_state,
+        mcn_flag2name(error_flag), error_flag,
+        count,
+        mask
+    );
+}
+
+void entl_link_up(entl_state_machine_t *mcn) {
+    struct timespec ts = current_kernel_time();
+    STM_LOCK;
+        uint32_t was_state = get_atomic_state(mcn);
+        if (was_state != ENTL_STATE_IDLE) {
+            STM_TDEBUG("%s - Link Up unexpected, ignored", mcn_state2name(was_state));
+        }
+        else if (current_error_pending(mcn)) {
+            // FIXME: is error 'LINKDONW' ??
+            STM_TDEBUG_ERROR(mcn, "%s - Link Up, error lock", mcn_state2name(was_state));
+        }
+        else {
+            STM_TDEBUG("%s -> HELLO - Link Up", mcn_state2name(was_state));
+            unicorn(mcn, ENTL_STATE_HELLO); set_update_time(mcn, ts);
+            clear_error(mcn);
+            clear_intervals(mcn);
+        }
+    STM_UNLOCK;
+}
+
+// AIT handling functions
+// add AIT message to send queue, return 0 when OK, -1 when queue full
+int entl_send_AIT_message(entl_state_machine_t *mcn, struct entt_ioctl_ait_data *data) {
+    struct timespec ts = current_kernel_time();
+    STM_LOCK;
+        int nfree = sendq_push(mcn, (void *) data); // sendq_space, after
+    STM_UNLOCK;
+    int pending = sendq_count(mcn);
+    // int nfree = sendq_space(mcn);
+    int avail = recvq_space(mcn);
+    // int delivered = recvq_count(mcn);
+    STM_TDEBUG("sendq_push - pending %d nfree %d", pending, nfree);
+    return nfree;
+}
+
+// peek at next AIT message to xmit
+struct entt_ioctl_ait_data *entl_next_AIT_message(entl_state_machine_t *mcn) {
+    struct timespec ts = current_kernel_time();
+    STM_LOCK;
+        struct entt_ioctl_ait_data *dt = (struct entt_ioctl_ait_data *) sendq_peek(mcn);
+    STM_UNLOCK;
+    // FIXME: access not under lock:
+    int pending = sendq_count(mcn);
+    // int nfree = sendq_space(mcn);
+    // int avail = recvq_space(mcn);
+    int delivered = recvq_count(mcn);
+    if (dt) {
+        dt->num_messages = delivered;
+        dt->num_queued = pending;
+        STM_TDEBUG("sendq_peek - pending %d recvq delivered %d", pending, delivered);
+    }
+    else {
+        STM_TDEBUG("sendq_peek - empty, recvq delivered %d", delivered);
+    }
+    return dt;
+}
+
+// new AIT message received
+void entl_new_AIT_message(entl_state_machine_t *mcn, struct entt_ioctl_ait_data *data) {
+    STM_LOCK;
+        mcn->receive_buffer = data;
+    STM_UNLOCK;
+}
+
+// Read (consume) AIT message, return NULL when queue empty
+struct entt_ioctl_ait_data *entl_read_AIT_message(entl_state_machine_t *mcn) {
+    struct timespec ts = current_kernel_time();
+    STM_LOCK;
+        struct entt_ioctl_ait_data *dt = recvq_pop(mcn);
+    STM_UNLOCK;
+    int pending = sendq_count(mcn);
+    // int nfree = sendq_space(mcn);
+    // int avail = recvq_space(mcn);
+    int delivered = recvq_count(mcn);
+    if (dt) {
+        // FIXME: access not under lock:
+        dt->num_messages = delivered;
+        dt->num_queued = pending;
+        STM_TDEBUG("recvq_pop - delivered %d sendq pending %d", delivered, pending);
+    }
+    else {
+        // FIXME: should allocate and return an 'empty' dt w/counts
+        STM_TDEBUG("recvq_pop - empty, sendq pending %d", pending);
+    }
+    return dt;
+}
+
+// number of pending xmits
+uint16_t entl_num_queued(entl_state_machine_t *mcn) {
+    // struct timespec ts = current_kernel_time();
+    STM_LOCK;
+    uint16_t pending = sendq_count(mcn);
+    // int nfree = sendq_space(mcn);
+    // int avail = recvq_space(mcn);
+    // int delivered = recvq_count(mcn);
+    STM_UNLOCK;
+    // don't log because only used for info, not logic
+    // STM_TDEBUG("sendq_count %d", pending);
+    return pending;
+}
+
+// eof
diff -ruN intel-e1000e-3.8.4/src/entl_state_machine.h e1000e-3.8.4/src/entl_state_machine.h
--- intel-e1000e-3.8.4/src/entl_state_machine.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_state_machine.h	2021-04-25 17:41:46.971474847 -0700
@@ -0,0 +1,189 @@
+#ifndef _ENTL_STATE_MACHINE_H_
+#define _ENTL_STATE_MACHINE_H_
+
+#define ENTL_ACTION_NOP      0x00
+#define ENTL_ACTION_SEND     0x01
+#define ENTL_ACTION_SEND_AIT 0x02
+#define ENTL_ACTION_PROC_AIT 0x04
+#define ENTL_ACTION_SIG_AIT  0x08
+#define ENTL_ACTION_SEND_DAT 0x10
+#define ENTL_ACTION_SIG_ERR  0x20
+#define ENTL_ACTION_ERROR    -1
+
+#define ENTL_MESSAGE_HELLO_U 0x0000
+#define ENTL_MESSAGE_HELLO_L 0x00000000
+#define ENTL_MESSAGE_EVENT_U 0x0001
+#define ENTL_MESSAGE_NOP_U   0x0002
+#define ENTL_MESSAGE_AIT_U   0x0003
+#define ENTL_MESSAGE_ACK_U   0x0004
+#define ENTL_MESSAGE_MASK    0x00ff
+#define ENTL_MESSAGE_ONLY_U  0x8000
+#define ENTL_TEST_MASK       0x7f00
+
+static inline int get_entl_msg(uint16_t u_daddr) { return u_daddr & ENTL_MESSAGE_MASK; }
+
+#define ENTL_STATE_IDLE     0
+#define ENTL_STATE_HELLO    1
+#define ENTL_STATE_WAIT     2
+#define ENTL_STATE_SEND     3
+#define ENTL_STATE_RECEIVE  4
+#define ENTL_STATE_AM       5
+#define ENTL_STATE_BM       6
+#define ENTL_STATE_AH       7
+#define ENTL_STATE_BH       8
+#define ENTL_STATE_ERROR    9
+
+// uint32_t entl_state = FETCH_STATE(p);
+
+#include "entt_queue.h"
+#include "entl_state.h"
+
+#define ENTL_COUNT_MAX 10
+#define ENTL_DEVICE_NAME_LEN 15
+typedef struct entl_state_machine {
+    spinlock_t state_lock;
+    uint32_t state_count;
+    entl_state_t current_state;
+    entl_state_t error_state;
+    entl_state_t return_state; // unused
+    int user_pid;
+    struct entt_ioctl_ait_data *receive_buffer;
+    ENTT_queue_t send_ATI_queue;
+    ENTT_queue_t receive_ATI_queue;
+    char name[ENTL_DEVICE_NAME_LEN];
+
+    uint16_t mac_hi; // MAC addr for Hello message
+    uint32_t mac_lo; // MAC addr for Hello message
+    uint8_t mac_valid;
+    uint16_t hello_hi;
+    uint32_t hello_lo;
+    uint8_t hello_valid;
+} entl_state_machine_t;
+
+// FIXME: fields should have prefix (i.e. esm_)
+static inline char *get_esm_name(entl_state_machine_t *mcn) { return mcn->name; }
+
+static inline void set_update_time(entl_state_machine_t *mcn, struct timespec ts) { memcpy(&mcn->current_state.update_time, &ts, sizeof(struct timespec)); }
+static inline int get_atomic_state(entl_state_machine_t *mcn) { return mcn->current_state.current_state; }
+static inline void set_atomic_state(entl_state_machine_t *mcn, int value) { mcn->current_state.current_state = value; }
+static inline int get_i_know(entl_state_machine_t *mcn) { return mcn->current_state.event_i_know; }
+static inline void set_i_know(entl_state_machine_t *mcn, int value) { mcn->current_state.event_i_know = value; }
+static inline int get_send_next(entl_state_machine_t *mcn) { return mcn->current_state.event_send_next; }
+static inline void set_send_next(entl_state_machine_t *mcn, int value) { mcn->current_state.event_send_next = value; }
+static inline void advance_send_next(entl_state_machine_t *mcn) { mcn->current_state.event_send_next += 2; }
+static inline int get_i_sent(entl_state_machine_t *mcn) { return mcn->current_state.event_i_sent; }
+static inline void set_i_sent(entl_state_machine_t *mcn, int value) { mcn->current_state.event_i_sent = value; }
+
+static inline void zebra(entl_state_machine_t *mcn) { set_i_sent(mcn, get_send_next(mcn)); }
+
+static inline void unicorn(entl_state_machine_t *mcn, int value) {
+    // when following 3 members are all zero, it means fresh out of Hello handshake
+    set_i_know(mcn, 0);
+    set_send_next(mcn, 0);
+    set_i_sent(mcn, 0);
+    set_atomic_state(mcn, value);
+}
+
+static inline void clear_intervals(entl_state_machine_t *mcn) {
+#ifdef ENTL_SPEED_CHECK
+    memset(&mcn->current_state.interval_time, 0, sizeof(struct timespec));
+    memset(&mcn->current_state.max_interval_time, 0, sizeof(struct timespec));
+    memset(&mcn->current_state.min_interval_time, 0, sizeof(struct timespec));
+#endif
+}
+
+static inline int current_error_pending(entl_state_machine_t *mcn) {
+    return mcn->error_state.error_count;
+}
+
+static inline void clear_error(entl_state_machine_t *mcn) {
+    mcn->current_state.error_flag = 0;
+    mcn->current_state.error_count = 0;
+}
+
+static inline int recvq_count(entl_state_machine_t *mcn) { return mcn->receive_ATI_queue.count; }
+static inline int recvq_space(entl_state_machine_t *mcn) { return ENTT_queue_space(&mcn->receive_ATI_queue); }
+static inline int recvq_full(entl_state_machine_t *mcn) { return ENTT_queue_full(&mcn->receive_ATI_queue); }
+static inline void* recvq_pop(entl_state_machine_t *mcn) { return ENTT_queue_front_pop(&mcn->receive_ATI_queue); }
+static inline int recvq_push(entl_state_machine_t *mcn) { return ENTT_queue_back_push(&mcn->receive_ATI_queue, mcn->receive_buffer); }
+
+static inline int sendq_count(entl_state_machine_t *mcn) { return mcn->send_ATI_queue.count; }
+static inline int sendq_space(entl_state_machine_t *mcn) { return ENTT_queue_space(&mcn->send_ATI_queue); }
+static inline void* sendq_peek(entl_state_machine_t *mcn) { return ENTT_queue_front(&mcn->send_ATI_queue); }
+static inline void* sendq_pop(entl_state_machine_t *mcn) { return ENTT_queue_front_pop(&mcn->send_ATI_queue); }
+static inline int sendq_push(entl_state_machine_t *mcn, void *data) { return ENTT_queue_back_push(&mcn->send_ATI_queue, data); }
+
+// when the 3 members (event_i_sent, event_i_know, event_send_next) are all zero, things are fresh out of Hello handshake
+static inline void entl_state_machine_init(entl_state_machine_t *mcn, int user_pid) {
+    mcn->state_count = 0;
+    // current_state
+        set_i_know(mcn, 0);
+        set_i_sent(mcn, 0);
+        set_send_next(mcn, 0);
+        set_atomic_state(mcn, 0);
+        clear_error(mcn); // error_flag, error_count
+        mcn->current_state.p_error_flag = 0;
+        memset(&mcn->current_state.update_time, 0, sizeof(struct timespec));
+        memset(&mcn->current_state.error_time, 0, sizeof(struct timespec));
+        clear_intervals(mcn); //  interval_time, max_interval_time, min_interval_time
+    // error_state
+    mcn->error_state.current_state = 0; // ENTL_STATE_IDLE
+    mcn->error_state.error_flag = 0;
+    // return_state
+    mcn->user_pid = user_pid;
+    // AIT mesage handling
+    mcn->receive_buffer = NULL;
+    ENTT_queue_init(&mcn->send_ATI_queue);
+    ENTT_queue_init(&mcn->receive_ATI_queue);
+    // hello
+    mcn->mac_valid = 0;
+    mcn->hello_valid = 0;
+    spin_lock_init(&mcn->state_lock);
+}
+
+// Record first error state, acculate error flags
+static inline void set_error(entl_state_machine_t *mcn, uint32_t error_flag) {
+    entl_state_t *ep = &mcn->error_state;
+
+    ep->error_count++;
+
+    // FIXME: assumes we never wrap
+    if (ep->error_count > 1) {
+        ep->p_error_flag |= error_flag;
+        return;
+    }
+
+    struct timespec ts = current_kernel_time();
+    ep->event_i_know = get_i_know(mcn);
+    ep->event_i_sent = get_i_sent(mcn);
+    ep->current_state = get_atomic_state(mcn);
+    ep->error_flag = error_flag;
+    memcpy(&ep->update_time, &mcn->current_state.update_time, sizeof(struct timespec));
+    memcpy(&ep->error_time, &ts, sizeof(struct timespec));
+}
+
+static inline void calc_intervals(entl_state_machine_t *mcn) {
+#ifdef ENTL_SPEED_CHECK
+    entl_state_t *cs = &mcn->current_state;
+    struct timespec *ts_update = &cs->update_time;
+
+    if (ts_update->tv_sec > 0 || ts_update->tv_nsec > 0) {
+        struct timespec now = current_kernel_time();
+        struct timespec *duration = &cs->interval_time;
+        *duration = timespec_sub(*now - *ts_update);
+
+        struct timespec *ts_max = &cs->max_interval_time;
+        if (timespec_compare(ts_max, duration) < 0) {
+            *ts_max = *duration;
+        }
+
+        struct timespec *ts_min = &cs->min_interval_time;
+        if ((ts_min->tv_sec == 0 && ts_min->tv_nsec == 0)
+        ||  (timespec_compare(duration, ts_min) < 0)) {
+            *ts_min = *duration;
+        }
+    }
+#endif
+}
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entl_stm_if.h e1000e-3.8.4/src/entl_stm_if.h
--- intel-e1000e-3.8.4/src/entl_stm_if.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_stm_if.h	2021-04-25 17:41:46.971474847 -0700
@@ -0,0 +1,21 @@
+#ifndef _ENTL_STM_IF_H_
+#define _ENTL_STM_IF_H_
+
+extern int entl_get_hello(entl_state_machine_t *p, uint16_t *emsg_raw, uint32_t *seqno);
+extern int entl_next_send(entl_state_machine_t *p, uint16_t *emsg_raw, uint32_t *seqno); // ENTL_ACTION
+extern int entl_next_send_tx(entl_state_machine_t *p, uint16_t *emsg_raw, uint32_t *seqno); // ENTL_ACTION
+extern int entl_received(entl_state_machine_t *p, uint16_t from_hi, uint32_t from_lo, uint16_t emsg_raw, uint32_t seqno); // ENTL_ACTION
+extern int entl_send_AIT_message(entl_state_machine_t *p, struct entt_ioctl_ait_data *data);
+extern struct entt_ioctl_ait_data *entl_next_AIT_message(entl_state_machine_t *p);
+extern struct entt_ioctl_ait_data *entl_read_AIT_message(entl_state_machine_t *p); 
+extern uint16_t entl_num_queued(entl_state_machine_t *p);
+extern void entl_link_up(entl_state_machine_t *p);
+extern void entl_new_AIT_message(entl_state_machine_t *p, struct entt_ioctl_ait_data *data);
+extern void entl_read_current_state(entl_state_machine_t *p, entl_state_t *st, entl_state_t *err);
+extern void entl_read_error_state(entl_state_machine_t *p, entl_state_t *st, entl_state_t *err);
+extern void entl_set_my_adder(entl_state_machine_t *p, uint16_t mac_hi, uint32_t mac_lo); 
+extern void entl_state_error(entl_state_machine_t *p, uint32_t error_flag); // enter error state
+
+extern void dump_ait_data(entl_state_machine_t *stm, char *tag, struct entt_ioctl_ait_data *ait_data);
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entl_user_api.h e1000e-3.8.4/src/entl_user_api.h
--- intel-e1000e-3.8.4/src/entl_user_api.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entl_user_api.h	2021-04-25 17:41:46.972474472 -0700
@@ -0,0 +1,20 @@
+#ifndef _ENTL_USER_API_H_
+#define _ENTL_USER_API_H_
+
+// Ethernet Protocol ID's
+#define ETH_P_ECLP  0xEAC0 /* Link Protocol (Atomic) */
+#define ETH_P_ECLD  0xEAC1 /* Link Discovery */
+#define ETH_P_ECLL  0xEAC2 /* Link Local Delivery (virtual, Control Messages) */
+
+// ref: entl_state_error
+#define ENTL_ERROR_FLAG_SEQUENCE 0x0001
+#define ENTL_ERROR_FLAG_TIMEOUT  0x0004
+#define ENTL_ERROR_SAME_ADDRESS  0x0008
+#define ENTL_ERROR_UNKOWN_CMD    0x0010
+#define ENTL_ERROR_UNKOWN_STATE  0x0020
+#define ENTL_ERROR_UNEXPECTED_LU 0x0040
+#define ENTL_ERROR_FATAL         0x8000
+
+#include "entl_ioctl.h"
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/entt_queue.h e1000e-3.8.4/src/entt_queue.h
--- intel-e1000e-3.8.4/src/entt_queue.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/entt_queue.h	2021-04-25 17:41:46.972474472 -0700
@@ -0,0 +1,49 @@
+#ifndef _ENTT_QUEUE_H_
+#define _ENTT_QUEUE_H_
+
+#define MAX_ENTT_QUEUE_SIZE 32
+typedef struct ENTT_queue {
+    uint16_t size;
+    uint16_t count;
+    uint16_t head;
+    uint16_t tail;
+    void *data[MAX_ENTT_QUEUE_SIZE];
+} ENTT_queue_t;
+
+static inline void ENTT_queue_init(ENTT_queue_t *q) {
+    q->size = MAX_ENTT_QUEUE_SIZE;
+    q->count = 0;
+    q->head = q->tail = 0;
+}
+
+static inline int ENTT_queue_space(ENTT_queue_t *q) {
+    return q->size - q->count;
+}
+
+static inline int ENTT_queue_full(ENTT_queue_t *q) {
+    return (q->size == q->count) ? 1 : 0;
+}
+
+static inline int ENTT_queue_back_push(ENTT_queue_t *q, void *dt) {
+    if (q->size == q->count) return -1;
+    q->data[q->tail] = dt;
+    q->tail = (q->tail + 1) % q->size;
+    q->count++;
+    return q->size - q->count;
+}
+
+static inline void *ENTT_queue_front(ENTT_queue_t *q) {
+    if (q->count == 0) return NULL;
+    void *dt = q->data[q->head];
+    return dt;
+}
+
+static inline void *ENTT_queue_front_pop(ENTT_queue_t *q) {
+    if (q->count == 0) return NULL;
+    void *dt = q->data[q->head];
+    q->head = (q->head + 1) % q->size;
+    q->count--;
+    return dt;
+}
+
+#endif
diff -ruN intel-e1000e-3.8.4/src/Makefile e1000e-3.8.4/src/Makefile
--- intel-e1000e-3.8.4/src/Makefile	2020-03-15 01:19:51.863950925 -0700
+++ e1000e-3.8.4/src/Makefile	2021-04-25 17:38:33.552999468 -0700
@@ -8,9 +8,10 @@
 #
 # Makefile for the @SUMMARY@
 #
-
 obj-$(CONFIG_E1000E) += e1000e.o
 
+BUILD_KERNEL := $(shell uname -r)
+EARTHHDR = entl_user_api.h entl_state_machine.h entl_device.h entl_device.c
 define e1000e-y
 	netdev.o
 	ethtool.o
@@ -22,6 +23,7 @@
 	80003es2lan.o
 	82571.o
 	param.o
+	entl_state_machine.o
 endef
 e1000e-y := $(strip ${e1000e-y})
 
@@ -69,7 +71,7 @@
 ###############
 
 # Standard compilation, with regular output
-default:
+default: ${EARTHHDR}
 	@+$(call devkernelbuild,modules)
 
 # Noisy output, for extra debugging
diff -ruN intel-e1000e-3.8.4/src/netdev.c e1000e-3.8.4/src/netdev.c
--- intel-e1000e-3.8.4/src/netdev.c	2020-03-15 01:19:51.863950925 -0700
+++ e1000e-3.8.4/src/netdev.c	2021-04-25 18:57:59.408455363 -0700
@@ -26,91 +26,95 @@
 #include <linux/if_vlan.h>
 #endif
 #include <linux/prefetch.h>
+#ifdef ENTL
+#define _IN_NETDEV_C_
+#endif
 
 #include "e1000.h"
 
 #ifdef CONFIG_E1000E_NAPI
-#define DRV_EXTRAVERSION "" "-NAPI"
+#define DRV_EXTRAVERSION "" \
+			 "-NAPI-ENTL"
 #else
-#define DRV_EXTRAVERSION ""
+#define DRV_EXTRAVERSION "ENTL"
 #endif
 
 #define DRV_VERSION "3.8.4" DRV_EXTRAVERSION
 char e1000e_driver_name[] = "e1000e";
 const char e1000e_driver_version[] = DRV_VERSION;
 
-#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV|NETIF_MSG_PROBE|NETIF_MSG_LINK)
+#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK)
 static int debug = -1;
 module_param(debug, int, 0);
 MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
 
 static const struct e1000_info *e1000_info_tbl[] = {
-	[board_82571]		= &e1000_82571_info,
-	[board_82572]		= &e1000_82572_info,
-	[board_82573]		= &e1000_82573_info,
-	[board_82574]		= &e1000_82574_info,
-	[board_82583]		= &e1000_82583_info,
-	[board_80003es2lan]	= &e1000_es2_info,
-	[board_ich8lan]		= &e1000_ich8_info,
-	[board_ich9lan]		= &e1000_ich9_info,
-	[board_ich10lan]	= &e1000_ich10_info,
-	[board_pchlan]		= &e1000_pch_info,
-	[board_pch2lan]		= &e1000_pch2_info,
-	[board_pch_lpt]		= &e1000_pch_lpt_info,
-	[board_pch_spt]		= &e1000_pch_spt_info,
-	[board_pch_cnp]		= &e1000_pch_cnp_info,
+    [board_82571] = &e1000_82571_info,
+    [board_82572] = &e1000_82572_info,
+    [board_82573] = &e1000_82573_info,
+    [board_82574] = &e1000_82574_info,
+    [board_82583] = &e1000_82583_info,
+    [board_80003es2lan] = &e1000_es2_info,
+    [board_ich8lan] = &e1000_ich8_info,
+    [board_ich9lan] = &e1000_ich9_info,
+    [board_ich10lan] = &e1000_ich10_info,
+    [board_pchlan] = &e1000_pch_info,
+    [board_pch2lan] = &e1000_pch2_info,
+    [board_pch_lpt] = &e1000_pch_lpt_info,
+    [board_pch_spt] = &e1000_pch_spt_info,
+    [board_pch_cnp] = &e1000_pch_cnp_info,
 };
 
-struct e1000_reg_info {
+struct e1000_reg_info
+{
 	u32 ofs;
 	char *name;
 };
 
 static const struct e1000_reg_info e1000_reg_info_tbl[] = {
-	/* General Registers */
-	{E1000_CTRL, "CTRL"},
-	{E1000_STATUS, "STATUS"},
-	{E1000_CTRL_EXT, "CTRL_EXT"},
-
-	/* Interrupt Registers */
-	{E1000_ICR, "ICR"},
-
-	/* Rx Registers */
-	{E1000_RCTL, "RCTL"},
-	{E1000_RDLEN(0), "RDLEN"},
-	{E1000_RDH(0), "RDH"},
-	{E1000_RDT(0), "RDT"},
-	{E1000_RDTR, "RDTR"},
-	{E1000_RXDCTL(0), "RXDCTL"},
-	{E1000_ERT, "ERT"},
-	{E1000_RDBAL(0), "RDBAL"},
-	{E1000_RDBAH(0), "RDBAH"},
-	{E1000_RDFH, "RDFH"},
-	{E1000_RDFT, "RDFT"},
-	{E1000_RDFHS, "RDFHS"},
-	{E1000_RDFTS, "RDFTS"},
-	{E1000_RDFPC, "RDFPC"},
-
-	/* Tx Registers */
-	{E1000_TCTL, "TCTL"},
-	{E1000_TDBAL(0), "TDBAL"},
-	{E1000_TDBAH(0), "TDBAH"},
-	{E1000_TDLEN(0), "TDLEN"},
-	{E1000_TDH(0), "TDH"},
-	{E1000_TDT(0), "TDT"},
-	{E1000_TIDV, "TIDV"},
-	{E1000_TXDCTL(0), "TXDCTL"},
-	{E1000_TADV, "TADV"},
-	{E1000_TARC(0), "TARC"},
-	{E1000_TDFH, "TDFH"},
-	{E1000_TDFT, "TDFT"},
-	{E1000_TDFHS, "TDFHS"},
-	{E1000_TDFTS, "TDFTS"},
-	{E1000_TDFPC, "TDFPC"},
+    /* General Registers */
+    {E1000_CTRL, "CTRL"},
+    {E1000_STATUS, "STATUS"},
+    {E1000_CTRL_EXT, "CTRL_EXT"},
+
+    /* Interrupt Registers */
+    {E1000_ICR, "ICR"},
+
+    /* Rx Registers */
+    {E1000_RCTL, "RCTL"},
+    {E1000_RDLEN(0), "RDLEN"},
+    {E1000_RDH(0), "RDH"},
+    {E1000_RDT(0), "RDT"},
+    {E1000_RDTR, "RDTR"},
+    {E1000_RXDCTL(0), "RXDCTL"},
+    {E1000_ERT, "ERT"},
+    {E1000_RDBAL(0), "RDBAL"},
+    {E1000_RDBAH(0), "RDBAH"},
+    {E1000_RDFH, "RDFH"},
+    {E1000_RDFT, "RDFT"},
+    {E1000_RDFHS, "RDFHS"},
+    {E1000_RDFTS, "RDFTS"},
+    {E1000_RDFPC, "RDFPC"},
+
+    /* Tx Registers */
+    {E1000_TCTL, "TCTL"},
+    {E1000_TDBAL(0), "TDBAL"},
+    {E1000_TDBAH(0), "TDBAH"},
+    {E1000_TDLEN(0), "TDLEN"},
+    {E1000_TDH(0), "TDH"},
+    {E1000_TDT(0), "TDT"},
+    {E1000_TIDV, "TIDV"},
+    {E1000_TXDCTL(0), "TXDCTL"},
+    {E1000_TADV, "TADV"},
+    {E1000_TARC(0), "TARC"},
+    {E1000_TDFH, "TDFH"},
+    {E1000_TDFT, "TDFT"},
+    {E1000_TDFHS, "TDFHS"},
+    {E1000_TDFTS, "TDFTS"},
+    {E1000_TDFPC, "TDFPC"},
 
-	/* List Terminator */
-	{0, NULL}
-};
+    /* List Terminator */
+    {0, NULL}};
 
 /**
  * __ew32_prepare - prepare to write to MAC CSR register on certain parts
@@ -153,7 +157,8 @@
 	char rname[16];
 	u32 regs[8];
 
-	switch (reginfo->ofs) {
+	switch (reginfo->ofs)
+	{
 	case E1000_RXDCTL(0):
 		for (n = 0; n < 2; n++)
 			regs[n] = __er32(hw, E1000_RXDCTL(n));
@@ -182,10 +187,12 @@
 	int i;
 	struct e1000_ps_page *ps_page;
 
-	for (i = 0; i < adapter->rx_ps_pages; i++) {
+	for (i = 0; i < adapter->rx_ps_pages; i++)
+	{
 		ps_page = &bi->ps_pages[i];
 
-		if (ps_page->page) {
+		if (ps_page->page)
+		{
 			pr_info("packet dump for ps_page %d:\n", i);
 			print_hex_dump(KERN_INFO, "", DUMP_PREFIX_ADDRESS,
 				       16, 1, page_address(ps_page->page),
@@ -205,20 +212,22 @@
 	struct e1000_reg_info *reginfo;
 	struct e1000_ring *tx_ring = adapter->tx_ring;
 	struct e1000_tx_desc *tx_desc;
-	struct my_u0 {
+	struct my_u0
+	{
 		__le64 a;
 		__le64 b;
-	} *u0;
+	} * u0;
 	struct e1000_buffer *buffer_info;
 	struct e1000_ring *rx_ring = adapter->rx_ring;
 	union e1000_rx_desc_packet_split *rx_desc_ps;
 	union e1000_rx_desc_extended *rx_desc;
-	struct my_u1 {
+	struct my_u1
+	{
 		__le64 a;
 		__le64 b;
 		__le64 c;
 		__le64 d;
-	} *u1;
+	} * u1;
 	u32 staterr;
 	int i = 0;
 
@@ -226,7 +235,8 @@
 		return;
 
 	/* Print netdevice Info */
-	if (netdev) {
+	if (netdev)
+	{
 		dev_info(pci_dev_to_dev(adapter->pdev), "Net device Info\n");
 		pr_info("Device Name     state            trans_start\n");
 		pr_info("%-15s %016lX %016lX\n", netdev->name,
@@ -237,7 +247,8 @@
 	dev_info(pci_dev_to_dev(adapter->pdev), "Register Dump\n");
 	pr_info(" Register Name   Value\n");
 	for (reginfo = (struct e1000_reg_info *)e1000_reg_info_tbl;
-	     reginfo->name; reginfo++) {
+	     reginfo->name; reginfo++)
+	{
 		e1000_regdump(hw, reginfo);
 	}
 
@@ -291,7 +302,8 @@
 	pr_info("Tl[desc]     [address 63:0  ] [SpeCssSCmCsLen] [bi->dma       ] leng  ntw timestamp        bi->skb <-- Legacy format\n");
 	pr_info("Tc[desc]     [Ce CoCsIpceCoS] [MssHlRSCm0Plen] [bi->dma       ] leng  ntw timestamp        bi->skb <-- Ext Context format\n");
 	pr_info("Td[desc]     [address 63:0  ] [VlaPoRSCm1Dlen] [bi->dma       ] leng  ntw timestamp        bi->skb <-- Ext Data format\n");
-	for (i = 0; tx_ring->desc && (i < tx_ring->count); i++) {
+	for (i = 0; tx_ring->desc && (i < tx_ring->count); i++)
+	{
 		const char *next_desc;
 		tx_desc = E1000_TX_DESC(*tx_ring, i);
 		buffer_info = &tx_ring->buffer_info[i];
@@ -305,8 +317,7 @@
 		else
 			next_desc = "";
 		pr_info("T%c[0x%03X]    %016llX %016llX %016llX %04X  %3X %016llX %p%s\n",
-			(!(le64_to_cpu(u0->b) & BIT(29)) ? 'l' :
-			 ((le64_to_cpu(u0->b) & BIT(20)) ? 'd' : 'c')),
+			(!(le64_to_cpu(u0->b) & BIT(29)) ? 'l' : ((le64_to_cpu(u0->b) & BIT(20)) ? 'd' : 'c')),
 			i,
 			(unsigned long long)le64_to_cpu(u0->a),
 			(unsigned long long)le64_to_cpu(u0->b),
@@ -333,7 +344,8 @@
 		return;
 
 	dev_info(pci_dev_to_dev(adapter->pdev), "Rx Ring Dump\n");
-	switch (adapter->rx_ps_pages) {
+	switch (adapter->rx_ps_pages)
+	{
 	case 1:
 	case 2:
 	case 3:
@@ -362,7 +374,8 @@
 		 *   63       48 47    32 31            20 19               0
 		 */
 		pr_info("RWB[desc]      [ck ipid mrqhsh] [vl   l0 ee  es] [ l3  l2  l1 hs] [reserved      ] ---------------- [bi->skb] <-- Ext Rx Write-Back format\n");
-		for (i = 0; i < rx_ring->count; i++) {
+		for (i = 0; i < rx_ring->count; i++)
+		{
 			const char *next_desc;
 			buffer_info = &rx_ring->buffer_info[i];
 			rx_desc_ps = E1000_RX_DESC_PS(*rx_ring, i);
@@ -377,7 +390,8 @@
 			else
 				next_desc = "";
 
-			if (staterr & E1000_RXD_STAT_DD) {
+			if (staterr & E1000_RXD_STAT_DD)
+			{
 				/* Descriptor Done */
 				pr_info("%s[0x%03X]     %016llX %016llX %016llX %016llX ---------------- %p%s\n",
 					"RWB", i,
@@ -386,7 +400,9 @@
 					(unsigned long long)le64_to_cpu(u1->c),
 					(unsigned long long)le64_to_cpu(u1->d),
 					buffer_info->skb, next_desc);
-			} else {
+			}
+			else
+			{
 				pr_info("%s[0x%03X]     %016llX %016llX %016llX %016llX %016llX %p%s\n",
 					"R  ", i,
 					(unsigned long long)le64_to_cpu(u1->a),
@@ -428,7 +444,8 @@
 		 */
 		pr_info("RWB[desc]      [cs ipid    mrq] [vt   ln xe  xs] [bi->skb] <-- Ext (Write-Back) format\n");
 
-		for (i = 0; i < rx_ring->count; i++) {
+		for (i = 0; i < rx_ring->count; i++)
+		{
 			const char *next_desc;
 
 			buffer_info = &rx_ring->buffer_info[i];
@@ -443,14 +460,17 @@
 			else
 				next_desc = "";
 
-			if (staterr & E1000_RXD_STAT_DD) {
+			if (staterr & E1000_RXD_STAT_DD)
+			{
 				/* Descriptor Done */
 				pr_info("%s[0x%03X]     %016llX %016llX ---------------- %p%s\n",
 					"RWB", i,
 					(unsigned long long)le64_to_cpu(u1->a),
 					(unsigned long long)le64_to_cpu(u1->b),
 					buffer_info->skb, next_desc);
-			} else {
+			}
+			else
+			{
 				pr_info("%s[0x%03X]     %016llX %016llX %016llX %p%s\n",
 					"R  ", i,
 					(unsigned long long)le64_to_cpu(u1->a),
@@ -583,13 +603,13 @@
 	else
 #endif /* NETIF_F_HW_VLAN_TX */
 		napi_gro_receive(&adapter->napi, skb);
-#else /* HAVE_VLAN_RX_REGISTER */
+#else  /* HAVE_VLAN_RX_REGISTER */
 	if (staterr & E1000_RXD_STAT_VP)
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
 
 	napi_gro_receive(&adapter->napi, skb);
 #endif /* HAVE_VLAN_RX_REGISTER */
-#else /* CONFIG_E1000E_NAPI */
+#else  /* CONFIG_E1000E_NAPI */
 #ifdef HAVE_VLAN_RX_REGISTER
 #ifdef NETIF_F_HW_VLAN_TX
 	if (adapter->vlgrp && (staterr & E1000_RXD_STAT_VP))
@@ -597,7 +617,7 @@
 	else
 #endif
 		ret = netif_rx(skb);
-#else /* HAVE_VLAN_RX_REGISTER */
+#else  /* HAVE_VLAN_RX_REGISTER */
 	if (staterr & E1000_RXD_STAT_VP)
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
 
@@ -639,7 +659,8 @@
 		return;
 
 	/* TCP/UDP checksum error bit or IP checksum error bit is set */
-	if (errors & (E1000_RXD_ERR_TCPE | E1000_RXD_ERR_IPE)) {
+	if (errors & (E1000_RXD_ERR_TCPE | E1000_RXD_ERR_IPE))
+	{
 		/* let the stack verify checksum errors */
 		adapter->hw_csum_err++;
 		return;
@@ -662,7 +683,8 @@
 
 	writel(i, rx_ring->tail);
 
-	if (unlikely(!ret_val && (i != readl(rx_ring->tail)))) {
+	if (unlikely(!ret_val && (i != readl(rx_ring->tail))))
+	{
 		u32 rctl = er32(RCTL);
 
 		ew32(RCTL, rctl & ~E1000_RCTL_EN);
@@ -679,7 +701,8 @@
 
 	writel(i, tx_ring->tail);
 
-	if (unlikely(!ret_val && (i != readl(tx_ring->tail)))) {
+	if (unlikely(!ret_val && (i != readl(tx_ring->tail))))
+	{
 		u32 tctl = er32(TCTL);
 
 		ew32(TCTL, tctl & ~E1000_TCTL_EN);
@@ -709,26 +732,30 @@
 	i = rx_ring->next_to_use;
 	buffer_info = &rx_ring->buffer_info[i];
 
-	while (cleaned_count--) {
+	while (cleaned_count--)
+	{
 		skb = buffer_info->skb;
-		if (skb) {
+		if (skb)
+		{
 			skb_trim(skb, 0);
 			goto map_skb;
 		}
 
 		skb = __netdev_alloc_skb_ip_align(netdev, bufsz, gfp);
-		if (!skb) {
+		if (!skb)
+		{
 			/* Better luck next round */
 			adapter->alloc_rx_buff_failed++;
 			break;
 		}
 
 		buffer_info->skb = skb;
-map_skb:
+	map_skb:
 		buffer_info->dma =
 		    dma_map_single(pci_dev_to_dev(pdev), skb->data,
 				   adapter->rx_buffer_len, DMA_FROM_DEVICE);
-		if (dma_mapping_error(pci_dev_to_dev(pdev), buffer_info->dma)) {
+		if (dma_mapping_error(pci_dev_to_dev(pdev), buffer_info->dma))
+		{
 			dev_err(pci_dev_to_dev(pdev), "Rx DMA map failed\n");
 			adapter->rx_dma_failed++;
 			break;
@@ -737,7 +764,8 @@
 		rx_desc = E1000_RX_DESC_EXT(*rx_ring, i);
 		rx_desc->read.buffer_addr = cpu_to_le64(buffer_info->dma);
 
-		if (unlikely(!(i & (E1000_RX_BUFFER_WRITE - 1)))) {
+		if (unlikely(!(i & (E1000_RX_BUFFER_WRITE - 1))))
+		{
 			/* Force memory writes to complete before letting h/w
 			 * know there are new descriptors to fetch.  (Only
 			 * applicable for weak-ordered memory model archs,
@@ -775,7 +803,8 @@
 	 * then demote LTR
 	 */
 	if (!adapter->c10_demote_ltr &&
-	    (mpc || (current_rx_bytes > adapter->c10_pba_bytes))) {
+	    (mpc || (current_rx_bytes > adapter->c10_pba_bytes)))
+	{
 		adapter->c10_demote_ltr = true;
 		e1000_demote_ltr(hw, adapter->c10_demote_ltr, true);
 	}
@@ -803,21 +832,26 @@
 	i = rx_ring->next_to_use;
 	buffer_info = &rx_ring->buffer_info[i];
 
-	while (cleaned_count--) {
+	while (cleaned_count--)
+	{
 		rx_desc = E1000_RX_DESC_PS(*rx_ring, i);
 
-		for (j = 0; j < PS_PAGE_BUFFERS; j++) {
+		for (j = 0; j < PS_PAGE_BUFFERS; j++)
+		{
 			ps_page = &buffer_info->ps_pages[j];
-			if (j >= adapter->rx_ps_pages) {
+			if (j >= adapter->rx_ps_pages)
+			{
 				/* all unused desc entries get hw null ptr */
 				rx_desc->read.buffer_addr[j + 1] =
 				    ~cpu_to_le64(0);
 				continue;
 			}
-			if (!ps_page->page) {
+			if (!ps_page->page)
+			{
 				ps_page->page = alloc_pages_node(adapter->node,
 								 gfp, 0);
-				if (!ps_page->page) {
+				if (!ps_page->page)
+				{
 					adapter->alloc_rx_buff_failed++;
 					goto no_buffers;
 				}
@@ -826,7 +860,8 @@
 							    0, PAGE_SIZE,
 							    DMA_FROM_DEVICE);
 				if (dma_mapping_error(pci_dev_to_dev(pdev),
-						      ps_page->dma)) {
+						      ps_page->dma))
+				{
 					dev_err(pci_dev_to_dev(adapter->pdev),
 						"Rx DMA page map failed\n");
 					adapter->rx_dma_failed++;
@@ -844,7 +879,8 @@
 		skb = __netdev_alloc_skb_ip_align(netdev, adapter->rx_ps_bsize0,
 						  gfp);
 
-		if (!skb) {
+		if (!skb)
+		{
 			adapter->alloc_rx_buff_failed++;
 			break;
 		}
@@ -853,7 +889,8 @@
 		buffer_info->dma =
 		    dma_map_single(pci_dev_to_dev(pdev), skb->data,
 				   adapter->rx_ps_bsize0, DMA_FROM_DEVICE);
-		if (dma_mapping_error(pci_dev_to_dev(pdev), buffer_info->dma)) {
+		if (dma_mapping_error(pci_dev_to_dev(pdev), buffer_info->dma))
+		{
 			dev_err(pci_dev_to_dev(pdev), "Rx DMA map failed\n");
 			adapter->rx_dma_failed++;
 			/* cleanup skb */
@@ -864,7 +901,8 @@
 
 		rx_desc->read.buffer_addr[0] = cpu_to_le64(buffer_info->dma);
 
-		if (unlikely(!(i & (E1000_RX_BUFFER_WRITE - 1)))) {
+		if (unlikely(!(i & (E1000_RX_BUFFER_WRITE - 1))))
+		{
 			/* Force memory writes to complete before letting h/w
 			 * know there are new descriptors to fetch.  (Only
 			 * applicable for weak-ordered memory model archs,
@@ -906,44 +944,51 @@
 	struct e1000_buffer *buffer_info;
 	struct sk_buff *skb;
 	unsigned int i;
-	unsigned int bufsz = 256 - 16;	/* for skb_reserve */
+	unsigned int bufsz = 256 - 16; /* for skb_reserve */
 
 	i = rx_ring->next_to_use;
 	buffer_info = &rx_ring->buffer_info[i];
 
-	while (cleaned_count--) {
+	while (cleaned_count--)
+	{
 		skb = buffer_info->skb;
-		if (skb) {
+		if (skb)
+		{
 			skb_trim(skb, 0);
 			goto check_page;
 		}
 
 		skb = __netdev_alloc_skb_ip_align(netdev, bufsz, gfp);
-		if (unlikely(!skb)) {
+		if (unlikely(!skb))
+		{
 			/* Better luck next round */
 			adapter->alloc_rx_buff_failed++;
 			break;
 		}
 
 		buffer_info->skb = skb;
-check_page:
+	check_page:
 		/* allocate a new page if necessary */
-		if (!buffer_info->page) {
+		if (!buffer_info->page)
+		{
 			buffer_info->page = alloc_pages_node(adapter->node,
 							     gfp, 0);
-			if (unlikely(!buffer_info->page)) {
+			if (unlikely(!buffer_info->page))
+			{
 				adapter->alloc_rx_buff_failed++;
 				break;
 			}
 		}
 
-		if (!buffer_info->dma) {
+		if (!buffer_info->dma)
+		{
 			buffer_info->dma = dma_map_page(pci_dev_to_dev(pdev),
 							buffer_info->page, 0,
 							PAGE_SIZE,
 							DMA_FROM_DEVICE);
 			if (dma_mapping_error(pci_dev_to_dev(pdev),
-					      buffer_info->dma)) {
+					      buffer_info->dma))
+			{
 				adapter->alloc_rx_buff_failed++;
 				break;
 			}
@@ -957,7 +1002,8 @@
 		buffer_info = &rx_ring->buffer_info[i];
 	}
 
-	if (likely(rx_ring->next_to_use != i)) {
+	if (likely(rx_ring->next_to_use != i))
+	{
 		rx_ring->next_to_use = i;
 		if (unlikely(i-- == 0))
 			i = (rx_ring->count - 1);
@@ -1018,7 +1064,8 @@
 	staterr = le32_to_cpu(rx_desc->wb.upper.status_error);
 	buffer_info = &rx_ring->buffer_info[i];
 
-	while (staterr & E1000_RXD_STAT_DD) {
+	while (staterr & E1000_RXD_STAT_DD)
+	{
 		struct sk_buff *skb;
 
 #ifdef CONFIG_E1000E_NAPI
@@ -1026,7 +1073,7 @@
 			break;
 		(*work_done)++;
 #endif
-		dma_rmb();	/* read descriptor and rx_buffer_info after status DD */
+		dma_rmb(); /* read descriptor and rx_buffer_info after status DD */
 
 		skb = buffer_info->skb;
 		buffer_info->skb = NULL;
@@ -1058,7 +1105,8 @@
 		if (unlikely(!(staterr & E1000_RXD_STAT_EOP)))
 			adapter->flags2 |= FLAG2_IS_DISCARDING;
 
-		if (adapter->flags2 & FLAG2_IS_DISCARDING) {
+		if (adapter->flags2 & FLAG2_IS_DISCARDING)
+		{
 			/* All receives must fit into a single buffer */
 			e_dbg("Receive packet consumed multiple buffers\n");
 			/* recycle */
@@ -1069,14 +1117,16 @@
 		}
 
 		if (unlikely((staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK) &&
-			     !(netdev->features & NETIF_F_RXALL))) {
+			     !(netdev->features & NETIF_F_RXALL)))
+		{
 			/* recycle */
 			buffer_info->skb = skb;
 			goto next_desc;
 		}
 
 		/* adjust length to remove Ethernet CRC */
-		if (!(adapter->flags2 & FLAG2_CRC_STRIPPING)) {
+		if (!(adapter->flags2 & FLAG2_CRC_STRIPPING))
+		{
 			/* If configured to store CRC, don't subtract FCS,
 			 * but keep the FCS bytes out of the total_rx_bytes
 			 * counter
@@ -1089,19 +1139,40 @@
 
 		total_rx_bytes += length;
 		total_rx_packets++;
+#ifdef ENTL
+		if (adapter->entl_flag)
+		{
+			skb_put(skb, length);
+			// ENTL only, do not forward
+			if (!entl_device_process_rx_packet(&adapter->entl_dev, skb))
+			{
+				buffer_info->skb = skb; //recycle
+				goto next_desc;
+			}
 
+			// drop non-EC types
+			struct ethhdr *eth = (struct ethhdr *)skb->data;
+			if (eth->h_proto != ETH_P_ECLP && eth->h_proto != ETH_P_ECLD)
+			{
+				buffer_info->skb = skb; //recycle
+				goto next_desc;
+			}
+		}
+#endif
 		/* code added for copybreak, this should improve
 		 * performance for small packets with large amounts
 		 * of reassembly being done in the stack
 		 */
-		if (length < copybreak) {
+		if (length < copybreak)
+		{
 			struct sk_buff *new_skb =
 #ifdef CONFIG_E1000E_NAPI
-				napi_alloc_skb(&adapter->napi, length);
+			    napi_alloc_skb(&adapter->napi, length);
 #else
-				netdev_alloc_skb_ip_align(netdev, length);
+			    netdev_alloc_skb_ip_align(netdev, length);
 #endif
-			if (new_skb) {
+			if (new_skb)
+			{
 				skb_copy_to_linear_data_offset(new_skb,
 							       -NET_IP_ALIGN,
 							       (skb->data -
@@ -1127,11 +1198,12 @@
 		e1000_receive_skb(adapter, netdev, skb, staterr,
 				  rx_desc->wb.upper.vlan);
 
-next_desc:
+	next_desc:
 		rx_desc->wb.upper.status_error &= cpu_to_le32(~0xFF);
 
 		/* return some buffers to hardware, one at a time is too slow */
-		if (cleaned_count >= E1000_RX_BUFFER_WRITE) {
+		if (cleaned_count >= E1000_RX_BUFFER_WRITE)
+		{
 			adapter->alloc_rx_buf(rx_ring, cleaned_count,
 					      GFP_ATOMIC);
 			cleaned_count = 0;
@@ -1170,7 +1242,8 @@
 {
 	struct e1000_adapter *adapter = tx_ring->adapter;
 
-	if (buffer_info->dma) {
+	if (buffer_info->dma)
+	{
 		if (buffer_info->mapped_as_page)
 			dma_unmap_page(pci_dev_to_dev(adapter->pdev),
 				       buffer_info->dma, buffer_info->length,
@@ -1181,7 +1254,8 @@
 					 DMA_TO_DEVICE);
 		buffer_info->dma = 0;
 	}
-	if (buffer_info->skb) {
+	if (buffer_info->skb)
+	{
 		dev_kfree_skb_any(buffer_info->skb);
 		buffer_info->skb = NULL;
 	}
@@ -1205,7 +1279,8 @@
 	if (test_bit(__E1000_DOWN, &adapter->state))
 		return;
 
-	if (!adapter->tx_hang_recheck && (adapter->flags2 & FLAG2_DMA_BURST)) {
+	if (!adapter->tx_hang_recheck && (adapter->flags2 & FLAG2_DMA_BURST))
+	{
 		/* May be block on write-back, flush and detect again
 		 * flush pending descriptor writebacks to memory
 		 */
@@ -1223,7 +1298,8 @@
 	}
 	adapter->tx_hang_recheck = false;
 
-	if (er32(TDH(0)) == er32(TDT(0))) {
+	if (er32(TDH(0)) == er32(TDT(0)))
+	{
 		e_dbg("false hang detected, ignoring\n");
 		return;
 	}
@@ -1280,7 +1356,8 @@
 						     tx_hwtstamp_work);
 	struct e1000_hw *hw = &adapter->hw;
 
-	if (er32(TSYNCTXCTL) & E1000_TSYNCTXCTL_VALID) {
+	if (er32(TSYNCTXCTL) & E1000_TSYNCTXCTL_VALID)
+	{
 		struct sk_buff *skb = adapter->tx_hwtstamp_skb;
 		struct skb_shared_hwtstamps shhwtstamps;
 		u64 txstmp;
@@ -1294,17 +1371,20 @@
 		 * prior to notifying the stack of a Tx timestamp.
 		 */
 		adapter->tx_hwtstamp_skb = NULL;
-		wmb();		/* force write prior to skb_tstamp_tx */
+		wmb(); /* force write prior to skb_tstamp_tx */
 
 		skb_tstamp_tx(skb, &shhwtstamps);
 		dev_kfree_skb_any(skb);
-	} else if (time_after(jiffies, adapter->tx_hwtstamp_start
-			      + adapter->tx_timeout_factor * HZ)) {
+	}
+	else if (time_after(jiffies, adapter->tx_hwtstamp_start + adapter->tx_timeout_factor * HZ))
+	{
 		dev_kfree_skb_any(adapter->tx_hwtstamp_skb);
 		adapter->tx_hwtstamp_skb = NULL;
 		adapter->tx_hwtstamp_timeouts++;
 		e_warn("clearing Tx timestamp hang\n");
-	} else {
+	}
+	else
+	{
 		/* reschedule to check later */
 		schedule_work(&adapter->tx_hwtstamp_work);
 	}
@@ -1335,19 +1415,23 @@
 	eop_desc = E1000_TX_DESC(*tx_ring, eop);
 
 	while ((eop_desc->upper.data & cpu_to_le32(E1000_TXD_STAT_DD)) &&
-	       (count < tx_ring->count)) {
+	       (count < tx_ring->count))
+	{
 		bool cleaned = false;
 
-		dma_rmb();		/* read buffer_info after eop_desc */
-		for (; !cleaned; count++) {
+		dma_rmb(); /* read buffer_info after eop_desc */
+		for (; !cleaned; count++)
+		{
 			tx_desc = E1000_TX_DESC(*tx_ring, i);
 			buffer_info = &tx_ring->buffer_info[i];
 			cleaned = (i == eop);
 
-			if (cleaned) {
+			if (cleaned)
+			{
 				total_tx_packets += buffer_info->segs;
 				total_tx_bytes += buffer_info->bytecount;
-				if (buffer_info->skb) {
+				if (buffer_info->skb)
+				{
 					bytes_compl += buffer_info->skb->len;
 					pkts_compl++;
 				}
@@ -1373,27 +1457,29 @@
 
 #define TX_WAKE_THRESHOLD 32
 	if (count && netif_carrier_ok(netdev) &&
-	    e1000_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD) {
+	    e1000_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD)
+	{
 		/* Make sure that anybody stopping the queue after this
 		 * sees the new next_to_clean.
 		 */
 		smp_mb();
 
 		if (netif_queue_stopped(netdev) &&
-		    !(test_bit(__E1000_DOWN, &adapter->state))) {
+		    !(test_bit(__E1000_DOWN, &adapter->state)))
+		{
 			netif_wake_queue(netdev);
 			++adapter->restart_queue;
 		}
 	}
 
-	if (adapter->detect_tx_hung) {
+	if (adapter->detect_tx_hung)
+	{
 		/* Detect a transmit hang in hardware, this serializes the
 		 * check with the clearing of time_stamp and movement of i
 		 */
 		adapter->detect_tx_hung = false;
 		if (tx_ring->buffer_info[i].time_stamp &&
-		    time_after(jiffies, tx_ring->buffer_info[i].time_stamp
-			       + (adapter->tx_timeout_factor * HZ)) &&
+		    time_after(jiffies, tx_ring->buffer_info[i].time_stamp + (adapter->tx_timeout_factor * HZ)) &&
 		    !(er32(STATUS) & E1000_STATUS_TXOFF))
 			schedule_work(&adapter->print_hang_task);
 		else
@@ -1447,14 +1533,15 @@
 	staterr = le32_to_cpu(rx_desc->wb.middle.status_error);
 	buffer_info = &rx_ring->buffer_info[i];
 
-	while (staterr & E1000_RXD_STAT_DD) {
+	while (staterr & E1000_RXD_STAT_DD)
+	{
 #ifdef CONFIG_E1000E_NAPI
 		if (*work_done >= work_to_do)
 			break;
 		(*work_done)++;
 #endif
 		skb = buffer_info->skb;
-		dma_rmb();	/* read descriptor and rx_buffer_info after status DD */
+		dma_rmb(); /* read descriptor and rx_buffer_info after status DD */
 
 		/* in the packet split case this is header only */
 		prefetch(skb->data - NET_IP_ALIGN);
@@ -1477,7 +1564,8 @@
 		if (!(staterr & E1000_RXD_STAT_EOP))
 			adapter->flags2 |= FLAG2_IS_DISCARDING;
 
-		if (adapter->flags2 & FLAG2_IS_DISCARDING) {
+		if (adapter->flags2 & FLAG2_IS_DISCARDING)
+		{
 			e_dbg("Packet Split buffers didn't pick up the full packet\n");
 			dev_kfree_skb_irq(skb);
 			if (staterr & E1000_RXD_STAT_EOP)
@@ -1486,19 +1574,33 @@
 		}
 
 		if (unlikely((staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK) &&
-			     !(netdev->features & NETIF_F_RXALL))) {
+			     !(netdev->features & NETIF_F_RXALL)))
+		{
 			dev_kfree_skb_irq(skb);
 			goto next_desc;
 		}
 
 		length = le16_to_cpu(rx_desc->wb.middle.length0);
 
-		if (!length) {
+		if (!length)
+		{
 			e_dbg("Last part of the packet spanning multiple descriptors\n");
 			dev_kfree_skb_irq(skb);
 			goto next_desc;
 		}
 
+#ifdef ENTL
+		if (adapter->entl_flag)
+		{
+			// ENTL only, do not forward
+			if (!entl_device_process_rx_packet(&adapter->entl_dev, skb))
+			{
+				dev_kfree_skb_irq(skb);
+				goto next_desc;
+			}
+		}
+#endif
+
 		/* Good Receive */
 		skb_put(skb, length);
 
@@ -1515,7 +1617,8 @@
 			 * context to call kmap_*
 			 */
 			if (l1 && (l1 <= copybreak) &&
-			    ((length + l1) <= adapter->rx_ps_bsize0)) {
+			    ((length + l1) <= adapter->rx_ps_bsize0))
+			{
 				u8 *vaddr;
 
 				ps_page = &buffer_info->ps_pages[0];
@@ -1537,18 +1640,20 @@
 							   DMA_FROM_DEVICE);
 
 				/* remove the CRC */
-				if (!(adapter->flags2 & FLAG2_CRC_STRIPPING)) {
+				if (!(adapter->flags2 & FLAG2_CRC_STRIPPING))
+				{
 					if (!(netdev->features & NETIF_F_RXFCS))
 						l1 -= 4;
 				}
 
 				skb_put(skb, l1);
 				goto copydone;
-			}	/* if */
+			} /* if */
 		}
 #endif
 
-		for (j = 0; j < PS_PAGE_BUFFERS; j++) {
+		for (j = 0; j < PS_PAGE_BUFFERS; j++)
+		{
 			length = le16_to_cpu(rx_desc->wb.upper.length[j]);
 			if (!length)
 				break;
@@ -1567,12 +1672,13 @@
 		/* strip the ethernet crc, problem is we're using pages now so
 		 * this whole operation can get a little cpu intensive
 		 */
-		if (!(adapter->flags2 & FLAG2_CRC_STRIPPING)) {
+		if (!(adapter->flags2 & FLAG2_CRC_STRIPPING))
+		{
 			if (!(netdev->features & NETIF_F_RXFCS))
 				pskb_trim(skb, skb->len - 4);
 		}
 #ifdef CONFIG_E1000E_NAPI
-copydone:
+	copydone:
 #endif
 		total_rx_bytes += skb->len;
 		total_rx_packets++;
@@ -1590,12 +1696,13 @@
 		e1000_receive_skb(adapter, netdev, skb, staterr,
 				  rx_desc->wb.middle.vlan);
 
-next_desc:
+	next_desc:
 		rx_desc->wb.middle.status_error &= cpu_to_le32(~0xFF);
 		buffer_info->skb = NULL;
 
 		/* return some buffers to hardware, one at a time is too slow */
-		if (cleaned_count >= E1000_RX_BUFFER_WRITE) {
+		if (cleaned_count >= E1000_RX_BUFFER_WRITE)
+		{
 			adapter->alloc_rx_buf(rx_ring, cleaned_count,
 					      GFP_ATOMIC);
 			cleaned_count = 0;
@@ -1674,13 +1781,14 @@
 	staterr = le32_to_cpu(rx_desc->wb.upper.status_error);
 	buffer_info = &rx_ring->buffer_info[i];
 
-	while (staterr & E1000_RXD_STAT_DD) {
+	while (staterr & E1000_RXD_STAT_DD)
+	{
 		struct sk_buff *skb;
 
 		if (*work_done >= work_to_do)
 			break;
 		(*work_done)++;
-		dma_rmb();	/* read descriptor and rx_buffer_info after status DD */
+		dma_rmb(); /* read descriptor and rx_buffer_info after status DD */
 
 		skb = buffer_info->skb;
 		buffer_info->skb = NULL;
@@ -1704,7 +1812,8 @@
 		/* errors is only valid for DD + EOP descriptors */
 		if (unlikely((staterr & E1000_RXD_STAT_EOP) &&
 			     ((staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK) &&
-			      !(netdev->features & NETIF_F_RXALL)))) {
+			      !(netdev->features & NETIF_F_RXALL))))
+		{
 			/* recycle both page and skb */
 			buffer_info->skb = skb;
 			/* an error means any chain goes out the window too */
@@ -1713,15 +1822,30 @@
 			rx_ring->rx_skb_top = NULL;
 			goto next_desc;
 		}
+#ifdef ENTL
+		if (adapter->entl_flag)
+		{
+			// ENTL only, do not forward
+			if (!entl_device_process_rx_packet(&adapter->entl_dev, skb))
+			{
+				buffer_info->skb = skb; //recycle
+				goto next_desc;
+			}
+		}
+#endif
 #define rxtop (rx_ring->rx_skb_top)
-		if (!(staterr & E1000_RXD_STAT_EOP)) {
+		if (!(staterr & E1000_RXD_STAT_EOP))
+		{
 			/* this descriptor is only the beginning (or middle) */
-			if (!rxtop) {
+			if (!rxtop)
+			{
 				/* this is the beginning of a chain */
 				rxtop = skb;
 				skb_fill_page_desc(rxtop, 0, buffer_info->page,
 						   0, length);
-			} else {
+			}
+			else
+			{
 				/* this is the middle of a chain */
 				shinfo = skb_shinfo(rxtop);
 				skb_fill_page_desc(rxtop, shinfo->nr_frags,
@@ -1732,8 +1856,11 @@
 			}
 			e1000_consume_page(buffer_info, rxtop, length);
 			goto next_desc;
-		} else {
-			if (rxtop) {
+		}
+		else
+		{
+			if (rxtop)
+			{
 				/* end of the chain */
 				shinfo = skb_shinfo(rxtop);
 				skb_fill_page_desc(rxtop, shinfo->nr_frags,
@@ -1746,12 +1873,15 @@
 				skb = rxtop;
 				rxtop = NULL;
 				e1000_consume_page(buffer_info, skb, length);
-			} else {
+			}
+			else
+			{
 				/* no chain, got EOP, this buf is the packet
 				 * copybreak to save the put_page/alloc_page
 				 */
 				if (length <= copybreak &&
-				    skb_tailroom(skb) >= length) {
+				    skb_tailroom(skb) >= length)
+				{
 					u8 *vaddr;
 					vaddr = kmap_atomic(buffer_info->page);
 					memcpy(skb_tail_pointer(skb), vaddr,
@@ -1761,7 +1891,9 @@
 					 * buffer_info->page
 					 */
 					skb_put(skb, length);
-				} else {
+				}
+				else
+				{
 					skb_fill_page_desc(skb, 0,
 							   buffer_info->page, 0,
 							   length);
@@ -1783,7 +1915,8 @@
 		total_rx_packets++;
 
 		/* eth type trans needs skb->data to point to something */
-		if (!pskb_may_pull(skb, ETH_HLEN)) {
+		if (!pskb_may_pull(skb, ETH_HLEN))
+		{
 			e_err("pskb_may_pull failed.\n");
 			dev_kfree_skb_irq(skb);
 			goto next_desc;
@@ -1792,11 +1925,12 @@
 		e1000_receive_skb(adapter, netdev, skb, staterr,
 				  rx_desc->wb.upper.vlan);
 
-next_desc:
+	next_desc:
 		rx_desc->wb.upper.status_error &= cpu_to_le32(~0xFF);
 
 		/* return some buffers to hardware, one at a time is too slow */
-		if (unlikely(cleaned_count >= E1000_RX_BUFFER_WRITE)) {
+		if (unlikely(cleaned_count >= E1000_RX_BUFFER_WRITE))
+		{
 			adapter->alloc_rx_buf(rx_ring, cleaned_count,
 					      GFP_ATOMIC);
 			cleaned_count = 0;
@@ -1844,9 +1978,11 @@
 	unsigned int i, j;
 
 	/* Free all the Rx ring sk_buffs */
-	for (i = 0; i < rx_ring->count; i++) {
+	for (i = 0; i < rx_ring->count; i++)
+	{
 		buffer_info = &rx_ring->buffer_info[i];
-		if (buffer_info->dma) {
+		if (buffer_info->dma)
+		{
 			if (adapter->clean_rx == e1000_clean_rx_irq)
 				dma_unmap_single(pci_dev_to_dev(pdev),
 						 buffer_info->dma,
@@ -1866,17 +2002,20 @@
 			buffer_info->dma = 0;
 		}
 
-		if (buffer_info->page) {
+		if (buffer_info->page)
+		{
 			put_page(buffer_info->page);
 			buffer_info->page = NULL;
 		}
 
-		if (buffer_info->skb) {
+		if (buffer_info->skb)
+		{
 			dev_kfree_skb(buffer_info->skb);
 			buffer_info->skb = NULL;
 		}
 
-		for (j = 0; j < PS_PAGE_BUFFERS; j++) {
+		for (j = 0; j < PS_PAGE_BUFFERS; j++)
+		{
 			ps_page = &buffer_info->ps_pages[j];
 			if (!ps_page->page)
 				break;
@@ -1890,7 +2029,8 @@
 
 #ifdef CONFIG_E1000E_NAPI
 	/* there also may be some cached data from a chained receive */
-	if (rx_ring->rx_skb_top) {
+	if (rx_ring->rx_skb_top)
+	{
 		dev_kfree_skb(rx_ring->rx_skb_top);
 		rx_ring->rx_skb_top = NULL;
 	}
@@ -1941,7 +2081,8 @@
 	u32 icr = er32(ICR);
 
 	/* read ICR disables interrupts using IAM */
-	if (icr & E1000_ICR_LSC) {
+	if (icr & E1000_ICR_LSC)
+	{
 		hw->mac.get_link_status = true;
 		/* ICH8 workaround-- Call gig speed drop workaround on cable
 		 * disconnect (LSC) before accessing any PHY registers
@@ -1955,7 +2096,8 @@
 		 * adapter in watchdog
 		 */
 		if (netif_carrier_ok(netdev) &&
-		    adapter->flags & FLAG_RX_NEEDS_RESTART) {
+		    adapter->flags & FLAG_RX_NEEDS_RESTART)
+		{
 			/* disable receives */
 			u32 rctl = er32(RCTL);
 
@@ -1968,7 +2110,8 @@
 	}
 
 	/* Reset on uncorrectable ECC error */
-	if ((icr & E1000_ICR_ECCER) && (hw->mac.type >= e1000_pch_lpt)) {
+	if ((icr & E1000_ICR_ECCER) && (hw->mac.type >= e1000_pch_lpt))
+	{
 		u32 pbeccsts = er32(PBECCSTS);
 
 		adapter->corr_errors +=
@@ -1984,7 +2127,8 @@
 		return IRQ_HANDLED;
 	}
 #ifdef CONFIG_E1000E_NAPI
-	if (napi_schedule_prep(&adapter->napi)) {
+	if (napi_schedule_prep(&adapter->napi))
+	{
 		adapter->total_tx_bytes = 0;
 		adapter->total_tx_packets = 0;
 		adapter->total_rx_bytes = 0;
@@ -1997,7 +2141,8 @@
 	adapter->total_tx_packets = 0;
 	adapter->total_rx_packets = 0;
 
-	for (i = 0; i < E1000_MAX_INTR; i++) {
+	for (i = 0; i < E1000_MAX_INTR; i++)
+	{
 		int rx_cleaned = adapter->clean_rx(adapter->rx_ring);
 		int tx_cleaned_complete = e1000_clean_tx_irq(adapter->tx_ring);
 		if (!rx_cleaned && tx_cleaned_complete)
@@ -2028,7 +2173,7 @@
 	u32 rctl, icr = er32(ICR);
 
 	if (!icr || test_bit(__E1000_DOWN, &adapter->state))
-		return IRQ_NONE;	/* Not our interrupt */
+		return IRQ_NONE; /* Not our interrupt */
 
 #ifdef CONFIG_E1000E_NAPI
 	/* IMS will not auto-mask if INT_ASSERTED is not set, and if it is
@@ -2043,7 +2188,8 @@
 	 * IMC write
 	 */
 
-	if (icr & E1000_ICR_LSC) {
+	if (icr & E1000_ICR_LSC)
+	{
 		hw->mac.get_link_status = true;
 		/* ICH8 workaround-- Call gig speed drop workaround on cable
 		 * disconnect (LSC) before accessing any PHY registers
@@ -2058,7 +2204,8 @@
 		 * reset adapter in watchdog
 		 */
 		if (netif_carrier_ok(netdev) &&
-		    (adapter->flags & FLAG_RX_NEEDS_RESTART)) {
+		    (adapter->flags & FLAG_RX_NEEDS_RESTART))
+		{
 			/* disable receives */
 			rctl = er32(RCTL);
 			ew32(RCTL, rctl & ~E1000_RCTL_EN);
@@ -2070,7 +2217,8 @@
 	}
 
 	/* Reset on uncorrectable ECC error */
-	if ((icr & E1000_ICR_ECCER) && (hw->mac.type >= e1000_pch_lpt)) {
+	if ((icr & E1000_ICR_ECCER) && (hw->mac.type >= e1000_pch_lpt))
+	{
 		u32 pbeccsts = er32(PBECCSTS);
 
 		adapter->corr_errors +=
@@ -2086,7 +2234,8 @@
 		return IRQ_HANDLED;
 	}
 #ifdef CONFIG_E1000E_NAPI
-	if (napi_schedule_prep(&adapter->napi)) {
+	if (napi_schedule_prep(&adapter->napi))
+	{
 		adapter->total_tx_bytes = 0;
 		adapter->total_tx_packets = 0;
 		adapter->total_rx_bytes = 0;
@@ -2099,7 +2248,8 @@
 	adapter->total_tx_packets = 0;
 	adapter->total_rx_packets = 0;
 
-	for (i = 0; i < E1000_MAX_INTR; i++) {
+	for (i = 0; i < E1000_MAX_INTR; i++)
+	{
 		rx_cleaned = adapter->clean_rx(adapter->rx_ring);
 		tx_cleaned_complete = e1000_clean_tx_irq(adapter->tx_ring);
 		if (!rx_cleaned && tx_cleaned_complete)
@@ -2122,9 +2272,14 @@
 	hw->mac.get_link_status = true;
 
 	/* guard against interrupt when we're going down */
-	if (!test_bit(__E1000_DOWN, &adapter->state)) {
+	if (!test_bit(__E1000_DOWN, &adapter->state))
+	{
 		mod_timer(&adapter->watchdog_timer, jiffies + 1);
+#ifdef ENTL
+		ew32(IMS, E1000_IMS_OTHER | E1000_IMS_LSC);
+#else
 		ew32(IMS, E1000_IMS_OTHER);
+#endif
 	}
 
 	return IRQ_HANDLED;
@@ -2163,15 +2318,16 @@
 	/* Write the ITR value calculated at the end of the
 	 * previous interrupt.
 	 */
-	if (rx_ring->set_itr) {
-		u32 itr = rx_ring->itr_val ?
-		    1000000000 / (rx_ring->itr_val * 256) : 0;
+	if (rx_ring->set_itr)
+	{
+		u32 itr = rx_ring->itr_val ? 1000000000 / (rx_ring->itr_val * 256) : 0;
 
 		writel(itr, rx_ring->itr_register);
 		rx_ring->set_itr = 0;
 	}
 #ifdef CONFIG_E1000E_NAPI
-	if (napi_schedule_prep(&adapter->napi)) {
+	if (napi_schedule_prep(&adapter->napi))
+	{
 		adapter->total_rx_bytes = 0;
 		adapter->total_rx_packets = 0;
 		__napi_schedule(&adapter->napi);
@@ -2180,7 +2336,8 @@
 	adapter->total_rx_bytes = 0;
 	adapter->total_rx_packets = 0;
 
-	for (i = 0; i < E1000_MAX_INTR; i++) {
+	for (i = 0; i < E1000_MAX_INTR; i++)
+	{
 		int rx_cleaned = adapter->clean_rx(rx_ring);
 		if (!rx_cleaned)
 			goto out;
@@ -2213,7 +2370,8 @@
 	adapter->eiac_mask = 0;
 
 	/* Workaround issue with spurious interrupts on 82574 in MSI-X mode */
-	if (hw->mac.type == e1000_82574) {
+	if (hw->mac.type == e1000_82574)
+	{
 		u32 rfctl = er32(RFCTL);
 
 		rfctl |= E1000_RFCTL_ACK_DIS;
@@ -2265,11 +2423,14 @@
 
 void e1000e_reset_interrupt_capability(struct e1000_adapter *adapter)
 {
-	if (adapter->msix_entries) {
+	if (adapter->msix_entries)
+	{
 		pci_disable_msix(adapter->pdev);
 		kfree(adapter->msix_entries);
 		adapter->msix_entries = NULL;
-	} else if (adapter->flags & FLAG_MSI_ENABLED) {
+	}
+	else if (adapter->flags & FLAG_MSI_ENABLED)
+	{
 		pci_disable_msi(adapter->pdev);
 		adapter->flags &= ~FLAG_MSI_ENABLED;
 	}
@@ -2287,15 +2448,19 @@
 	int err;
 	int i;
 
-	switch (adapter->int_mode) {
+	switch (adapter->int_mode)
+	{
 	case E1000E_INT_MODE_MSIX:
-		if (adapter->flags & FLAG_HAS_MSIX) {
+		if (adapter->flags & FLAG_HAS_MSIX)
+		{
 			adapter->num_vectors = 3; /* RxQ0, TxQ0 and other */
 			adapter->msix_entries =
 			    kzalloc_node(adapter->num_vectors *
-					 sizeof(struct msix_entry), GFP_KERNEL,
+					     sizeof(struct msix_entry),
+					 GFP_KERNEL,
 					 adapter->node);
-			if (adapter->msix_entries) {
+			if (adapter->msix_entries)
+			{
 				struct e1000_adapter *a = adapter;
 
 				for (i = 0; i < adapter->num_vectors; i++)
@@ -2315,9 +2480,12 @@
 		adapter->int_mode = E1000E_INT_MODE_MSI;
 		/* Fall through */
 	case E1000E_INT_MODE_MSI:
-		if (!pci_enable_msi(adapter->pdev)) {
+		if (!pci_enable_msi(adapter->pdev))
+		{
 			adapter->flags |= FLAG_MSI_ENABLED;
-		} else {
+		}
+		else
+		{
 			adapter->int_mode = E1000E_INT_MODE_LEGACY;
 			e_err("Failed to initialize MSI interrupts.  Falling back to legacy interrupts.\n");
 		}
@@ -2351,7 +2519,7 @@
 	if (err)
 		return err;
 	adapter->rx_ring->itr_register = adapter->hw.hw_addr +
-	    E1000_EITR_82574(vector);
+					 E1000_EITR_82574(vector);
 	adapter->rx_ring->itr_val = adapter->itr;
 	vector++;
 
@@ -2363,7 +2531,7 @@
 	if (err)
 		return err;
 	adapter->tx_ring->itr_register = adapter->hw.hw_addr +
-	    E1000_EITR_82574(vector);
+					 E1000_EITR_82574(vector);
 	adapter->tx_ring->itr_val = adapter->itr;
 	vector++;
 
@@ -2389,7 +2557,8 @@
 	struct net_device *netdev = adapter->netdev;
 	int err;
 
-	if (adapter->msix_entries) {
+	if (adapter->msix_entries)
+	{
 		err = e1000_request_msix(adapter);
 		if (!err)
 			return err;
@@ -2398,7 +2567,8 @@
 		adapter->int_mode = E1000E_INT_MODE_MSI;
 		e1000e_set_interrupt_capability(adapter);
 	}
-	if (adapter->flags & FLAG_MSI_ENABLED) {
+	if (adapter->flags & FLAG_MSI_ENABLED)
+	{
 		err = request_irq(adapter->pdev->irq, e1000_intr_msi, 0,
 				  netdev->name, netdev);
 		if (!err)
@@ -2421,7 +2591,8 @@
 {
 	struct net_device *netdev = adapter->netdev;
 
-	if (adapter->msix_entries) {
+	if (adapter->msix_entries)
+	{
 		int vector = 0;
 
 		free_irq(adapter->msix_entries[vector].vector, netdev);
@@ -2451,12 +2622,15 @@
 		ew32(EIAC_82574, 0);
 	e1e_flush();
 
-	if (adapter->msix_entries) {
+	if (adapter->msix_entries)
+	{
 		int i;
 
 		for (i = 0; i < adapter->num_vectors; i++)
 			synchronize_irq(adapter->msix_entries[i].vector);
-	} else {
+	}
+	else
+	{
 		synchronize_irq(adapter->pdev->irq);
 	}
 }
@@ -2469,12 +2643,17 @@
 {
 	struct e1000_hw *hw = &adapter->hw;
 
-	if (adapter->msix_entries) {
+	if (adapter->msix_entries)
+	{
 		ew32(EIAC_82574, adapter->eiac_mask & E1000_EIAC_MASK_82574);
 		ew32(IMS, adapter->eiac_mask | E1000_IMS_LSC);
-	} else if (hw->mac.type >= e1000_pch_lpt) {
+	}
+	else if (hw->mac.type >= e1000_pch_lpt)
+	{
 		ew32(IMS, IMS_ENABLE_MASK | E1000_IMS_ECCER);
-	} else {
+	}
+	else
+	{
 		ew32(IMS, IMS_ENABLE_MASK);
 	}
 	e1e_flush();
@@ -2496,10 +2675,13 @@
 	u32 swsm;
 
 	/* Let firmware know the driver has taken over */
-	if (adapter->flags & FLAG_HAS_SWSM_ON_LOAD) {
+	if (adapter->flags & FLAG_HAS_SWSM_ON_LOAD)
+	{
 		swsm = er32(SWSM);
 		ew32(SWSM, swsm | E1000_SWSM_DRV_LOAD);
-	} else if (adapter->flags & FLAG_HAS_CTRLEXT_ON_LOAD) {
+	}
+	else if (adapter->flags & FLAG_HAS_CTRLEXT_ON_LOAD)
+	{
 		ctrl_ext = er32(CTRL_EXT);
 		ew32(CTRL_EXT, ctrl_ext | E1000_CTRL_EXT_DRV_LOAD);
 	}
@@ -2522,10 +2704,13 @@
 	u32 swsm;
 
 	/* Let firmware taken over control of h/w */
-	if (adapter->flags & FLAG_HAS_SWSM_ON_LOAD) {
+	if (adapter->flags & FLAG_HAS_SWSM_ON_LOAD)
+	{
 		swsm = er32(SWSM);
 		ew32(SWSM, swsm & ~E1000_SWSM_DRV_LOAD);
-	} else if (adapter->flags & FLAG_HAS_CTRLEXT_ON_LOAD) {
+	}
+	else if (adapter->flags & FLAG_HAS_CTRLEXT_ON_LOAD)
+	{
 		ctrl_ext = er32(CTRL_EXT);
 		ew32(CTRL_EXT, ctrl_ext & ~E1000_CTRL_EXT_DRV_LOAD);
 	}
@@ -2610,11 +2795,12 @@
 	if (!rx_ring->buffer_info)
 		goto err;
 
-	for (i = 0; i < rx_ring->count; i++) {
+	for (i = 0; i < rx_ring->count; i++)
+	{
 		buffer_info = &rx_ring->buffer_info[i];
 		buffer_info->ps_pages = kzalloc_node(PS_PAGE_BUFFERS *
-						     sizeof(struct
-							    e1000_ps_page),
+							 sizeof(struct
+								e1000_ps_page),
 						     GFP_KERNEL, adapter->node);
 		if (!buffer_info->ps_pages)
 			goto err_pages;
@@ -2637,7 +2823,8 @@
 	return 0;
 
 err_pages:
-	for (i = 0; i < rx_ring->count; i++) {
+	for (i = 0; i < rx_ring->count; i++)
+	{
 		buffer_info = &rx_ring->buffer_info[i];
 		kfree(buffer_info->ps_pages);
 	}
@@ -2658,7 +2845,8 @@
 	unsigned long size;
 	unsigned int i;
 
-	for (i = 0; i < tx_ring->count; i++) {
+	for (i = 0; i < tx_ring->count; i++)
+	{
 		buffer_info = &tx_ring->buffer_info[i];
 		e1000_put_txbuf(tx_ring, buffer_info);
 	}
@@ -2747,7 +2935,8 @@
 	if (packets == 0)
 		return itr_setting;
 
-	switch (itr_setting) {
+	switch (itr_setting)
+	{
 	case lowest_latency:
 		/* handle TSO and jumbo frames */
 		if (bytes / packets > 8000)
@@ -2755,8 +2944,9 @@
 		else if ((packets < 5) && (bytes > 512))
 			retval = low_latency;
 		break;
-	case low_latency:	/* 50 usec aka 20000 ints/s */
-		if (bytes > 10000) {
+	case low_latency: /* 50 usec aka 20000 ints/s */
+		if (bytes > 10000)
+		{
 			/* this if handles the TSO accounting */
 			if (bytes / packets > 8000)
 				retval = bulk_latency;
@@ -2764,17 +2954,24 @@
 				retval = bulk_latency;
 			else if ((packets > 35))
 				retval = lowest_latency;
-		} else if (bytes / packets > 2000) {
+		}
+		else if (bytes / packets > 2000)
+		{
 			retval = bulk_latency;
-		} else if (packets <= 2 && bytes < 512) {
+		}
+		else if (packets <= 2 && bytes < 512)
+		{
 			retval = lowest_latency;
 		}
 		break;
-	case bulk_latency:	/* 250 usec aka 4000 ints/s */
-		if (bytes > 25000) {
+	case bulk_latency: /* 250 usec aka 4000 ints/s */
+		if (bytes > 25000)
+		{
 			if (packets > 35)
 				retval = low_latency;
-		} else if (bytes < 6000) {
+		}
+		else if (bytes < 6000)
+		{
 			retval = low_latency;
 		}
 		break;
@@ -2789,13 +2986,15 @@
 	u32 new_itr = adapter->itr;
 
 	/* for non-gigabit speeds, just fix the interrupt rate at 4000 */
-	if (adapter->link_speed != SPEED_1000) {
+	if (adapter->link_speed != SPEED_1000)
+	{
 		current_itr = 0;
 		new_itr = 4000;
 		goto set_itr_now;
 	}
 
-	if (adapter->flags2 & FLAG2_DISABLE_AIM) {
+	if (adapter->flags2 & FLAG2_DISABLE_AIM)
+	{
 		new_itr = 0;
 		goto set_itr_now;
 	}
@@ -2817,12 +3016,13 @@
 	current_itr = max(adapter->rx_itr, adapter->tx_itr);
 
 	/* counts and packets in update_itr are dependent on these numbers */
-	switch (current_itr) {
+	switch (current_itr)
+	{
 	case lowest_latency:
 		new_itr = 70000;
 		break;
 	case low_latency:
-		new_itr = 20000;	/* aka hwitr = ~200 */
+		new_itr = 20000; /* aka hwitr = ~200 */
 		break;
 	case bulk_latency:
 		new_itr = 4000;
@@ -2832,13 +3032,13 @@
 	}
 
 set_itr_now:
-	if (new_itr != adapter->itr) {
+	if (new_itr != adapter->itr)
+	{
 		/* this attempts to bias the interrupt rate towards Bulk
 		 * by adding intermediate steps when interrupt rate is
 		 * increasing
 		 */
-		new_itr = new_itr > adapter->itr ?
-		    min(adapter->itr + (new_itr >> 2), new_itr) : new_itr;
+		new_itr = new_itr > adapter->itr ? min(adapter->itr + (new_itr >> 2), new_itr) : new_itr;
 		adapter->itr = new_itr;
 		adapter->rx_ring->itr_val = new_itr;
 		if (adapter->msix_entries)
@@ -2862,12 +3062,15 @@
 	struct e1000_hw *hw = &adapter->hw;
 	u32 new_itr = itr ? 1000000000 / (itr * 256) : 0;
 
-	if (adapter->msix_entries) {
+	if (adapter->msix_entries)
+	{
 		int vector;
 
 		for (vector = 0; vector < adapter->num_vectors; vector++)
 			writel(new_itr, hw->hw_addr + E1000_EITR_82574(vector));
-	} else {
+	}
+	else
+	{
 		ew32(ITR, new_itr);
 	}
 }
@@ -2938,10 +3141,12 @@
 	/* Exit the polling mode, but don't re-enable interrupts if stack might
 	 * poll us due to busy-polling
 	 */
-	if (likely(napi_complete_done(napi, work_done))) {
+	if (likely(napi_complete_done(napi, work_done)))
+	{
 		if (adapter->itr_setting & 3)
 			e1000_set_itr(adapter);
-		if (!test_bit(__E1000_DOWN, &adapter->state)) {
+		if (!test_bit(__E1000_DOWN, &adapter->state))
+		{
 			if (adapter->msix_entries)
 				ew32(IMS, adapter->rx_ring->ims_val);
 			else
@@ -2980,7 +3185,8 @@
 #endif
 
 	/* add VID to filter table */
-	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER) {
+	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER)
+	{
 		index = (vid >> 5) & 0x7F;
 		vfta = E1000_READ_REG_ARRAY(hw, E1000_VFTA, index);
 		vfta |= BIT((vid & 0x1F));
@@ -2991,10 +3197,12 @@
 	/* Copy feature flags from netdev to the vlan netdev for this vid.
 	 * This allows things like TSO to bubble down to our vlan device.
 	 */
-	if (adapter->vlgrp) {
+	if (adapter->vlgrp)
+	{
 		struct vlan_group *vlgrp = adapter->vlgrp;
 		struct net_device *v_netdev = vlan_group_get_device(vlgrp, vid);
-		if (v_netdev) {
+		if (v_netdev)
+		{
 			v_netdev->features |= netdev->features;
 			vlan_group_set_device(vlgrp, vid, v_netdev);
 		}
@@ -3037,7 +3245,8 @@
 #endif /* HAVE_VLAN_RX_REGISTER */
 	if ((adapter->hw.mng_cookie.status &
 	     E1000_MNG_DHCP_COOKIE_STATUS_VLAN) &&
-	    (vid == adapter->mng_vlan_id)) {
+	    (vid == adapter->mng_vlan_id))
+	{
 		/* release control to f/w */
 		e1000e_release_hw_control(adapter);
 #ifdef HAVE_INT_NDO_VLAN_RX_ADD_VID
@@ -3048,7 +3257,8 @@
 	}
 
 	/* remove VID from filter table */
-	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER) {
+	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER)
+	{
 		index = (vid >> 5) & 0x7F;
 		vfta = E1000_READ_REG_ARRAY(hw, E1000_VFTA, index);
 		vfta &= ~BIT((vid & 0x1F));
@@ -3075,13 +3285,15 @@
 	struct e1000_hw *hw = &adapter->hw;
 	u32 rctl;
 
-	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER) {
+	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER)
+	{
 		/* disable VLAN receive filtering */
 		rctl = er32(RCTL);
 		rctl &= ~(E1000_RCTL_VFE | E1000_RCTL_CFIEN);
 		ew32(RCTL, rctl);
 
-		if (adapter->mng_vlan_id != (u16)E1000_MNG_VLAN_NONE) {
+		if (adapter->mng_vlan_id != (u16)E1000_MNG_VLAN_NONE)
+		{
 #ifdef NETIF_F_HW_VLAN_CTAG_RX
 			e1000_vlan_rx_kill_vid(netdev, htons(ETH_P_8021Q),
 					       adapter->mng_vlan_id);
@@ -3102,7 +3314,8 @@
 	struct e1000_hw *hw = &adapter->hw;
 	u32 rctl;
 
-	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER) {
+	if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER)
+	{
 		/* enable VLAN receive filtering */
 		rctl = er32(RCTL);
 		rctl |= E1000_RCTL_VFE;
@@ -3152,10 +3365,12 @@
 	if (!adapter->vlgrp)
 		return;
 
-	if (!vlan_group_get_device(adapter->vlgrp, vid)) {
+	if (!vlan_group_get_device(adapter->vlgrp, vid))
+	{
 		adapter->mng_vlan_id = E1000_MNG_VLAN_NONE;
 		if (adapter->hw.mng_cookie.status &
-		    E1000_MNG_DHCP_COOKIE_STATUS_VLAN) {
+		    E1000_MNG_DHCP_COOKIE_STATUS_VLAN)
+		{
 			e1000_vlan_rx_add_vid(netdev, vid);
 			adapter->mng_vlan_id = vid;
 		}
@@ -3164,11 +3379,14 @@
 		    (vid != old_vid) &&
 		    !vlan_group_get_device(adapter->vlgrp, old_vid))
 			e1000_vlan_rx_kill_vid(netdev, old_vid);
-	} else {
+	}
+	else
+	{
 		adapter->mng_vlan_id = vid;
 	}
 #else /* HAVE_VLAN_RX_REGISTER */
-	if (adapter->hw.mng_cookie.status & E1000_MNG_DHCP_COOKIE_STATUS_VLAN) {
+	if (adapter->hw.mng_cookie.status & E1000_MNG_DHCP_COOKIE_STATUS_VLAN)
+	{
 #ifdef NETIF_F_HW_VLAN_CTAG_RX
 		e1000_vlan_rx_add_vid(netdev, htons(ETH_P_8021Q), vid);
 #else
@@ -3198,27 +3416,33 @@
 		e1000_irq_disable(adapter);
 	adapter->vlgrp = grp;
 
-	if (grp) {
+	if (grp)
+	{
 		/* enable VLAN tag insert/strip */
 		ctrl = er32(CTRL);
 		ctrl |= E1000_CTRL_VME;
 		ew32(CTRL, ctrl);
 
-		if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER) {
+		if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER)
+		{
 			/* enable VLAN receive filtering */
 			rctl = er32(RCTL);
 			rctl &= ~E1000_RCTL_CFIEN;
 			ew32(RCTL, rctl);
 			e1000_update_mng_vlan(adapter);
 		}
-	} else {
+	}
+	else
+	{
 		/* disable VLAN tag insert/strip */
 		ctrl = er32(CTRL);
 		ctrl &= ~E1000_CTRL_VME;
 		ew32(CTRL, ctrl);
 
-		if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER) {
-			if (adapter->mng_vlan_id != (u16)E1000_MNG_VLAN_NONE) {
+		if (adapter->flags & FLAG_HAS_HW_VLAN_FILTER)
+		{
+			if (adapter->mng_vlan_id != (u16)E1000_MNG_VLAN_NONE)
+			{
 				e1000_vlan_rx_kill_vid(netdev,
 						       adapter->mng_vlan_id);
 				adapter->mng_vlan_id = E1000_MNG_VLAN_NONE;
@@ -3241,7 +3465,8 @@
 	if (!adapter->vlgrp)
 		return;
 
-	for (vid = 0; vid < VLAN_N_VID; vid++) {
+	for (vid = 0; vid < VLAN_N_VID; vid++)
+	{
 		if (!vlan_group_get_device(adapter->vlgrp, vid))
 			continue;
 		e1000_vlan_rx_add_vid(adapter->netdev, vid);
@@ -3280,7 +3505,8 @@
 	manc |= E1000_MANC_EN_MNG2HOST;
 	manc2h = er32(MANC2H);
 
-	switch (hw->mac.type) {
+	switch (hw->mac.type)
+	{
 	default:
 		manc2h |= (E1000_MANC2H_PORT_623 | E1000_MANC2H_PORT_664);
 		break;
@@ -3289,7 +3515,8 @@
 		/* Check if IPMI pass-through decision filter already exists;
 		 * if so, enable it.
 		 */
-		for (i = 0, j = 0; i < 8; i++) {
+		for (i = 0, j = 0; i < 8; i++)
+		{
 			mdef = er32(MDEF(i));
 
 			/* Ignore filters with anything other than IPMI ports */
@@ -3308,7 +3535,8 @@
 
 		/* Create new decision filter in an empty filter */
 		for (i = 0, j = 0; i < 8; i++)
-			if (er32(MDEF(i)) == 0) {
+			if (er32(MDEF(i)) == 0)
+			{
 				ew32(MDEF(i), (E1000_MDEF_PORT_623 |
 					       E1000_MDEF_PORT_664));
 				manc2h |= BIT(1);
@@ -3354,7 +3582,8 @@
 	/* Tx irq moderation */
 	ew32(TADV, adapter->tx_abs_int_delay);
 
-	if (adapter->flags2 & FLAG2_DMA_BURST) {
+	if (adapter->flags2 & FLAG2_DMA_BURST)
+	{
 		u32 txdctl = er32(TXDCTL(0));
 
 		txdctl &= ~(E1000_TXDCTL_PTHRESH | E1000_TXDCTL_HTHRESH |
@@ -3380,7 +3609,8 @@
 	tctl |= E1000_TCTL_PSP | E1000_TCTL_RTLC |
 		(E1000_COLLISION_THRESHOLD << E1000_CT_SHIFT);
 
-	if (adapter->flags & FLAG_TARC_SPEED_MODE_BIT) {
+	if (adapter->flags & FLAG_TARC_SPEED_MODE_BIT)
+	{
 		tarc = er32(TARC(0));
 		/* set the speed mode bit, we'll clear it if we're not at
 		 * gigabit link later
@@ -3391,7 +3621,8 @@
 	}
 
 	/* errata: program both queues to unweighted RR */
-	if (adapter->flags & FLAG_TARC_SET_BIT_ZERO) {
+	if (adapter->flags & FLAG_TARC_SET_BIT_ZERO)
+	{
 		tarc = er32(TARC(0));
 		tarc |= 1;
 		ew32(TARC(0), tarc);
@@ -3415,7 +3646,8 @@
 	hw->mac.ops.config_collision_dist(hw);
 
 	/* SPT and KBL Si errata workaround to avoid data corruption */
-	if (hw->mac.type == e1000_pch_spt) {
+	if (hw->mac.type == e1000_pch_spt)
+	{
 		u32 reg_val;
 
 		reg_val = er32(IOSFPC);
@@ -3449,7 +3681,8 @@
 	 * If jumbo frames not set, program related MAC/PHY registers
 	 * to h/w defaults
 	 */
-	if (hw->mac.type >= e1000_pch2lan) {
+	if (hw->mac.type >= e1000_pch2lan)
+	{
 		s32 ret_val;
 
 		if (adapter->netdev->mtu > ETH_DATA_LEN)
@@ -3465,8 +3698,8 @@
 	rctl = er32(RCTL);
 	rctl &= ~(3 << E1000_RCTL_MO_SHIFT);
 	rctl |= E1000_RCTL_EN | E1000_RCTL_BAM |
-	    E1000_RCTL_LBM_NO | E1000_RCTL_RDMTS_HALF |
-	    (adapter->hw.mac.mc_filter_type << E1000_RCTL_MO_SHIFT);
+		E1000_RCTL_LBM_NO | E1000_RCTL_RDMTS_HALF |
+		(adapter->hw.mac.mc_filter_type << E1000_RCTL_MO_SHIFT);
 
 	/* Do not Store bad packets */
 	rctl &= ~E1000_RCTL_SBP;
@@ -3485,7 +3718,8 @@
 		rctl |= E1000_RCTL_SECRC;
 
 	/* Workaround Si errata on 82577/82578 - configure IPG for jumbos */
-	if ((hw->mac.type == e1000_pchlan) && (rctl & E1000_RCTL_LPE)) {
+	if ((hw->mac.type == e1000_pchlan) && (rctl & E1000_RCTL_LPE))
+	{
 		u32 mac_data;
 		u16 phy_data;
 
@@ -3498,7 +3732,8 @@
 		mac_data |= BIT(17);
 		ew32(FFLT_DBG, mac_data);
 
-		if (hw->phy.type == e1000_phy_82577) {
+		if (hw->phy.type == e1000_phy_82577)
+		{
 			e1e_rphy(hw, 22, &phy_data);
 			phy_data &= 0x0fff;
 			phy_data |= BIT(14);
@@ -3511,7 +3746,8 @@
 	/* Setup buffer sizes */
 	rctl &= ~E1000_RCTL_SZ_4096;
 	rctl |= E1000_RCTL_BSEX;
-	switch (adapter->rx_buffer_len) {
+	switch (adapter->rx_buffer_len)
+	{
 	case 2048:
 	default:
 		rctl |= E1000_RCTL_SZ_2048;
@@ -3553,7 +3789,8 @@
 	else
 		adapter->rx_ps_pages = 0;
 
-	if (adapter->rx_ps_pages) {
+	if (adapter->rx_ps_pages)
+	{
 		u32 psrctl = 0;
 
 		/* Enable Packet split descriptors */
@@ -3561,7 +3798,8 @@
 
 		psrctl |= adapter->rx_ps_bsize0 >> E1000_PSRCTL_BSIZE0_SHIFT;
 
-		switch (adapter->rx_ps_pages) {
+		switch (adapter->rx_ps_pages)
+		{
 		case 3:
 			psrctl |= PAGE_SIZE << E1000_PSRCTL_BSIZE3_SHIFT;
 			/* fall-through */
@@ -3577,18 +3815,19 @@
 	}
 
 	/* This is useful for sniffing bad packets. */
-	if (adapter->netdev->features & NETIF_F_RXALL) {
+	if (adapter->netdev->features & NETIF_F_RXALL)
+	{
 		/* UPE and MPE will be handled by normal PROMISC logic
 		 * in e1000e_set_rx_mode
 		 */
-		rctl |= (E1000_RCTL_SBP |	/* Receive bad packets */
-			 E1000_RCTL_BAM |	/* RX All Bcast Pkts */
-			 E1000_RCTL_PMCF);	/* RX All MAC Ctrl Pkts */
-
-		rctl &= ~(E1000_RCTL_VFE |	/* Disable VLAN filter */
-			  E1000_RCTL_DPF |	/* Allow filtered pause */
-			  E1000_RCTL_CFIEN);	/* Dis VLAN CFIEN Filter */
-		/* Do not mess with E1000_CTRL_VME, it affects transmit as well,
+		rctl |= (E1000_RCTL_SBP |  /* Receive bad packets */
+			 E1000_RCTL_BAM |  /* RX All Bcast Pkts */
+			 E1000_RCTL_PMCF); /* RX All MAC Ctrl Pkts */
+
+		rctl &= ~(E1000_RCTL_VFE |   /* Disable VLAN filter */
+			  E1000_RCTL_DPF |   /* Allow filtered pause */
+			  E1000_RCTL_CFIEN); /* Dis VLAN CFIEN Filter */
+					     /* Do not mess with E1000_CTRL_VME, it affects transmit as well,
 		 * and that breaks VLANs.
 		 */
 	}
@@ -3611,19 +3850,24 @@
 	u64 rdba;
 	u32 rdlen, rctl, rxcsum, ctrl_ext;
 
-	if (adapter->rx_ps_pages) {
+	if (adapter->rx_ps_pages)
+	{
 		/* this is a 32 byte descriptor */
 		rdlen = rx_ring->count *
-		    sizeof(union e1000_rx_desc_packet_split);
+			sizeof(union e1000_rx_desc_packet_split);
 		adapter->clean_rx = e1000_clean_rx_irq_ps;
 		adapter->alloc_rx_buf = e1000_alloc_rx_buffers_ps;
 #ifdef CONFIG_E1000E_NAPI
-	} else if (adapter->netdev->mtu > ETH_FRAME_LEN + ETH_FCS_LEN) {
+	}
+	else if (adapter->netdev->mtu > ETH_FRAME_LEN + ETH_FCS_LEN)
+	{
 		rdlen = rx_ring->count * sizeof(union e1000_rx_desc_extended);
 		adapter->clean_rx = e1000_clean_jumbo_rx_irq;
 		adapter->alloc_rx_buf = e1000_alloc_jumbo_rx_buffers;
 #endif
-	} else {
+	}
+	else
+	{
 		rdlen = rx_ring->count * sizeof(union e1000_rx_desc_extended);
 		adapter->clean_rx = e1000_clean_rx_irq;
 		adapter->alloc_rx_buf = e1000_alloc_rx_buffers;
@@ -3635,7 +3879,8 @@
 		ew32(RCTL, rctl & ~E1000_RCTL_EN);
 	e1e_flush();
 	usleep_range(10000, 11000);
-	if (adapter->flags2 & FLAG2_DMA_BURST) {
+	if (adapter->flags2 & FLAG2_DMA_BURST)
+	{
 		/* set the writeback threshold (only takes effect if the RDTR
 		 * is set). set GRAN=1 and write back up to 0x4 worth, and
 		 * enable prefetching of 0x20 Rx descriptors
@@ -3700,12 +3945,15 @@
 	/* With jumbo frames, excessive C-state transition latencies result
 	 * in dropped transactions.
 	 */
-	if (adapter->netdev->mtu > ETH_DATA_LEN) {
+	if (adapter->netdev->mtu > ETH_DATA_LEN)
+	{
 		u32 lat =
 		    ((er32(PBA) & E1000_PBA_RXA_MASK) * 1024 -
-		     adapter->max_frame_size) * 8 / 1000;
+		     adapter->max_frame_size) *
+		    8 / 1000;
 
-		if (adapter->flags & FLAG_IS_ICH) {
+		if (adapter->flags & FLAG_IS_ICH)
+		{
 			u32 rxdctl = er32(RXDCTL(0));
 
 			ew32(RXDCTL(0), rxdctl | 0x3);
@@ -3718,7 +3966,9 @@
 		pm_qos_update_requirement(PM_QOS_CPU_DMA_LATENCY,
 					  adapter->netdev->name, lat);
 #endif
-	} else {
+	}
+	else
+	{
 #ifdef HAVE_PM_QOS_REQUEST_LIST_NEW
 		pm_qos_update_request(&adapter->pm_qos_req,
 				      PM_QOS_DEFAULT_VALUE);
@@ -3757,7 +4007,8 @@
 	u8 *mta_list;
 	int i;
 
-	if (netdev_mc_empty(netdev)) {
+	if (netdev_mc_empty(netdev))
+	{
 		/* nothing to program, so clear mc list */
 		hw->mac.ops.update_mc_addr_list(hw, NULL, 0);
 		return 0;
@@ -3812,7 +4063,8 @@
 	if (netdev_uc_count(netdev) > rar_entries)
 		return -ENOMEM;
 
-	if (!netdev_uc_empty(netdev) && rar_entries) {
+	if (!netdev_uc_empty(netdev) && rar_entries)
+	{
 #ifdef NETDEV_HW_ADDR_T_UNICAST
 		struct netdev_hw_addr *ha;
 #else
@@ -3822,7 +4074,8 @@
 		/* write the addresses in reverse order to avoid write
 		 * combining
 		 */
-		netdev_for_each_uc_addr(ha, netdev) {
+		netdev_for_each_uc_addr(ha, netdev)
+		{
 			int ret_val;
 
 			if (!rar_entries)
@@ -3841,7 +4094,8 @@
 	}
 
 	/* zero out the remaining RAR entries not used above */
-	for (; rar_entries > 0; rar_entries--) {
+	for (; rar_entries > 0; rar_entries--)
+	{
 		ew32(RAH(rar_entries), 0);
 		ew32(RAL(rar_entries), 0);
 	}
@@ -3875,7 +4129,13 @@
 	/* clear the affected bits */
 	rctl &= ~(E1000_RCTL_UPE | E1000_RCTL_MPE);
 
-	if (netdev->flags & IFF_PROMISC) {
+#ifdef ENTL
+	if (adapter->entl_flag || netdev->flags & IFF_PROMISC)
+	{
+#else
+	if (netdev->flags & IFF_PROMISC)
+	{
+#endif
 		rctl |= (E1000_RCTL_UPE | E1000_RCTL_MPE);
 #ifdef HAVE_VLAN_RX_REGISTER
 		rctl &= ~E1000_RCTL_VFE;
@@ -3883,12 +4143,17 @@
 		/* Do not hardware filter VLANs in promisc mode */
 		e1000e_vlan_filter_disable(adapter);
 #endif /* HAVE_VLAN_RX_REGISTER */
-	} else {
+	}
+	else
+	{
 		int count;
 
-		if (netdev->flags & IFF_ALLMULTI) {
+		if (netdev->flags & IFF_ALLMULTI)
+		{
 			rctl |= E1000_RCTL_MPE;
-		} else {
+		}
+		else
+		{
 			/* Write addresses to the MTA, if the attempt fails
 			 * then we should just turn on promiscuous mode so
 			 * that we can at least receive multicast traffic
@@ -3982,16 +4247,19 @@
 	 */
 	if ((hw->mac.type >= e1000_pch_lpt) &&
 	    !(er32(TSYNCTXCTL) & E1000_TSYNCTXCTL_ENABLED) &&
-	    !(er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_ENABLED)) {
+	    !(er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_ENABLED))
+	{
 		u32 fextnvm7 = er32(FEXTNVM7);
 
-		if (!(fextnvm7 & BIT(0))) {
+		if (!(fextnvm7 & BIT(0)))
+		{
 			ew32(FEXTNVM7, fextnvm7 | BIT(0));
 			e1e_flush();
 		}
 	}
 
-	switch (hw->mac.type) {
+	switch (hw->mac.type)
+	{
 	case e1000_pch2lan:
 		/* Stable 96MHz frequency */
 		incperiod = INCPERIOD_96MHZ;
@@ -4000,13 +4268,16 @@
 		adapter->cc.shift = shift + INCPERIOD_SHIFT_96MHZ;
 		break;
 	case e1000_pch_lpt:
-		if (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_SYSCFI) {
+		if (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_SYSCFI)
+		{
 			/* Stable 96MHz frequency */
 			incperiod = INCPERIOD_96MHZ;
 			incvalue = INCVALUE_96MHZ;
 			shift = INCVALUE_SHIFT_96MHZ;
 			adapter->cc.shift = shift + INCPERIOD_SHIFT_96MHZ;
-		} else {
+		}
+		else
+		{
 			/* Stable 25MHz frequency */
 			incperiod = INCPERIOD_25MHZ;
 			incvalue = INCVALUE_25MHZ;
@@ -4015,7 +4286,8 @@
 		}
 		break;
 	case e1000_pch_spt:
-		if (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_SYSCFI) {
+		if (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_SYSCFI)
+		{
 			/* Stable 24MHz frequency */
 			incperiod = INCPERIOD_24MHZ;
 			incvalue = INCVALUE_24MHZ;
@@ -4027,13 +4299,16 @@
 	case e1000_pch_cnp:
 	case e1000_pch_tgp:
 	case e1000_pch_adp:
-		if (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_SYSCFI) {
+		if (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_SYSCFI)
+		{
 			/* Stable 24MHz frequency */
 			incperiod = INCPERIOD_24MHZ;
 			incvalue = INCVALUE_24MHZ;
 			shift = INCVALUE_SHIFT_24MHZ;
 			adapter->cc.shift = shift;
-		} else {
+		}
+		else
+		{
 			/* Stable 38400KHz frequency */
 			incperiod = INCPERIOD_38400KHZ;
 			incvalue = INCVALUE_38400KHZ;
@@ -4097,7 +4372,8 @@
 	if (config->flags)
 		return -EINVAL;
 
-	switch (config->tx_type) {
+	switch (config->tx_type)
+	{
 	case HWTSTAMP_TX_OFF:
 		tsync_tx_ctl = 0;
 		break;
@@ -4107,7 +4383,8 @@
 		return -ERANGE;
 	}
 
-	switch (config->rx_filter) {
+	switch (config->rx_filter)
+	{
 	case HWTSTAMP_FILTER_NONE:
 		tsync_rx_ctl = 0;
 		break;
@@ -4197,7 +4474,8 @@
 	regval |= tsync_tx_ctl;
 	ew32(TSYNCTXCTL, regval);
 	if ((er32(TSYNCTXCTL) & E1000_TSYNCTXCTL_ENABLED) !=
-	    (regval & E1000_TSYNCTXCTL_ENABLED)) {
+	    (regval & E1000_TSYNCTXCTL_ENABLED))
+	{
 		e_err("Timesync Tx Control register not set as expected\n");
 		return -EAGAIN;
 	}
@@ -4210,7 +4488,8 @@
 	if ((er32(TSYNCRXCTL) & (E1000_TSYNCRXCTL_ENABLED |
 				 E1000_TSYNCRXCTL_TYPE_MASK)) !=
 	    (regval & (E1000_TSYNCRXCTL_ENABLED |
-		       E1000_TSYNCRXCTL_TYPE_MASK))) {
+		       E1000_TSYNCRXCTL_TYPE_MASK)))
+	{
 		e_err("Timesync Rx Control register not set as expected\n");
 		return -EAGAIN;
 	}
@@ -4223,7 +4502,8 @@
 	ew32(RXMTRL, rxmtrl);
 
 	/* Filter by destination port */
-	if (is_l4) {
+	if (is_l4)
+	{
 		rxudp = PTP_EV_PORT;
 		cpu_to_be16s(&rxudp);
 	}
@@ -4316,7 +4596,7 @@
 	ew32(TCTL, tctl | E1000_TCTL_EN);
 	tdt = er32(TDT(0));
 	BUG_ON(tdt != tx_ring->next_to_use);
-	tx_desc =  E1000_TX_DESC(*tx_ring, tx_ring->next_to_use);
+	tx_desc = E1000_TX_DESC(*tx_ring, tx_ring->next_to_use);
 	tx_desc->buffer_addr = tx_ring->dma;
 
 	tx_desc->lower.data = cpu_to_le32(txd_lower | size);
@@ -4429,17 +4709,21 @@
 	if (!(adapter->flags & FLAG_HAS_HW_TIMESTAMP))
 		return;
 
-	if (info->adjfreq) {
+	if (info->adjfreq)
+	{
 		/* restore the previous ptp frequency delta */
 		ret_val = info->adjfreq(info, adapter->ptp_delta);
-	} else {
+	}
+	else
+	{
 		/* set the default base frequency if no adjustment possible */
 		ret_val = e1000e_get_base_timinca(adapter, &timinca);
 		if (!ret_val)
 			ew32(TIMINCA, timinca);
 	}
 
-	if (ret_val) {
+	if (ret_val)
+	{
 		dev_warn(&adapter->pdev->dev,
 			 "Failed to restore TIMINCA clock rate delta: %d\n",
 			 ret_val);
@@ -4479,7 +4763,8 @@
 	/* reset Packet Buffer Allocation to default */
 	ew32(PBA, pba);
 
-	if (adapter->max_frame_size > (VLAN_ETH_FRAME_LEN + ETH_FCS_LEN)) {
+	if (adapter->max_frame_size > (VLAN_ETH_FRAME_LEN + ETH_FCS_LEN))
+	{
 		/* To maintain wire speed transmits, the Tx FIFO should be
 		 * large enough to accommodate two full transmit packets,
 		 * rounded up to the next 1KB and expressed in KB.  Likewise,
@@ -4496,7 +4781,8 @@
 		 * but don't include ethernet FCS because hardware appends it
 		 */
 		min_tx_space = (adapter->max_frame_size +
-				sizeof(struct e1000_tx_desc) - ETH_FCS_LEN) * 2;
+				sizeof(struct e1000_tx_desc) - ETH_FCS_LEN) *
+			       2;
 		min_tx_space = ALIGN(min_tx_space, 1024);
 		min_tx_space >>= 10;
 		/* software strips receive CRC, so leave room for it */
@@ -4509,7 +4795,8 @@
 		 * allocation, take space away from current Rx allocation
 		 */
 		if ((tx_space < min_tx_space) &&
-		    ((min_tx_space - tx_space) < pba)) {
+		    ((min_tx_space - tx_space) < pba))
+		{
 			pba -= min_tx_space - tx_space;
 
 			/* if short on Rx space, Rx wins and must trump Tx
@@ -4537,10 +4824,12 @@
 	fc->send_xon = true;
 	fc->current_mode = fc->requested_mode;
 
-	switch (hw->mac.type) {
+	switch (hw->mac.type)
+	{
 	case e1000_ich9lan:
 	case e1000_ich10lan:
-		if (adapter->netdev->mtu > ETH_DATA_LEN) {
+		if (adapter->netdev->mtu > ETH_DATA_LEN)
+		{
 			pba = 14;
 			ew32(PBA, pba);
 			fc->high_water = 0x2800;
@@ -4552,17 +4841,20 @@
 		hwm = min(((pba << 10) * 9 / 10),
 			  ((pba << 10) - adapter->max_frame_size));
 
-		fc->high_water = hwm & E1000_FCRTH_RTH;	/* 8-byte granularity */
+		fc->high_water = hwm & E1000_FCRTH_RTH; /* 8-byte granularity */
 		fc->low_water = fc->high_water - 8;
 		break;
 	case e1000_pchlan:
 		/* Workaround PCH LOM adapter hangs with certain network
 		 * loads.  If hangs persist, try disabling Tx flow control.
 		 */
-		if (adapter->netdev->mtu > ETH_DATA_LEN) {
+		if (adapter->netdev->mtu > ETH_DATA_LEN)
+		{
 			fc->high_water = 0x3500;
 			fc->low_water = 0x1500;
-		} else {
+		}
+		else
+		{
 			fc->high_water = 0x5000;
 			fc->low_water = 0x3000;
 		}
@@ -4578,7 +4870,8 @@
 
 		fc->refresh_time = 0x0400;
 
-		if (adapter->netdev->mtu <= ETH_DATA_LEN) {
+		if (adapter->netdev->mtu <= ETH_DATA_LEN)
+		{
 			fc->high_water = 0x05C20;
 			fc->low_water = 0x05048;
 			fc->pause_time = 0x0650;
@@ -4603,15 +4896,20 @@
 	/* Disable Adaptive Interrupt Moderation if 2 full packets cannot
 	 * fit in receive buffer.
 	 */
-	if (adapter->itr_setting & 0x3) {
-		if ((adapter->max_frame_size * 2) > (pba << 10)) {
-			if (!(adapter->flags2 & FLAG2_DISABLE_AIM)) {
+	if (adapter->itr_setting & 0x3)
+	{
+		if ((adapter->max_frame_size * 2) > (pba << 10))
+		{
+			if (!(adapter->flags2 & FLAG2_DISABLE_AIM))
+			{
 				dev_info(pci_dev_to_dev(adapter->pdev),
 					 "Interrupt Throttle Rate off\n");
 				adapter->flags2 |= FLAG2_DISABLE_AIM;
 				e1000e_write_itr(adapter, 0);
 			}
-		} else if (adapter->flags2 & FLAG2_DISABLE_AIM) {
+		}
+		else if (adapter->flags2 & FLAG2_DISABLE_AIM)
+		{
 			dev_info(pci_dev_to_dev(adapter->pdev),
 				 "Interrupt Throttle Rate on\n");
 			adapter->flags2 &= ~FLAG2_DISABLE_AIM;
@@ -4656,11 +4954,13 @@
 #endif
 
 	/* Set EEE advertisement as appropriate */
-	if (adapter->flags2 & FLAG2_HAS_EEE) {
+	if (adapter->flags2 & FLAG2_HAS_EEE)
+	{
 		s32 ret_val;
 		u16 adv_addr;
 
-		switch (hw->phy.type) {
+		switch (hw->phy.type)
+		{
 		case e1000_phy_82579:
 			adv_addr = I82579_EEE_ADVERTISEMENT;
 			break;
@@ -4674,7 +4974,8 @@
 		}
 
 		ret_val = hw->phy.ops.acquire(hw);
-		if (ret_val) {
+		if (ret_val)
+		{
 			dev_err(pci_dev_to_dev(adapter->pdev),
 				"EEE advertisement - unable to acquire PHY\n");
 			return;
@@ -4695,7 +4996,8 @@
 	e1000_get_phy_info(hw);
 
 	if ((adapter->flags & FLAG_HAS_SMART_POWER_DOWN) &&
-	    !(adapter->flags & FLAG_SMART_POWER_DOWN)) {
+	    !(adapter->flags & FLAG_SMART_POWER_DOWN))
+	{
 		u16 phy_data = 0;
 		/* speed up time to link by disabling smart power down, ignore
 		 * the return value of this function because there is nothing
@@ -4705,7 +5007,8 @@
 		phy_data &= ~IGP02E1000_PM_SPD;
 		e1e_wphy(hw, IGP02E1000_PHY_POWER_MGMT, phy_data);
 	}
-	if (hw->mac.type >= e1000_pch_spt && adapter->int_mode == 0) {
+	if (hw->mac.type >= e1000_pch_spt && adapter->int_mode == 0)
+	{
 		u32 reg;
 
 		/* Fextnvm7 @ 0xe4[2] = 1 */
@@ -4718,7 +5021,6 @@
 		       E1000_FEXTNVM9_IOSFSB_CLKREQ_DIS;
 		ew32(FEXTNVM9, reg);
 	}
-
 }
 
 /**
@@ -4740,7 +5042,15 @@
 void e1000e_up(struct e1000_adapter *adapter)
 {
 	/* hardware has been reset, we need to reload some things */
+#ifdef ENTL
+	if (adapter->entl_flag)
+	{
+		entl_e1000_configure(adapter);
+		entl_device_link_up(&adapter->entl_dev);
+	}
+#else
 	e1000_configure(adapter);
+#endif
 
 	clear_bit(__E1000_DOWN, &adapter->state);
 
@@ -4798,6 +5108,12 @@
 	set_bit(__E1000_DOWN, &adapter->state);
 
 	netif_carrier_off(netdev);
+#ifdef ENTL
+	if (adapter->entl_flag)
+	{
+		entl_device_link_down(&adapter->entl_dev);
+	}
+#endif
 #ifdef DYNAMIC_LTR_SUPPORT
 	adapter->c10_demote_ltr = false;
 	e1000_demote_ltr(hw, false, false);
@@ -4846,7 +5162,8 @@
 		e_dbg("failed to disable jumbo frame workaround mode\n");
 
 #ifdef HAVE_PCI_ERS
-	if (!pci_channel_offline(adapter->pdev)) {
+	if (!pci_channel_offline(adapter->pdev))
+	{
 		if (reset)
 			e1000e_reset(adapter);
 		else if (hw->mac.type >= e1000_pch_spt)
@@ -4890,7 +5207,8 @@
 	int i;
 
 	incvalue = er32(TIMINCA) & E1000_TIMINCA_INCVALUE_MASK;
-	for (i = 0; i < E1000_MAX_82574_SYSTIM_REREADS; i++) {
+	for (i = 0; i < E1000_MAX_82574_SYSTIM_REREADS; i++)
+	{
 		/* latch SYSTIMH on read of SYSTIML */
 		systim_next = (u64)er32(SYSTIML);
 		systim_next |= (u64)er32(SYSTIMH) << 32;
@@ -4931,9 +5249,11 @@
 	systimel = er32(SYSTIML);
 	systimeh = er32(SYSTIMH);
 	/* Is systimel is so large that overflow is possible? */
-	if (systimel >= (u32)0xffffffff - E1000_TIMINCA_INCVALUE_MASK) {
+	if (systimel >= (u32)0xffffffff - E1000_TIMINCA_INCVALUE_MASK)
+	{
 		u32 systimel_2 = er32(SYSTIML);
-		if (systimel > systimel_2) {
+		if (systimel > systimel_2)
+		{
 			/* There was an overflow, read again SYSTIMH, and use
 			 * systimel_2
 			 */
@@ -4985,7 +5305,8 @@
 
 #ifdef HAVE_HW_TIME_STAMP
 	/* Setup hardware time stamping cyclecounter */
-	if (adapter->flags & FLAG_HAS_HW_TIMESTAMP) {
+	if (adapter->flags & FLAG_HAS_HW_TIMESTAMP)
+	{
 		adapter->cc.read = e1000e_cyclecounter_read;
 #ifdef HAVE_INCLUDE_LINUX_TIMECOUNTER_H
 		adapter->cc.mask = CYCLECOUNTER_MASK(64);
@@ -5020,7 +5341,8 @@
 	u32 icr = er32(ICR);
 
 	e_dbg("icr is %08X\n", icr);
-	if (icr & E1000_ICR_RXSEQ) {
+	if (icr & E1000_ICR_RXSEQ)
+	{
 		adapter->flags &= ~FLAG_MSI_TEST_FAILED;
 		/* Force memory writes to complete before acknowledging the
 		 * interrupt is handled.
@@ -5062,7 +5384,8 @@
 
 	err = request_irq(adapter->pdev->irq, e1000_intr_msi_test, 0,
 			  netdev->name, netdev);
-	if (err) {
+	if (err)
+	{
 		pci_disable_msi(adapter->pdev);
 		goto msi_test_failed;
 	}
@@ -5081,12 +5404,15 @@
 
 	e1000_irq_disable(adapter);
 
-	rmb();			/* read flags after interrupt has been fired */
+	rmb(); /* read flags after interrupt has been fired */
 
-	if (adapter->flags & FLAG_MSI_TEST_FAILED) {
+	if (adapter->flags & FLAG_MSI_TEST_FAILED)
+	{
 		adapter->int_mode = E1000E_INT_MODE_LEGACY;
 		e_info("MSI interrupt test failed, using legacy interrupt.\n");
-	} else {
+	}
+	else
+	{
 		e_dbg("MSI interrupt test succeeded!\n");
 	}
 
@@ -5121,7 +5447,8 @@
 	err = e1000_test_msi_interrupt(adapter);
 
 	/* re-enable SERR */
-	if (pci_cmd & PCI_COMMAND_SERR) {
+	if (pci_cmd & PCI_COMMAND_SERR)
+	{
 		pci_read_config_word(adapter->pdev, PCI_COMMAND, &pci_cmd);
 		pci_cmd |= PCI_COMMAND_SERR;
 		pci_write_config_word(adapter->pdev, PCI_COMMAND, pci_cmd);
@@ -5171,7 +5498,8 @@
 	/* If AMT is enabled, let the firmware know that the network
 	 * interface is now open and reset the part to a known state.
 	 */
-	if (adapter->flags & FLAG_HAS_AMT) {
+	if (adapter->flags & FLAG_HAS_AMT)
+	{
 		e1000e_get_hw_control(adapter);
 		e1000e_reset(adapter);
 	}
@@ -5184,7 +5512,7 @@
 		e1000_update_mng_vlan(adapter);
 
 #endif
-	/* DMA latency requirement to workaround jumbo issue */
+		/* DMA latency requirement to workaround jumbo issue */
 #ifdef HAVE_PM_QOS_REQUEST_LIST_NEW
 	pm_qos_add_request(&adapter->pm_qos_req, PM_QOS_CPU_DMA_LATENCY,
 			   PM_QOS_DEFAULT_VALUE);
@@ -5201,7 +5529,18 @@
 	 * as soon as we call pci_request_irq, so we have to setup our
 	 * clean_rx handler before we do so.
 	 */
+#ifdef ENTL
+	if (adapter->entl_flag)
+	{
+		entl_e1000_configure(adapter);
+	}
+	else
+	{
+		e1000_configure(adapter);
+	}
+#else
 	e1000_configure(adapter);
+#endif
 
 	err = e1000_request_irq(adapter);
 	if (err)
@@ -5211,9 +5550,11 @@
 	 * ignore e1000e MSI messages, which means we need to test our MSI
 	 * interrupt now
 	 */
-	if (adapter->int_mode != E1000E_INT_MODE_LEGACY) {
+	if (adapter->int_mode != E1000E_INT_MODE_LEGACY)
+	{
 		err = e1000_test_msi(adapter);
-		if (err) {
+		if (err)
+		{
 			e_err("Interrupt allocation failed\n");
 			goto err_req_irq;
 		}
@@ -5285,7 +5626,8 @@
 
 	pm_runtime_get_sync(pci_dev_to_dev(pdev));
 
-	if (netif_device_present(netdev)) {
+	if (netif_device_present(netdev))
+	{
 		e1000e_down(adapter, true);
 		e1000_free_irq(adapter);
 
@@ -5308,7 +5650,7 @@
 	     E1000_MNG_DHCP_COOKIE_STATUS_VLAN) &&
 	    !(adapter->vlgrp &&
 	      vlan_group_get_device(adapter->vlgrp, adapter->mng_vlan_id)))
-#else /* HAVE_VLAN_RX_REGISTER */
+#else  /* HAVE_VLAN_RX_REGISTER */
 	if (adapter->hw.mng_cookie.status & E1000_MNG_DHCP_COOKIE_STATUS_VLAN)
 #endif /* HAVE_VLAN_RX_REGISTER */
 #ifdef NETIF_F_HW_VLAN_CTAG_RX
@@ -5361,7 +5703,8 @@
 
 	hw->mac.ops.rar_set(&adapter->hw, adapter->hw.mac.addr, 0);
 
-	if (adapter->flags & FLAG_RESET_OVERWRITES_LAA) {
+	if (adapter->flags & FLAG_RESET_OVERWRITES_LAA)
+	{
 		/* activate the work around */
 		e1000e_set_laa_state_82571(&adapter->hw, 1);
 
@@ -5445,7 +5788,8 @@
 					   &phy_data);
 	if (ret_val)
 		goto release;
-	if (phy_data != (HV_STATS_PAGE << IGP_PAGE_SHIFT)) {
+	if (phy_data != (HV_STATS_PAGE << IGP_PAGE_SHIFT))
+	{
 		ret_val = hw->phy.ops.set_page(hw,
 					       HV_STATS_PAGE << IGP_PAGE_SHIFT);
 		if (ret_val)
@@ -5529,7 +5873,7 @@
 	adapter->stats.crcerrs += er32(CRCERRS);
 	adapter->stats.gprc += er32(GPRC);
 	adapter->stats.gorc += er32(GORCL);
-	er32(GORCH);		/* Clear gorc */
+	er32(GORCH); /* Clear gorc */
 #ifdef DYNAMIC_LTR_SUPPORT
 	adapter->c10_rx_bytes = adapter->stats.gorc;
 #endif /* DYNAMIC_LTR_SUPPORT */
@@ -5549,10 +5893,14 @@
 #endif /* DYNAMIC_LTR_SUPPORT */
 
 	/* Half-duplex statistics */
-	if (adapter->link_duplex == HALF_DUPLEX) {
-		if (adapter->flags2 & FLAG2_HAS_PHY_STATS) {
+	if (adapter->link_duplex == HALF_DUPLEX)
+	{
+		if (adapter->flags2 & FLAG2_HAS_PHY_STATS)
+		{
 			e1000e_update_phy_stats(adapter);
-		} else {
+		}
+		else
+		{
 			adapter->stats.scc += er32(SCC);
 			adapter->stats.ecol += er32(ECOL);
 			adapter->stats.mcc += er32(MCC);
@@ -5574,7 +5922,7 @@
 	adapter->stats.xofftxc += er32(XOFFTXC);
 	adapter->stats.gptc += er32(GPTC);
 	adapter->stats.gotc += er32(GOTCL);
-	er32(GOTCH);		/* Clear gotc */
+	er32(GOTCH); /* Clear gotc */
 	adapter->stats.rnbc += er32(RNBC);
 	adapter->stats.ruc += er32(RUC);
 
@@ -5611,17 +5959,17 @@
 #else
 	adapter->net_stats.rx_errors = adapter->stats.rxerrc +
 #endif
-	    adapter->stats.crcerrs + adapter->stats.algnerrc +
-	    adapter->stats.ruc + adapter->stats.roc + adapter->stats.cexterr;
+				  adapter->stats.crcerrs + adapter->stats.algnerrc +
+				  adapter->stats.ruc + adapter->stats.roc + adapter->stats.cexterr;
 #ifdef HAVE_NETDEV_STATS_IN_NETDEV
 	netdev->stats.rx_length_errors = adapter->stats.ruc +
-	    adapter->stats.roc;
+					 adapter->stats.roc;
 	netdev->stats.rx_crc_errors = adapter->stats.crcerrs;
 	netdev->stats.rx_frame_errors = adapter->stats.algnerrc;
 	netdev->stats.rx_missed_errors = adapter->stats.mpc;
 #else
 	adapter->net_stats.rx_length_errors = adapter->stats.ruc +
-	    adapter->stats.roc;
+					      adapter->stats.roc;
 	adapter->net_stats.rx_crc_errors = adapter->stats.crcerrs;
 	adapter->net_stats.rx_frame_errors = adapter->stats.algnerrc;
 	adapter->net_stats.rx_missed_errors = adapter->stats.mpc;
@@ -5635,7 +5983,7 @@
 	netdev->stats.tx_carrier_errors = adapter->stats.tncrs;
 #else
 	adapter->net_stats.tx_errors = adapter->stats.ecol +
-	    adapter->stats.latecol;
+				       adapter->stats.latecol;
 	adapter->net_stats.tx_aborted_errors = adapter->stats.ecol;
 	adapter->net_stats.tx_window_errors = adapter->stats.latecol;
 	adapter->net_stats.tx_carrier_errors = adapter->stats.tncrs;
@@ -5649,7 +5997,8 @@
 	adapter->stats.mgpdc += er32(MGTPDC);
 
 	/* Correctable ECC Errors */
-	if (hw->mac.type >= e1000_pch_lpt) {
+	if (hw->mac.type >= e1000_pch_lpt)
+	{
 		u32 pbeccsts = er32(PBECCSTS);
 
 		adapter->corr_errors +=
@@ -5672,7 +6021,8 @@
 
 	if (!pm_runtime_suspended((pci_dev_to_dev(adapter->pdev))->parent) &&
 	    (er32(STATUS) & E1000_STATUS_LU) &&
-	    (adapter->hw.phy.media_type == e1000_media_type_copper)) {
+	    (adapter->hw.phy.media_type == e1000_media_type_copper))
+	{
 		int ret_val;
 
 		ret_val = e1e_rphy(hw, MII_BMCR, &phy->bmcr);
@@ -5685,7 +6035,9 @@
 		ret_val |= e1e_rphy(hw, MII_ESTATUS, &phy->estatus);
 		if (ret_val)
 			e_warn("Error reading PHY register\n");
-	} else {
+	}
+	else
+	{
 		/* Do not read PHY registers if link is not up
 		 * Set values to typical power-on defaults
 		 */
@@ -5715,9 +6067,9 @@
 		    adapter->link_speed,
 		    adapter->link_duplex == FULL_DUPLEX ? "Full" : "Half",
 		    (ctrl & E1000_CTRL_TFCE) &&
-		    (ctrl & E1000_CTRL_RFCE) ? "Rx/Tx" :
-		    (ctrl & E1000_CTRL_RFCE) ? "Rx" :
-		    (ctrl & E1000_CTRL_TFCE) ? "Tx" : "None");
+			    (ctrl & E1000_CTRL_RFCE)
+			? "Rx/Tx"
+			: (ctrl & E1000_CTRL_RFCE) ? "Rx" : (ctrl & E1000_CTRL_TFCE) ? "Tx" : "None");
 }
 
 static bool e1000e_has_link(struct e1000_adapter *adapter)
@@ -5731,12 +6083,16 @@
 	 * false until the check_for_link establishes link
 	 * for copper adapters ONLY
 	 */
-	switch (hw->phy.media_type) {
+	switch (hw->phy.media_type)
+	{
 	case e1000_media_type_copper:
-		if (hw->mac.get_link_status) {
+		if (hw->mac.get_link_status)
+		{
 			ret_val = hw->mac.ops.check_for_link(hw);
 			link_active = !hw->mac.get_link_status;
-		} else {
+		}
+		else
+		{
 			link_active = true;
 		}
 		break;
@@ -5754,7 +6110,8 @@
 	}
 
 	if ((ret_val == E1000_ERR_PHY) && (hw->phy.type == e1000_phy_igp_3) &&
-	    (er32(CTRL) & E1000_PHY_CTRL_GBE_DISABLE)) {
+	    (er32(CTRL) & E1000_PHY_CTRL_GBE_DISABLE))
+	{
 		/* See e1000_kmrn_lock_loss_workaround_ich8lan() */
 		e_info("Gigabit has been disabled, downgrading speed\n");
 	}
@@ -5766,7 +6123,8 @@
 {
 	/* make sure the receive unit is started */
 	if ((adapter->flags & FLAG_RX_NEEDS_RESTART) &&
-	    (adapter->flags & FLAG_RESTART_NOW)) {
+	    (adapter->flags & FLAG_RESTART_NOW))
+	{
 		struct e1000_hw *hw = &adapter->hw;
 		u32 rctl = er32(RCTL);
 
@@ -5787,7 +6145,8 @@
 	else
 		adapter->phy_hang_count = 0;
 
-	if (adapter->phy_hang_count > 1) {
+	if (adapter->phy_hang_count > 1)
+	{
 		adapter->phy_hang_count = 0;
 		e_dbg("PHY appears hung - resetting\n");
 		schedule_work(&adapter->reset_task);
@@ -5822,8 +6181,10 @@
 	u32 dmoff_exit_timeout = 100, tries = 0;
 
 #ifdef DYNAMIC_LTR_SUPPORT
-	if (test_bit(__E1000_DOWN, &adapter->state)) {
-		if (adapter->c10_demote_ltr) {
+	if (test_bit(__E1000_DOWN, &adapter->state))
+	{
+		if (adapter->c10_demote_ltr)
+		{
 			adapter->c10_demote_ltr = false;
 			e1000_demote_ltr(hw, adapter->c10_demote_ltr, false);
 		}
@@ -5835,7 +6196,8 @@
 #endif /* DYNAMIC_LTR_SUPPORT */
 
 	link = e1000e_has_link(adapter);
-	if ((netif_carrier_ok(netdev)) && link) {
+	if ((netif_carrier_ok(netdev)) && link)
+	{
 		/* Cancel scheduled suspend requests. */
 		pm_runtime_resume(netdev->dev.parent);
 
@@ -5848,8 +6210,10 @@
 		e1000_update_mng_vlan(adapter);
 
 #endif
-	if (link) {
-		if (!netif_carrier_ok(netdev)) {
+	if (link)
+	{
+		if (!netif_carrier_ok(netdev))
+		{
 			bool txb2b = true;
 
 			/* Cancel scheduled suspend requests. */
@@ -5857,8 +6221,10 @@
 
 			/* Checking if MAC is in DMoff state */
 			pcim_state = er32(STATUS);
-			while (pcim_state & E1000_STATUS_PCIM_STATE) {
-				if (tries++ == dmoff_exit_timeout) {
+			while (pcim_state & E1000_STATUS_PCIM_STATE)
+			{
+				if (tries++ == dmoff_exit_timeout)
+				{
 					e_dbg("Error in exiting dmoff\n");
 					break;
 				}
@@ -5892,7 +6258,8 @@
 			    hw->mac.autoneg &&
 			    (adapter->link_speed == SPEED_10 ||
 			     adapter->link_speed == SPEED_100) &&
-			    (adapter->link_duplex == HALF_DUPLEX)) {
+			    (adapter->link_duplex == HALF_DUPLEX))
+			{
 				u16 autoneg_exp;
 
 				e1e_rphy(hw, MII_EXPANSION, &autoneg_exp);
@@ -5903,7 +6270,8 @@
 
 			/* adjust timeout factor according to speed/duplex */
 			adapter->tx_timeout_factor = 1;
-			switch (adapter->link_speed) {
+			switch (adapter->link_speed)
+			{
 			case SPEED_10:
 				txb2b = false;
 				adapter->tx_timeout_factor = 16;
@@ -5918,7 +6286,8 @@
 			 * link-up event
 			 */
 			if ((adapter->flags & FLAG_TARC_SPEED_MODE_BIT) &&
-			    !txb2b) {
+			    !txb2b)
+			{
 				u32 tarc0;
 
 				tarc0 = er32(TARC(0));
@@ -5929,8 +6298,10 @@
 			/* disable TSO for pcie and 10/100 speeds, to avoid
 			 * some hardware issues
 			 */
-			if (!(adapter->flags & FLAG_TSO_FORCE)) {
-				switch (adapter->link_speed) {
+			if (!(adapter->flags & FLAG_TSO_FORCE))
+			{
+				switch (adapter->link_speed)
+				{
 				case SPEED_10:
 				case SPEED_100:
 					e_info("10/100 speed: disabling TSO\n");
@@ -5966,13 +6337,21 @@
 				phy->ops.cfg_on_link_up(hw);
 
 			netif_carrier_on(netdev);
-
+#ifdef ENTL
+			if (adapter->entl_flag)
+			{
+				entl_device_link_up(&adapter->entl_dev);
+			}
+#endif
 			if (!test_bit(__E1000_DOWN, &adapter->state))
 				mod_timer(&adapter->phy_info_timer,
 					  round_jiffies(jiffies + 2 * HZ));
 		}
-	} else {
-		if (netif_carrier_ok(netdev)) {
+	}
+	else
+	{
+		if (netif_carrier_ok(netdev))
+		{
 			adapter->link_speed = 0;
 			adapter->link_duplex = 0;
 			/* Link status message must follow this format */
@@ -5981,7 +6360,12 @@
 			if (!test_bit(__E1000_DOWN, &adapter->state))
 				mod_timer(&adapter->phy_info_timer,
 					  round_jiffies(jiffies + 2 * HZ));
-
+#ifdef ENTL
+			if (adapter->entl_flag)
+			{
+				entl_device_link_down(&adapter->entl_dev);
+			}
+#endif
 			/* 8000ES2LAN requires a Rx packet buffer work-around
 			 * on link down event; reset the controller to flush
 			 * the Rx packet buffer.
@@ -6001,7 +6385,8 @@
 	    adapter->c10_demote_ltr &&
 	    (adapter->stats.mpc <= adapter->c10_mpc_count) &&
 	    ((adapter->c10_rx_bytes - adapter->stats.gorc) <
-	     adapter->c10_pba_bytes)) {
+	     adapter->c10_pba_bytes))
+	{
 		adapter->c10_demote_ltr = false;
 		e1000_demote_ltr(hw, adapter->c10_demote_ltr, link);
 	}
@@ -6035,7 +6420,8 @@
 		adapter->flags |= FLAG_RESTART_NOW;
 
 	/* If reset is necessary, do it outside of interrupt context. */
-	if (adapter->flags & FLAG_RESTART_NOW) {
+	if (adapter->flags & FLAG_RESTART_NOW)
+	{
 		schedule_work(&adapter->reset_task);
 		/* return immediately since reset is imminent */
 		return;
@@ -6044,15 +6430,14 @@
 	e1000e_update_adaptive(&adapter->hw);
 
 	/* Simple mode for Interrupt Throttle Rate (ITR) */
-	if (adapter->itr_setting == 4) {
+	if (adapter->itr_setting == 4)
+	{
 		/* Symmetric Tx/Rx gets a reduced ITR=2000;
 		 * Total asymmetrical Tx or Rx gets ITR=8000;
 		 * everyone else is between 2000-8000.
 		 */
 		u32 goc = (adapter->gotc + adapter->gorc) / 10000;
-		u32 dif = (adapter->gotc > adapter->gorc ?
-			   adapter->gotc - adapter->gorc :
-			   adapter->gorc - adapter->gotc) / 10000;
+		u32 dif = (adapter->gotc > adapter->gorc ? adapter->gotc - adapter->gorc : adapter->gorc - adapter->gotc) / 10000;
 		u32 itr = goc > 0 ? (dif * 6000 / goc + 2000) : 8000;
 
 		e1000e_write_itr(adapter, itr);
@@ -6081,12 +6466,16 @@
 
 #ifdef HAVE_HW_TIME_STAMP
 	/* Clear valid timestamp stuck in RXSTMPL/H due to a Rx error */
-	if (adapter->hwtstamp_config.rx_filter != HWTSTAMP_FILTER_NONE) {
+	if (adapter->hwtstamp_config.rx_filter != HWTSTAMP_FILTER_NONE)
+	{
 		if ((adapter->flags2 & FLAG2_CHECK_RX_HWTSTAMP) &&
-		    (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_VALID)) {
+		    (er32(TSYNCRXCTL) & E1000_TSYNCRXCTL_VALID))
+		{
 			er32(RXSTMPH);
 			adapter->rx_hwtstamp_cleared++;
-		} else {
+		}
+		else
+		{
 			adapter->flags2 |= FLAG2_CHECK_RX_HWTSTAMP;
 		}
 	}
@@ -6098,14 +6487,14 @@
 			  round_jiffies(jiffies + 2 * HZ));
 }
 
-#define E1000_TX_FLAGS_CSUM		0x00000001
-#define E1000_TX_FLAGS_VLAN		0x00000002
-#define E1000_TX_FLAGS_TSO		0x00000004
-#define E1000_TX_FLAGS_IPV4		0x00000008
-#define E1000_TX_FLAGS_NO_FCS		0x00000010
-#define E1000_TX_FLAGS_HWTSTAMP		0x00000020
-#define E1000_TX_FLAGS_VLAN_MASK	0xffff0000
-#define E1000_TX_FLAGS_VLAN_SHIFT	16
+#define E1000_TX_FLAGS_CSUM 0x00000001
+#define E1000_TX_FLAGS_VLAN 0x00000002
+#define E1000_TX_FLAGS_TSO 0x00000004
+#define E1000_TX_FLAGS_IPV4 0x00000008
+#define E1000_TX_FLAGS_NO_FCS 0x00000010
+#define E1000_TX_FLAGS_HWTSTAMP 0x00000020
+#define E1000_TX_FLAGS_VLAN_MASK 0xffff0000
+#define E1000_TX_FLAGS_VLAN_SHIFT 16
 
 static int e1000_tso(struct e1000_ring *tx_ring, struct sk_buff *skb,
 		     __be16 protocol)
@@ -6128,7 +6517,8 @@
 
 	hdr_len = skb_transport_offset(skb) + tcp_hdrlen(skb);
 	mss = skb_shinfo(skb)->gso_size;
-	if (protocol == htons(ETH_P_IP)) {
+	if (protocol == htons(ETH_P_IP))
+	{
 		struct iphdr *iph = ip_hdr(skb);
 		iph->tot_len = 0;
 		iph->check = 0;
@@ -6137,7 +6527,9 @@
 		cmd_length = E1000_TXD_CMD_IP;
 		ipcse = skb_transport_offset(skb) - 1;
 #ifdef NETIF_F_TSO6
-	} else if (skb_is_gso_v6(skb)) {
+	}
+	else if (skb_is_gso_v6(skb))
+	{
 		ipv6_hdr(skb)->payload_len = 0;
 		tcp_hdr(skb)->check = ~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,
 						       &ipv6_hdr(skb)->daddr,
@@ -6176,7 +6568,7 @@
 	tx_ring->next_to_use = i;
 
 	return 1;
-#else /* NETIF_F_TSO */
+#else  /* NETIF_F_TSO */
 	return 0;
 #endif /* NETIF_F_TSO */
 }
@@ -6194,7 +6586,8 @@
 	if (skb->ip_summed != CHECKSUM_PARTIAL)
 		return false;
 
-	switch (protocol) {
+	switch (protocol)
+	{
 	case cpu_to_be16(ETH_P_IP):
 		if (ip_hdr(skb)->protocol == IPPROTO_TCP)
 			cmd_len |= E1000_TXD_CMD_TCP;
@@ -6248,7 +6641,8 @@
 
 	i = tx_ring->next_to_use;
 
-	while (len) {
+	while (len)
+	{
 		buffer_info = &tx_ring->buffer_info[i];
 		size = min(len, max_per_txd);
 
@@ -6266,20 +6660,23 @@
 		offset += size;
 		count++;
 
-		if (len) {
+		if (len)
+		{
 			i++;
 			if (i == tx_ring->count)
 				i = 0;
 		}
 	}
 
-	for (f = 0; f < nr_frags; f++) {
+	for (f = 0; f < nr_frags; f++)
+	{
 		const skb_frag_t *frag = &skb_shinfo(skb)->frags[f];
 
 		len = skb_frag_size(frag);
 		offset = 0;
 
-		while (len) {
+		while (len)
+		{
 			i++;
 			if (i == tx_ring->count)
 				i = 0;
@@ -6294,8 +6691,7 @@
 			    skb_frag_dma_map(pci_dev_to_dev(pdev), frag, offset,
 					     size, DMA_TO_DEVICE);
 			buffer_info->mapped_as_page = true;
-			if (dma_mapping_error
-			    (pci_dev_to_dev(pdev), buffer_info->dma))
+			if (dma_mapping_error(pci_dev_to_dev(pdev), buffer_info->dma))
 				goto dma_error;
 
 			len -= size;
@@ -6305,7 +6701,7 @@
 	}
 
 #ifdef NETIF_F_TSO
-	segs = skb_shinfo(skb)->gso_segs ? : 1;
+	segs = skb_shinfo(skb)->gso_segs ?: 1;
 #else
 	segs = 1;
 #endif
@@ -6325,7 +6721,8 @@
 	if (count)
 		count--;
 
-	while (count--) {
+	while (count--)
+	{
 		if (i == 0)
 			i += tx_ring->count;
 		i--;
@@ -6344,21 +6741,24 @@
 	u32 txd_upper = 0, txd_lower = E1000_TXD_CMD_IFCS;
 	unsigned int i;
 
-	if (tx_flags & E1000_TX_FLAGS_TSO) {
+	if (tx_flags & E1000_TX_FLAGS_TSO)
+	{
 		txd_lower |= E1000_TXD_CMD_DEXT | E1000_TXD_DTYP_D |
-		    E1000_TXD_CMD_TSE;
+			     E1000_TXD_CMD_TSE;
 		txd_upper |= E1000_TXD_POPTS_TXSM << 8;
 
 		if (tx_flags & E1000_TX_FLAGS_IPV4)
 			txd_upper |= E1000_TXD_POPTS_IXSM << 8;
 	}
 
-	if (tx_flags & E1000_TX_FLAGS_CSUM) {
+	if (tx_flags & E1000_TX_FLAGS_CSUM)
+	{
 		txd_lower |= E1000_TXD_CMD_DEXT | E1000_TXD_DTYP_D;
 		txd_upper |= E1000_TXD_POPTS_TXSM << 8;
 	}
 
-	if (tx_flags & E1000_TX_FLAGS_VLAN) {
+	if (tx_flags & E1000_TX_FLAGS_VLAN)
+	{
 		txd_lower |= E1000_TXD_CMD_VLE;
 		txd_upper |= (tx_flags & E1000_TX_FLAGS_VLAN_MASK);
 	}
@@ -6367,7 +6767,8 @@
 		txd_lower &= ~(E1000_TXD_CMD_IFCS);
 
 #ifdef HAVE_HW_TIME_STAMP
-	if (unlikely(tx_flags & E1000_TX_FLAGS_HWTSTAMP)) {
+	if (unlikely(tx_flags & E1000_TX_FLAGS_HWTSTAMP))
+	{
 		txd_lower |= E1000_TXD_CMD_DEXT | E1000_TXD_DTYP_D;
 		txd_upper |= E1000_TXD_EXTCMD_TSTAMP;
 	}
@@ -6375,7 +6776,8 @@
 
 	i = tx_ring->next_to_use;
 
-	do {
+	do
+	{
 		buffer_info = &tx_ring->buffer_info[i];
 		tx_desc = E1000_TX_DESC(*tx_ring, i);
 		tx_desc->buffer_addr = cpu_to_le64(buffer_info->dma);
@@ -6490,13 +6892,17 @@
 	int tso;
 	unsigned int f;
 	__be16 protocol = vlan_get_protocol(skb);
-
-	if (test_bit(__E1000_DOWN, &adapter->state)) {
+#ifdef ENTL
+	unsigned long flags = 0; // FIXME: uninitialized ?
+#endif
+	if (test_bit(__E1000_DOWN, &adapter->state))
+	{
 		dev_kfree_skb_any(skb);
 		return NETDEV_TX_OK;
 	}
 
-	if (skb->len <= 0) {
+	if (skb->len <= 0)
+	{
 		dev_kfree_skb_any(skb);
 		return NETDEV_TX_OK;
 	}
@@ -6507,9 +6913,17 @@
 	if (skb_put_padto(skb, 17))
 		return NETDEV_TX_OK;
 
+#ifdef ENTL
+	if (adapter->entl_flag)
+	{
+		entl_device_process_tx_packet(&adapter->entl_dev, skb);
+	}
+#endif
+
 #ifdef NETIF_F_TSO
 	mss = skb_shinfo(skb)->gso_size;
-	if (mss) {
+	if (mss)
+	{
 		u8 hdr_len;
 
 		/* TSO Workaround for 82571/2/3 Controllers -- if skb->data
@@ -6520,11 +6934,13 @@
 		/* we do this workaround for ES2LAN, but it is un-necessary,
 		 * avoiding it could save a lot of cycles
 		 */
-		if (skb->data_len && (hdr_len == len)) {
+		if (skb->data_len && (hdr_len == len))
+		{
 			unsigned int pull_size;
 
 			pull_size = min_t(unsigned int, 4, skb->data_len);
-			if (!__pskb_pull_tail(skb, pull_size)) {
+			if (!__pskb_pull_tail(skb, pull_size))
+			{
 				e_err("__pskb_pull_tail failed.\n");
 				dev_kfree_skb_any(skb);
 				return NETDEV_TX_OK;
@@ -6554,23 +6970,40 @@
 	if (adapter->hw.mac.tx_pkt_filtering)
 		e1000_transfer_dhcp_info(adapter, skb);
 
+#ifdef ENTL_TX_ON_ENTL_ENABLE
 	/* need: count + 2 desc gap to keep tail from touching
 	 * head, otherwise try next time
 	 */
 	if (e1000_maybe_stop_tx(tx_ring, count + 2))
+	{
+#ifdef ENTL
+		if (adapter->entl_flag)
+		{
+			spin_unlock_irqrestore(&adapter->entl_txring_lock, flags);
+		}
+#endif
 		return NETDEV_TX_BUSY;
+	}
+#endif
 
 #if defined(NETIF_F_HW_VLAN_TX) || defined(NETIF_F_HW_VLAN_CTAG_TX)
-	if (skb_vlan_tag_present(skb)) {
+	if (skb_vlan_tag_present(skb))
+	{
 		tx_flags |= E1000_TX_FLAGS_VLAN;
-		tx_flags |= (skb_vlan_tag_get(skb) <<
-			     E1000_TX_FLAGS_VLAN_SHIFT);
+		tx_flags |= (skb_vlan_tag_get(skb) << E1000_TX_FLAGS_VLAN_SHIFT);
+	}
+#endif
+#ifdef ENTL
+	if (adapter->entl_flag)
+	{
+		spin_lock_irqsave(&adapter->entl_txring_lock, flags);
 	}
 #endif
 	first = tx_ring->next_to_use;
 
 	tso = e1000_tso(tx_ring, skb, protocol);
-	if (tso < 0) {
+	if (tso < 0)
+	{
 		dev_kfree_skb_any(skb);
 		return NETDEV_TX_OK;
 	}
@@ -6595,20 +7028,25 @@
 	/* if count is 0 then mapping error has occurred */
 	count = e1000_tx_map(tx_ring, skb, first, adapter->tx_fifo_limit,
 			     nr_frags);
-	if (count) {
+	if (count)
+	{
 #ifdef HAVE_HW_TIME_STAMP
 #ifdef SKB_SHARED_TX_IS_UNION
 		if (unlikely(skb_shinfo(skb)->tx_flags.flags &
 			     SKBTX_HW_TSTAMP) &&
-		    (adapter->flags & FLAG_HAS_HW_TIMESTAMP)) {
-			if (!adapter->tx_hwtstamp_skb) {
+		    (adapter->flags & FLAG_HAS_HW_TIMESTAMP))
+		{
+			if (!adapter->tx_hwtstamp_skb)
+			{
 				skb_shinfo(skb)->tx_flags.flags |=
 				    SKBTX_IN_PROGRESS;
 				tx_flags |= E1000_TX_FLAGS_HWTSTAMP;
 				adapter->tx_hwtstamp_skb = skb_get(skb);
 				adapter->tx_hwtstamp_start = jiffies;
 				schedule_work(&adapter->tx_hwtstamp_work);
-			} else {
+			}
+			else
+			{
 				adapter->tx_hwtstamp_skipped++;
 			}
 		}
@@ -6616,14 +7054,18 @@
 		skb_tx_timestamp(skb);
 #else
 		if (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
-		    (adapter->flags & FLAG_HAS_HW_TIMESTAMP)) {
-			if (!adapter->tx_hwtstamp_skb) {
+		    (adapter->flags & FLAG_HAS_HW_TIMESTAMP))
+		{
+			if (!adapter->tx_hwtstamp_skb)
+			{
 				skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
 				tx_flags |= E1000_TX_FLAGS_HWTSTAMP;
 				adapter->tx_hwtstamp_skb = skb_get(skb);
 				adapter->tx_hwtstamp_start = jiffies;
 				schedule_work(&adapter->tx_hwtstamp_work);
-			} else {
+			}
+			else
+			{
 				adapter->tx_hwtstamp_skipped++;
 			}
 		}
@@ -6639,11 +7081,13 @@
 		/* Make sure there is space in the ring for the next send. */
 		e1000_maybe_stop_tx(tx_ring,
 				    (MAX_SKB_FRAGS *
-				     DIV_ROUND_UP(PAGE_SIZE,
-						  adapter->tx_fifo_limit) + 2));
+					 DIV_ROUND_UP(PAGE_SIZE,
+						      adapter->tx_fifo_limit) +
+				     2));
 
 		if (!netdev_xmit_more() ||
-		    netif_xmit_stopped(netdev_get_tx_queue(netdev, 0))) {
+		    netif_xmit_stopped(netdev_get_tx_queue(netdev, 0)))
+		{
 			if (adapter->flags2 & FLAG2_PCIM2PCI_ARBITER_WA)
 				e1000e_update_tdt_wa(tx_ring,
 						     tx_ring->next_to_use);
@@ -6665,7 +7109,9 @@
 			mmiowb();
 #endif /* SPIN_UNLOCK_IMPLIES_MMIOWB */
 		}
-	} else {
+	}
+	else
+	{
 		dev_kfree_skb_any(skb);
 		tx_ring->buffer_info[first].time_stamp = 0;
 		tx_ring->next_to_use = first;
@@ -6674,6 +7120,13 @@
 	netdev->trans_start = jiffies;
 #endif
 
+#ifdef ENTL
+	if (adapter->entl_flag)
+	{
+		spin_unlock_irqrestore(&adapter->entl_txring_lock, flags);
+	}
+#endif
+
 	return NETDEV_TX_OK;
 }
 
@@ -6704,7 +7157,8 @@
 	if (test_bit(__E1000_DOWN, &adapter->state))
 		return;
 
-	if (!(adapter->flags & FLAG_RESTART_NOW)) {
+	if (!(adapter->flags & FLAG_RESTART_NOW))
+	{
 		e1000e_dump(adapter);
 		e_err("Reset adapter unexpectedly\n");
 	}
@@ -6725,7 +7179,7 @@
 #else
 struct rtnl_link_stats64 *e1000e_get_stats64(struct net_device *netdev,
 					     struct rtnl_link_stats64 *stats)
-#endif				/* HAVE_VOID_NDO_GET_STATS64 */
+#endif /* HAVE_VOID_NDO_GET_STATS64 */
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
@@ -6746,8 +7200,8 @@
 	 * our own version based on RUC and ROC
 	 */
 	stats->rx_errors = adapter->stats.rxerrc +
-	    adapter->stats.crcerrs + adapter->stats.algnerrc +
-	    adapter->stats.ruc + adapter->stats.roc + adapter->stats.cexterr;
+			   adapter->stats.crcerrs + adapter->stats.algnerrc +
+			   adapter->stats.ruc + adapter->stats.roc + adapter->stats.cexterr;
 	stats->rx_length_errors = adapter->stats.ruc + adapter->stats.roc;
 	stats->rx_crc_errors = adapter->stats.crcerrs;
 	stats->rx_frame_errors = adapter->stats.algnerrc;
@@ -6781,7 +7235,7 @@
 
 	/* only return the current stats */
 	return &adapter->net_stats;
-#else /* HAVE_NETDEV_STATS_IN_NETDEV */
+#else  /* HAVE_NETDEV_STATS_IN_NETDEV */
 	/* only return the current stats */
 	return &netdev->stats;
 #endif /* HAVE_NETDEV_STATS_IN_NETDEV */
@@ -6801,7 +7255,8 @@
 	int max_frame = new_mtu + VLAN_ETH_HLEN + ETH_FCS_LEN;
 
 	/* Jumbo frame support */
-	if (new_mtu > ETH_DATA_LEN && !(adapter->flags & FLAG_HAS_JUMBO_FRAMES)) {
+	if (new_mtu > ETH_DATA_LEN && !(adapter->flags & FLAG_HAS_JUMBO_FRAMES))
+	{
 		e_err("Jumbo Frames not supported.\n");
 		return -EINVAL;
 	}
@@ -6809,7 +7264,8 @@
 	/* Jumbo frame workaround on 82579 and newer requires CRC be stripped */
 	if ((adapter->hw.mac.type >= e1000_pch2lan) &&
 	    !(adapter->flags2 & FLAG2_CRC_STRIPPING) &&
-	    (new_mtu > ETH_DATA_LEN)) {
+	    (new_mtu > ETH_DATA_LEN))
+	{
 		e_err("Jumbo Frames not supported on this device when CRC stripping is disabled.\n");
 		return -EINVAL;
 	}
@@ -6875,7 +7331,8 @@
 	if (adapter->hw.phy.media_type != e1000_media_type_copper)
 		return -EOPNOTSUPP;
 
-	switch (cmd) {
+	switch (cmd)
+	{
 #ifdef SIOCGMIIPHY
 	case SIOCGMIIPHY:
 		data->phy_id = adapter->hw.phy.addr;
@@ -6885,7 +7342,8 @@
 	case SIOCGMIIREG:
 		e1000_phy_read_status(adapter);
 
-		switch (data->reg_num & 0x1F) {
+		switch (data->reg_num & 0x1F)
+		{
 		case MII_BMCR:
 			data->val_out = adapter->phy_regs.bmcr;
 			break;
@@ -6962,7 +7420,8 @@
 		return ret_val;
 
 #ifdef HAVE_PTP_1588_CLOCK
-	switch (config.rx_filter) {
+	switch (config.rx_filter)
+	{
 	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
 	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
 	case HWTSTAMP_FILTER_PTP_V2_SYNC:
@@ -6982,7 +7441,9 @@
 #endif /* HAVE_PTP_1588_CLOCK */
 
 	return copy_to_user(ifr->ifr_data, &config,
-			    sizeof(config)) ? -EFAULT : 0;
+			    sizeof(config))
+		   ? -EFAULT
+		   : 0;
 }
 #endif /* SIOCSHWTSTAMP && HAVE_HW_TIME_STAMP */
 #if defined(SIOCGHWTSTAMP) && defined(HAVE_HW_TIME_STAMP)
@@ -6992,14 +7453,17 @@
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
 	return copy_to_user(ifr->ifr_data, &adapter->hwtstamp_config,
-			    sizeof(adapter->hwtstamp_config)) ? -EFAULT : 0;
+			    sizeof(adapter->hwtstamp_config))
+		   ? -EFAULT
+		   : 0;
 }
 
 #endif /* SIOCGHWTSTAMP && HAVE_HW_TIME_STAMP */
 
 static int e1000_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
 {
-	switch (cmd) {
+	switch (cmd)
+	{
 #ifdef SIOCGMIIPHY
 	case SIOCGMIIPHY:
 #endif
@@ -7024,6 +7488,16 @@
 	case SIOCETHTOOL:
 		return ethtool_ioctl(ifr);
 #endif
+#ifdef ENTL
+	case SIOCDEVPRIVATE_ENTL_RD_CURRENT:
+	case SIOCDEVPRIVATE_ENTL_RD_ERROR:
+	case SIOCDEVPRIVATE_ENTL_SET_SIGRCVR:
+	case SIOCDEVPRIVATE_ENTL_GEN_SIGNAL:
+	case SIOCDEVPRIVATE_ENTL_DO_INIT:
+	case SIOCDEVPRIVATE_ENTT_SEND_AIT:
+	case SIOCDEVPRIVATE_ENTT_READ_AIT:
+		return entl_do_ioctl(netdev, ifr, cmd); 
+#endif
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -7040,7 +7514,8 @@
 	e1000_copy_rx_addrs_to_phy_ich8lan(hw);
 
 	retval = hw->phy.ops.acquire(hw);
-	if (retval) {
+	if (retval)
+	{
 		e_err("Could not acquire PHY\n");
 		return retval;
 	}
@@ -7051,7 +7526,8 @@
 		goto release;
 
 	/* copy MAC MTA to PHY MTA - only needed for pchlan */
-	for (i = 0; i < adapter->hw.mac.mta_reg_count; i++) {
+	for (i = 0; i < adapter->hw.mac.mta_reg_count; i++)
+	{
 		mac_reg = E1000_READ_REG_ARRAY(hw, E1000_MTA, i);
 		hw->phy.ops.write_reg_page(hw, BM_MTA(i),
 					   (u16)(mac_reg & 0xFFFF));
@@ -7358,7 +7834,8 @@
 	present = netif_device_present(netdev);
 	netif_device_detach(netdev);
 
-	if (present && netif_running(netdev)) {
+	if (present && netif_running(netdev))
+	{
 		int count = E1000_CHECK_RESET_COUNT;
 
 		while (test_bit(__E1000_RESETTING, &adapter->state) && count--)
@@ -7400,12 +7877,14 @@
 	if (status & E1000_STATUS_LU)
 		wufc &= ~E1000_WUFC_LNKC;
 
-	if (wufc) {
+	if (wufc)
+	{
 		e1000_setup_rctl(adapter);
 		e1000e_set_rx_mode(netdev);
 
 		/* turn on all-multi mode if wake on multicast is enabled */
-		if (wufc & E1000_WUFC_MC) {
+		if (wufc & E1000_WUFC_MC)
+		{
 			rctl = er32(RCTL);
 			rctl |= E1000_RCTL_MPE;
 			ew32(RCTL, rctl);
@@ -7419,7 +7898,8 @@
 
 		if (adapter->hw.phy.media_type == e1000_media_type_fiber ||
 		    adapter->hw.phy.media_type ==
-		    e1000_media_type_internal_serdes) {
+			e1000_media_type_internal_serdes)
+		{
 			/* keep the laser running in D3 */
 			ctrl_ext = er32(CTRL_EXT);
 			ctrl_ext |= E1000_CTRL_EXT_SDP3_DATA;
@@ -7432,26 +7912,34 @@
 		if (adapter->flags & FLAG_IS_ICH)
 			e1000_suspend_workarounds_ich8lan(&adapter->hw);
 
-		if (adapter->flags2 & FLAG2_HAS_PHY_WAKEUP) {
+		if (adapter->flags2 & FLAG2_HAS_PHY_WAKEUP)
+		{
 			/* enable wakeup by the PHY */
 			retval = e1000_init_phy_wakeup(adapter, wufc);
 			if (retval)
 				return retval;
-		} else {
+		}
+		else
+		{
 			/* enable wakeup by the MAC */
 			ew32(WUFC, wufc);
 			ew32(WUC, E1000_WUC_PME_EN);
 		}
-	} else {
+	}
+	else
+	{
 		ew32(WUC, 0);
 		ew32(WUFC, 0);
 
 		e1000_power_down_phy(adapter);
 	}
 
-	if (adapter->hw.phy.type == e1000_phy_igp_3) {
+	if (adapter->hw.phy.type == e1000_phy_igp_3)
+	{
 		e1000e_igp3_phy_powerdown_workaround_ich8lan(&adapter->hw);
-	} else if (hw->mac.type >= e1000_pch_lpt) {
+	}
+	else if (hw->mac.type >= e1000_pch_lpt)
+	{
 		if (!(wufc & (E1000_WUFC_EX | E1000_WUFC_MC | E1000_WUFC_BC)))
 			/* ULP does not support wake from unicast, multicast
 			 * or broadcast.
@@ -7466,14 +7954,17 @@
 	 * for EEE in Sx
 	 */
 	if ((hw->phy.type >= e1000_phy_i217) &&
-	    adapter->eee_advert && hw->dev_spec.ich8lan.eee_lp_ability) {
+	    adapter->eee_advert && hw->dev_spec.ich8lan.eee_lp_ability)
+	{
 		u16 lpi_ctrl = 0;
 
 		retval = hw->phy.ops.acquire(hw);
-		if (!retval) {
+		if (!retval)
+		{
 			retval = e1e_rphy_locked(hw, I82579_LPI_CTRL,
 						 &lpi_ctrl);
-			if (!retval) {
+			if (!retval)
+			{
 				if (adapter->eee_advert &
 				    hw->dev_spec.ich8lan.eee_lp_ability &
 				    I82579_EEE_100_SUPPORTED)
@@ -7510,7 +8001,8 @@
 	 * the PCI device into guest. For example, the KVM on power is
 	 * one of the cases.
 	 */
-	if (adapter->flags & FLAG_IS_QUAD_PORT) {
+	if (adapter->flags & FLAG_IS_QUAD_PORT)
+	{
 		struct pci_dev *us_dev = pdev->bus->self;
 		u16 devctl;
 
@@ -7548,7 +8040,8 @@
 	u16 aspm_dis_mask = 0;
 	u16 pdev_aspmc, parent_aspmc;
 
-	switch (state) {
+	switch (state)
+	{
 	case PCIE_LINK_STATE_L0S:
 	case PCIE_LINK_STATE_L0S | PCIE_LINK_STATE_L1:
 		aspm_dis_mask |= PCI_EXP_LNKCTL_ASPM_L0S;
@@ -7563,7 +8056,8 @@
 	pcie_capability_read_word(pdev, PCI_EXP_LNKCTL, &pdev_aspmc);
 	pdev_aspmc &= PCI_EXP_LNKCTL_ASPMC;
 
-	if (parent) {
+	if (parent)
+	{
 		pcie_capability_read_word(parent, PCI_EXP_LNKCTL,
 					  &parent_aspmc);
 		parent_aspmc &= PCI_EXP_LNKCTL_ASPMC;
@@ -7575,10 +8069,8 @@
 		return;
 
 	dev_info(&pdev->dev, "Disabling ASPM %s %s\n",
-		 (aspm_dis_mask & pdev_aspmc & PCI_EXP_LNKCTL_ASPM_L0S) ?
-		 "L0s" : "",
-		 (aspm_dis_mask & pdev_aspmc & PCI_EXP_LNKCTL_ASPM_L1) ?
-		 "L1" : "");
+		 (aspm_dis_mask & pdev_aspmc & PCI_EXP_LNKCTL_ASPM_L0S) ? "L0s" : "",
+		 (aspm_dis_mask & pdev_aspmc & PCI_EXP_LNKCTL_ASPM_L1) ? "L1" : "");
 
 #ifdef CONFIG_PCIEASPM
 	if (locked)
@@ -7657,7 +8149,8 @@
 	pci_save_state(pdev);
 
 	err = pci_enable_device_mem(pdev);
-	if (err) {
+	if (err)
+	{
 		dev_err(pci_dev_to_dev(pdev),
 			"Cannot enable PCI device from suspend\n");
 		return err;
@@ -7667,7 +8160,7 @@
 
 	pci_enable_wake(pdev, PCI_D3hot, 0);
 	pci_enable_wake(pdev, PCI_D3cold, 0);
-#else /* USE_LEGACY_PM_SUPPORT */
+#else  /* USE_LEGACY_PM_SUPPORT */
 	pci_set_master(pdev);
 #endif /* USE_LEGACY_PM_SUPPORT */
 
@@ -7677,31 +8170,26 @@
 	e1000e_power_up_phy(adapter);
 
 	/* report the system wakeup cause from S3/S4 */
-	if (adapter->flags2 & FLAG2_HAS_PHY_WAKEUP) {
+	if (adapter->flags2 & FLAG2_HAS_PHY_WAKEUP)
+	{
 		u16 phy_data;
 
 		e1e_rphy(&adapter->hw, BM_WUS, &phy_data);
-		if (phy_data) {
+		if (phy_data)
+		{
 			e_info("PHY Wakeup cause - %s\n",
-			       phy_data & E1000_WUS_EX ? "Unicast Packet" :
-			       phy_data & E1000_WUS_MC ? "Multicast Packet" :
-			       phy_data & E1000_WUS_BC ? "Broadcast Packet" :
-			       phy_data & E1000_WUS_MAG ? "Magic Packet" :
-			       phy_data & E1000_WUS_LNKC ?
-			       "Link Status Change" : "other");
+			       phy_data & E1000_WUS_EX ? "Unicast Packet" : phy_data & E1000_WUS_MC ? "Multicast Packet" : phy_data & E1000_WUS_BC ? "Broadcast Packet" : phy_data & E1000_WUS_MAG ? "Magic Packet" : phy_data & E1000_WUS_LNKC ? "Link Status Change" : "other");
 		}
 		e1e_wphy(&adapter->hw, BM_WUS, ~0);
-	} else {
+	}
+	else
+	{
 		u32 wus = er32(WUS);
 
-		if (wus) {
+		if (wus)
+		{
 			e_info("MAC Wakeup cause - %s\n",
-			       wus & E1000_WUS_EX ? "Unicast Packet" :
-			       wus & E1000_WUS_MC ? "Multicast Packet" :
-			       wus & E1000_WUS_BC ? "Broadcast Packet" :
-			       wus & E1000_WUS_MAG ? "Magic Packet" :
-			       wus & E1000_WUS_LNKC ? "Link Status Change" :
-			       "other");
+			       wus & E1000_WUS_EX ? "Unicast Packet" : wus & E1000_WUS_MC ? "Multicast Packet" : wus & E1000_WUS_BC ? "Broadcast Packet" : wus & E1000_WUS_MAG ? "Magic Packet" : wus & E1000_WUS_LNKC ? "Link Status Change" : "other");
 		}
 		ew32(WUS, ~0);
 	}
@@ -7729,7 +8217,8 @@
 	e1000e_set_interrupt_capability(adapter);
 
 	rtnl_lock();
-	if (netif_running(netdev)) {
+	if (netif_running(netdev))
+	{
 		rc = e1000_request_irq(adapter);
 		if (rc)
 			goto err_irq;
@@ -7749,7 +8238,7 @@
 static int e1000e_pm_suspend(struct device *dev)
 #else
 static int e1000e_pm_suspend(struct pci_dev *pdev, pm_message_t state)
-#endif				/* USE_LEGACY_PM_SUPPORT */
+#endif /* USE_LEGACY_PM_SUPPORT */
 {
 #ifndef USE_LEGACY_PM_SUPPORT
 	struct pci_dev *pdev = to_pci_dev(dev);
@@ -7820,7 +8309,8 @@
 
 	eee_lp = adapter->hw.dev_spec.ich8lan.eee_lp_ability;
 
-	if (!e1000e_has_link(adapter)) {
+	if (!e1000e_has_link(adapter))
+	{
 		adapter->hw.dev_spec.ich8lan.eee_lp_ability = eee_lp;
 		pm_schedule_suspend(dev, 5 * MSEC_PER_SEC);
 	}
@@ -7851,7 +8341,8 @@
 	struct net_device *netdev = pci_get_drvdata(pdev);
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	if (netdev->flags & IFF_UP) {
+	if (netdev->flags & IFF_UP)
+	{
 		int count = E1000_CHECK_RESET_COUNT;
 
 		while (test_bit(__E1000_RESETTING, &adapter->state) && count--)
@@ -7862,7 +8353,8 @@
 		e1000e_down(adapter, false);
 	}
 
-	if (__e1000_shutdown(pdev, true)) {
+	if (__e1000_shutdown(pdev, true))
+	{
 		e1000e_pm_runtime_resume(dev);
 		return -EBUSY;
 	}
@@ -7870,7 +8362,7 @@
 	return 0;
 }
 #endif /* CONFIG_PM_RUNTIME */
-#else /* ! HAVE_CONFIG_PM_RUNTIME: unconditional since we are already under CONFIG_PM */
+#else  /* ! HAVE_CONFIG_PM_RUNTIME: unconditional since we are already under CONFIG_PM */
 static int e1000e_pm_runtime_idle(struct device *dev)
 {
 	struct pci_dev *pdev = to_pci_dev(dev);
@@ -7880,7 +8372,8 @@
 
 	eee_lp = adapter->hw.dev_spec.ich8lan.eee_lp_ability;
 
-	if (!e1000e_has_link(adapter)) {
+	if (!e1000e_has_link(adapter))
+	{
 		adapter->hw.dev_spec.ich8lan.eee_lp_ability = eee_lp;
 		pm_schedule_suspend(dev, 5 * MSEC_PER_SEC);
 	}
@@ -7911,7 +8404,8 @@
 	struct net_device *netdev = pci_get_drvdata(pdev);
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	if (netdev->flags & IFF_UP) {
+	if (netdev->flags & IFF_UP)
+	{
 		int count = E1000_CHECK_RESET_COUNT;
 
 		while (test_bit(__E1000_RESETTING, &adapter->state) && count--)
@@ -7923,7 +8417,8 @@
 		e1000e_down(adapter, false);
 	}
 
-	if (__e1000_shutdown(pdev, true)) {
+	if (__e1000_shutdown(pdev, true))
+	{
 		e1000e_pm_runtime_resume(dev);
 		return -EBUSY;
 	}
@@ -7950,12 +8445,15 @@
 {
 	struct pci_dev *pdev = NULL;
 
-	switch (event) {
+	switch (event)
+	{
 	case SYS_DOWN:
 	case SYS_HALT:
 	case SYS_POWER_OFF:
-		while ((pdev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, pdev))) {
-			if (pci_dev_driver(pdev) == &e1000_driver) {
+		while ((pdev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, pdev)))
+		{
+			if (pci_dev_driver(pdev) == &e1000_driver)
+			{
 				e1000e_pm_freeze(pci_dev_to_dev(pdev));
 				__e1000_shutdown(pdev, false);
 			}
@@ -7966,10 +8464,9 @@
 }
 
 static struct notifier_block e1000_notifier_reboot = {
-	.notifier_call = e1000_notify_reboot,
-	.next = NULL,
-	.priority = 0
-};
+    .notifier_call = e1000_notify_reboot,
+    .next = NULL,
+    .priority = 0};
 #endif /* USE_REBOOT_NOTIFIER */
 
 #ifdef CONFIG_NET_POLL_CONTROLLER
@@ -7979,7 +8476,8 @@
 	struct net_device *netdev = data;
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	if (adapter->msix_entries) {
+	if (adapter->msix_entries)
+	{
 		int vector, msix_irq;
 
 		vector = 0;
@@ -8016,7 +8514,8 @@
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
 
-	switch (adapter->int_mode) {
+	switch (adapter->int_mode)
+	{
 	case E1000E_INT_MODE_MSIX:
 		e1000_intr_msix(adapter->pdev->irq, netdev);
 		break;
@@ -8025,7 +8524,7 @@
 		e1000_intr_msi(adapter->pdev->irq, netdev);
 		enable_irq(adapter->pdev->irq);
 		break;
-	default:		/* E1000E_INT_MODE_LEGACY */
+	default: /* E1000E_INT_MODE_LEGACY */
 		disable_irq(adapter->pdev->irq);
 		e1000_intr(adapter->pdev->irq, netdev);
 		enable_irq(adapter->pdev->irq);
@@ -8080,11 +8579,14 @@
 		e1000e_disable_aspm_locked(pdev, aspm_disable_flag);
 
 	err = pci_enable_device_mem(pdev);
-	if (err) {
+	if (err)
+	{
 		dev_err(pci_dev_to_dev(pdev),
 			"Cannot re-enable PCI device after reset.\n");
 		result = PCI_ERS_RESULT_DISCONNECT;
-	} else {
+	}
+	else
+	{
 		pci_restore_state(pdev);
 		pci_set_master(pdev);
 		pci_save_state(pdev);
@@ -8138,8 +8640,7 @@
 	/* print bus type/speed/width info */
 	e_info("(PCI Express:2.5GT/s:%s) %02x:%02x:%02x:%02x:%02x:%02x\n",
 	       /* bus width */
-	       ((hw->bus.width == e1000_bus_width_pcie_x4) ? "Width x4" :
-		"Width x1"),
+	       ((hw->bus.width == e1000_bus_width_pcie_x4) ? "Width x4" : "Width x1"),
 	       /* MAC address */
 	       netdev->dev_addr[0], netdev->dev_addr[1],
 	       netdev->dev_addr[2], netdev->dev_addr[3],
@@ -8165,7 +8666,8 @@
 
 	ret_val = e1000_read_nvm(hw, NVM_INIT_CONTROL2_REG, 1, &buf);
 	le16_to_cpus(&buf);
-	if (!ret_val && (!(buf & BIT(0)))) {
+	if (!ret_val && (!(buf & BIT(0))))
+	{
 		/* Deep Smart Power Down (DSPD) */
 		dev_warn(pci_dev_to_dev(adapter->pdev),
 			 "Warning: detected DSPD enabled in EEPROM\n");
@@ -8184,7 +8686,7 @@
 	if ((hw->mac.type >= e1000_pch2lan) && (netdev->mtu > ETH_DATA_LEN))
 		features &= ~NETIF_F_RXFCS;
 
-	/* Since there is no support for separate Rx/Tx vlan accel
+		/* Since there is no support for separate Rx/Tx vlan accel
 	 * enable/disable make sure Tx flag is always in same state as Rx.
 	 */
 #ifdef NETIF_F_HW_VLAN_CTAG_RX
@@ -8214,19 +8716,22 @@
 
 	if (!(changed & (
 #if defined(NETIF_F_HW_VLAN_CTAG_TX)
-				NETIF_F_HW_VLAN_CTAG_RX |
-				NETIF_F_HW_VLAN_CTAG_TX |
+			    NETIF_F_HW_VLAN_CTAG_RX |
+			    NETIF_F_HW_VLAN_CTAG_TX |
 #elif defined(NETIF_F_HW_VLAN_TX)
-				NETIF_F_HW_VLAN_RX | NETIF_F_HW_VLAN_TX |
+			    NETIF_F_HW_VLAN_RX | NETIF_F_HW_VLAN_TX |
 #endif
-				NETIF_F_RXCSUM | NETIF_F_RXHASH | NETIF_F_RXFCS
-				| NETIF_F_RXALL)))
+			    NETIF_F_RXCSUM | NETIF_F_RXHASH | NETIF_F_RXFCS | NETIF_F_RXALL)))
 		return 0;
 
-	if (changed & NETIF_F_RXFCS) {
-		if (features & NETIF_F_RXFCS) {
+	if (changed & NETIF_F_RXFCS)
+	{
+		if (features & NETIF_F_RXFCS)
+		{
 			adapter->flags2 &= ~FLAG2_CRC_STRIPPING;
-		} else {
+		}
+		else
+		{
 			/* We need to take it back to defaults, which might mean
 			 * stripping is still disabled at the adapter level.
 			 */
@@ -8250,34 +8755,38 @@
 #endif /* HAVE_NDO_SET_FEATURES */
 #ifdef HAVE_NET_DEVICE_OPS
 static const struct net_device_ops e1000e_netdev_ops = {
-	.ndo_open		= e1000e_open,
-	.ndo_stop		= e1000e_close,
-	.ndo_start_xmit		= e1000_xmit_frame,
+	.ndo_open = e1000e_open,
+	.ndo_stop = e1000e_close,
+#ifdef ENTL_TX_ON_ENTL_ENABLE
+	.ndo_start_xmit	= entl_tx_transmit,
+#else
+	.ndo_start_xmit = e1000_xmit_frame,
+#endif
 #ifdef HAVE_NDO_GET_STATS64
-	.ndo_get_stats64	= e1000e_get_stats64,
-#else /* HAVE_NDO_GET_STATS64 */
-	.ndo_get_stats		= e1000_get_stats,
+	.ndo_get_stats64 = e1000e_get_stats64,
+#else  /* HAVE_NDO_GET_STATS64 */
+	.ndo_get_stats = e1000_get_stats,
 #endif /* HAVE_NDO_GET_STATS64 */
-	.ndo_set_rx_mode	= e1000e_set_rx_mode,
-	.ndo_set_mac_address	= e1000_set_mac,
+	.ndo_set_rx_mode = e1000e_set_rx_mode,
+	.ndo_set_mac_address = e1000_set_mac,
 #ifdef HAVE_RHEL7_EXTENDED_MIN_MAX_MTU
-	.extended.ndo_change_mtu	= e1000_change_mtu,
+	.extended.ndo_change_mtu = e1000_change_mtu,
 #else
-	.ndo_change_mtu		= e1000_change_mtu,
+	.ndo_change_mtu = e1000_change_mtu,
 #endif
-	.ndo_do_ioctl		= e1000_ioctl,
-	.ndo_tx_timeout		= e1000_tx_timeout,
-	.ndo_validate_addr	= eth_validate_addr,
+	.ndo_do_ioctl = e1000_ioctl,
+	.ndo_tx_timeout = e1000_tx_timeout,
+	.ndo_validate_addr = eth_validate_addr,
 
 #if defined(NETIF_F_HW_VLAN_RX) || defined(NETIF_F_HW_VLAN_CTAG_RX)
 #ifdef HAVE_VLAN_RX_REGISTER
-	.ndo_vlan_rx_register	= e1000_vlan_rx_register,
+	.ndo_vlan_rx_register = e1000_vlan_rx_register,
 #endif
-	.ndo_vlan_rx_add_vid	= e1000_vlan_rx_add_vid,
-	.ndo_vlan_rx_kill_vid	= e1000_vlan_rx_kill_vid,
+	.ndo_vlan_rx_add_vid = e1000_vlan_rx_add_vid,
+	.ndo_vlan_rx_kill_vid = e1000_vlan_rx_kill_vid,
 #endif
 #ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= e1000_netpoll,
+	.ndo_poll_controller = e1000_netpoll,
 #endif
 #if defined(HAVE_NDO_SET_FEATURES) && !defined(HAVE_RHEL6_NET_DEVICE_OPS_EXT)
 	.ndo_set_features = e1000_set_features,
@@ -8285,7 +8794,7 @@
 #endif /* HAVE_NDO_SET_FEATURES */
 #ifdef HAVE_NDO_FEATURES_CHECK
 #ifdef HAVE_PASSTHRU_FEATURES_CHECK
-	.ndo_features_check	= passthru_features_check,
+	.ndo_features_check = passthru_features_check,
 #endif
 #endif /*HAVE_NDO_FEATURES_CHECK*/
 };
@@ -8335,13 +8844,17 @@
 
 	pci_using_dac = 0;
 	err = dma_set_mask_and_coherent(pci_dev_to_dev(pdev), DMA_BIT_MASK(64));
-	if (!err) {
+	if (!err)
+	{
 		pci_using_dac = 1;
-	} else {
+	}
+	else
+	{
 		err =
 		    dma_set_mask_and_coherent(pci_dev_to_dev(pdev),
 					      DMA_BIT_MASK(32));
-		if (err) {
+		if (err)
+		{
 			dev_err(pci_dev_to_dev(pdev),
 				"No usable DMA configuration, aborting\n");
 			goto err_dma;
@@ -8349,8 +8862,8 @@
 	}
 
 	err = pci_request_selected_regions_exclusive(pdev,
-					  pci_select_bars(pdev, IORESOURCE_MEM),
-					  e1000e_driver_name);
+						     pci_select_bars(pdev, IORESOURCE_MEM),
+						     e1000e_driver_name);
 	if (err)
 		goto err_pci_reg;
 
@@ -8389,7 +8902,11 @@
 	adapter->hw.mac.type = ei->mac;
 	adapter->max_hw_frame_size = ei->max_hw_frame_size;
 	adapter->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);
-
+#ifdef ENTL
+	spin_lock_init(&adapter->entl_txring_lock);
+	entl_device_init(&adapter->entl_dev);
+	adapter->entl_flag = 1;
+#endif
 	/* Workaround FLR issues for 82579
 	 * This code disables the FLR (Function Level Reset) via PCIe, in order
 	 * to workaround a bug found while using device passthrough, where the
@@ -8398,16 +8915,20 @@
 	 * the BIOS or kernel writes this register * then this workaround will
 	 * not work.
 	 */
-	if (hw->mac.type == e1000_pch2lan) {
+	if (hw->mac.type == e1000_pch2lan)
+	{
 		int pos = pci_find_capability(pdev, PCI_CAP_ID_AF);
 
-		if (pos) {
+		if (pos)
+		{
 			u8 cap;
 
 			pci_read_config_byte(pdev, pos + PCI_AF_CAP, &cap);
 			cap = cap & (~PCI_AF_CAP_FLR);
 			pci_write_config_byte(pdev, pos + PCI_AF_CAP, cap);
-		} else {
+		}
+		else
+		{
 			e_info("PCI AF capability not found\n");
 		}
 	}
@@ -8422,7 +8943,8 @@
 
 	if ((adapter->flags & FLAG_HAS_FLASH) &&
 	    (pci_resource_flags(pdev, 1) & IORESOURCE_MEM) &&
-	    (hw->mac.type < e1000_pch_spt)) {
+	    (hw->mac.type < e1000_pch_spt))
+	{
 		flash_start = pci_resource_start(pdev, 1);
 		flash_len = pci_resource_len(pdev, 1);
 		adapter->hw.flash_address = ioremap(flash_start, flash_len);
@@ -8434,13 +8956,17 @@
 	if (adapter->flags2 & FLAG2_HAS_EEE)
 		adapter->eee_advert = MDIO_EEE_100TX | MDIO_EEE_1000T;
 
-	/* construct the net_device struct */
+		/* construct the net_device struct */
 #ifdef HAVE_NET_DEVICE_OPS
 	netdev->netdev_ops = &e1000e_netdev_ops;
 #else
 	netdev->open = &e1000e_open;
 	netdev->stop = &e1000e_close;
+#ifdef ENTL_TX_ON_ENTL_ENABLE
+	netdev->hard_start_xmit = &entl_tx_transmit;
+#else
 	netdev->hard_start_xmit = &e1000_xmit_frame;
+#endif
 	netdev->get_stats = &e1000_get_stats;
 #ifdef HAVE_SET_RX_MODE
 	netdev->set_rx_mode = &e1000e_set_rx_mode;
@@ -8496,7 +9022,8 @@
 	adapter->hw.phy.autoneg_wait_to_complete = 0;
 
 	/* Copper options */
-	if (adapter->hw.phy.media_type == e1000_media_type_copper) {
+	if (adapter->hw.phy.media_type == e1000_media_type_copper)
+	{
 		adapter->hw.phy.mdix = AUTO_ALL_MODES;
 		adapter->hw.phy.disable_polarity_correction = 0;
 		adapter->hw.phy.ms_type = e1000_ms_hw_default;
@@ -8568,7 +9095,8 @@
 	netdev->priv_flags |= IFF_UNICAST_FLT;
 
 #endif /* IFF_UNICAST_FLT */
-	if (pci_using_dac) {
+	if (pci_using_dac)
+	{
 		netdev->features |= NETIF_F_HIGHDMA;
 #ifdef HAVE_NETDEV_VLAN_FEATURES
 		netdev->vlan_features |= NETIF_F_HIGHDMA;
@@ -8579,11 +9107,11 @@
 #ifdef HAVE_RHEL7_EXTENDED_MIN_MAX_MTU
 	netdev->extended->min_mtu = ETH_MIN_MTU;
 	netdev->extended->max_mtu = adapter->max_hw_frame_size -
-	    (VLAN_ETH_HLEN + ETH_FCS_LEN);
+				    (VLAN_ETH_HLEN + ETH_FCS_LEN);
 #else
 	netdev->min_mtu = ETH_MIN_MTU;
 	netdev->max_mtu = adapter->max_hw_frame_size -
-	    (VLAN_ETH_HLEN + ETH_FCS_LEN);
+			  (VLAN_ETH_HLEN + ETH_FCS_LEN);
 #endif /* HAVE_RHEL7_EXTENDED_MIN_MAX_MTU */
 #endif /* HAVE_NETDEVICE_MIN_MAX_MTU */
 
@@ -8598,10 +9126,12 @@
 	/* systems with ASPM and others may see the checksum fail on the first
 	 * attempt. Let's give it a few tries
 	 */
-	for (i = 0;; i++) {
+	for (i = 0;; i++)
+	{
 		if (e1000_validate_nvm_checksum(&adapter->hw) >= 0)
 			break;
-		if (i == 2) {
+		if (i == 2)
+		{
 			dev_err(pci_dev_to_dev(pdev),
 				"The NVM Checksum Is Not Valid\n");
 			err = -EIO;
@@ -8621,7 +9151,8 @@
 	memcpy(netdev->perm_addr, adapter->hw.mac.addr, netdev->addr_len);
 #endif
 
-	if (!is_valid_ether_addr(netdev->dev_addr)) {
+	if (!is_valid_ether_addr(netdev->dev_addr))
+	{
 		dev_err(pci_dev_to_dev(pdev),
 			"Invalid MAC Address: %02x:%02x:%02x:%02x:%02x:%02x\n",
 			netdev->dev_addr[0], netdev->dev_addr[1],
@@ -8631,6 +9162,10 @@
 		goto err_eeprom;
 	}
 
+#ifdef ENTL
+	entl_e1000_set_my_addr(adapter, netdev->dev_addr);
+#endif
+
 	timer_setup(&adapter->watchdog_timer, e1000_watchdog, 0);
 
 	timer_setup(&adapter->phy_info_timer, e1000_update_phy_info, 0);
@@ -8654,14 +9189,17 @@
 	/* Initial Wake on LAN setting - If APM wake is enabled in
 	 * the EEPROM, enable the ACPI Magic Packet filter
 	 */
-	if (adapter->flags & FLAG_APME_IN_WUC) {
+	if (adapter->flags & FLAG_APME_IN_WUC)
+	{
 		/* APME bit in EEPROM is mapped to WUC.APME */
 		eeprom_data = er32(WUC);
 		eeprom_apme_mask = E1000_WUC_APME;
 		if ((hw->mac.type > e1000_ich10lan) &&
 		    (eeprom_data & E1000_WUC_PHY_WAKE))
 			adapter->flags2 |= FLAG2_HAS_PHY_WAKEUP;
-	} else if (adapter->flags & FLAG_APME_IN_CTRL3) {
+	}
+	else if (adapter->flags & FLAG_APME_IN_CTRL3)
+	{
 		if (adapter->flags & FLAG_APME_CHECK_PORT_B &&
 		    (adapter->hw.bus.func == 1))
 			ret_val = e1000_read_nvm(&adapter->hw,
@@ -8712,7 +9250,8 @@
 	/* save off EEPROM version number */
 	ret_val = e1000_read_nvm(&adapter->hw, 5, 1, &adapter->eeprom_vers);
 
-	if (ret_val) {
+	if (ret_val)
+	{
 		e_dbg("NVM read error getting EEPROM version: %d\n", ret_val);
 		adapter->eeprom_vers = 0;
 	}
@@ -8808,9 +9347,11 @@
 	cancel_work_sync(&adapter->print_hang_task);
 
 #ifdef HAVE_HW_TIME_STAMP
-	if (adapter->flags & FLAG_HAS_HW_TIMESTAMP) {
+	if (adapter->flags & FLAG_HAS_HW_TIMESTAMP)
+	{
 		cancel_work_sync(&adapter->tx_hwtstamp_work);
-		if (adapter->tx_hwtstamp_skb) {
+		if (adapter->tx_hwtstamp_skb)
+		{
 			dev_kfree_skb_any(adapter->tx_hwtstamp_skb);
 			adapter->tx_hwtstamp_skb = NULL;
 		}
@@ -8860,160 +9401,159 @@
 #endif
 
 static const struct pci_device_id e1000_pci_tbl[] = {
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_COPPER), board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_FIBER), board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_QUAD_COPPER), board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_QUAD_COPPER_LP),
-	  board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_QUAD_FIBER), board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_SERDES), board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_SERDES_DUAL), board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_SERDES_QUAD), board_82571 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82571PT_QUAD_COPPER), board_82571 },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI), board_82572 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI_COPPER), board_82572 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI_FIBER), board_82572 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI_SERDES), board_82572 },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82573E), board_82573 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82573E_IAMT), board_82573 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82573L), board_82573 },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82574L), board_82574 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82574LA), board_82574 },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_82583V), board_82583 },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_COPPER_DPT),
-	  board_80003es2lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_COPPER_SPT),
-	  board_80003es2lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_SERDES_DPT),
-	  board_80003es2lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_SERDES_SPT),
-	  board_80003es2lan },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IFE), board_ich8lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IFE_G), board_ich8lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IFE_GT), board_ich8lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_AMT), board_ich8lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_C), board_ich8lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_M), board_ich8lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_M_AMT), board_ich8lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_82567V_3), board_ich8lan },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IFE), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IFE_G), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IFE_GT), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_AMT), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_C), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_BM), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_M), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_M_AMT), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_M_V), board_ich9lan },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_R_BM_LM), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_R_BM_LF), board_ich9lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_R_BM_V), board_ich9lan },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_D_BM_LM), board_ich10lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_D_BM_LF), board_ich10lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_D_BM_V), board_ich10lan },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_M_HV_LM), board_pchlan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_M_HV_LC), board_pchlan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_D_HV_DM), board_pchlan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_D_HV_DC), board_pchlan },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH2_LV_LM), board_pch2lan },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH2_LV_V), board_pch2lan },
-
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPT_I217_LM), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPT_I217_V), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPTLP_I218_LM), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPTLP_I218_V), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_LM2), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_V2), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_LM3), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_V3), board_pch_lpt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM2), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V2), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LBG_I219_LM3), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM4), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V4), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM5), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V5), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_LM12), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_V12), board_pch_spt },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_LM6), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_V6), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_LM7), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_V7), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_LM8), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_V8), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_LM9), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_V9), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_LM10), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_V10), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_LM11), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_V11), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_LM13), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_V13), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_LM14), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_V14), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_LM15), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_V15), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_LM16), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_V16), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_LM17), board_pch_cnp },
-	{ PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_V17), board_pch_cnp },
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_COPPER), board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_FIBER), board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_QUAD_COPPER), board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_QUAD_COPPER_LP),
+     board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_QUAD_FIBER), board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_SERDES), board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_SERDES_DUAL), board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571EB_SERDES_QUAD), board_82571},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82571PT_QUAD_COPPER), board_82571},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI), board_82572},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI_COPPER), board_82572},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI_FIBER), board_82572},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82572EI_SERDES), board_82572},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82573E), board_82573},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82573E_IAMT), board_82573},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82573L), board_82573},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82574L), board_82574},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82574LA), board_82574},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_82583V), board_82583},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_COPPER_DPT),
+     board_80003es2lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_COPPER_SPT),
+     board_80003es2lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_SERDES_DPT),
+     board_80003es2lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_80003ES2LAN_SERDES_SPT),
+     board_80003es2lan},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IFE), board_ich8lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IFE_G), board_ich8lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IFE_GT), board_ich8lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_AMT), board_ich8lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_C), board_ich8lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_M), board_ich8lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_IGP_M_AMT), board_ich8lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH8_82567V_3), board_ich8lan},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IFE), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IFE_G), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IFE_GT), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_AMT), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_C), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_BM), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_M), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_M_AMT), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH9_IGP_M_V), board_ich9lan},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_R_BM_LM), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_R_BM_LF), board_ich9lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_R_BM_V), board_ich9lan},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_D_BM_LM), board_ich10lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_D_BM_LF), board_ich10lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_ICH10_D_BM_V), board_ich10lan},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_M_HV_LM), board_pchlan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_M_HV_LC), board_pchlan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_D_HV_DM), board_pchlan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_D_HV_DC), board_pchlan},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH2_LV_LM), board_pch2lan},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH2_LV_V), board_pch2lan},
+
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPT_I217_LM), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPT_I217_V), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPTLP_I218_LM), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LPTLP_I218_V), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_LM2), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_V2), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_LM3), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_I218_V3), board_pch_lpt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM2), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V2), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_LBG_I219_LM3), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM4), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V4), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_LM5), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_SPT_I219_V5), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_LM12), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_V12), board_pch_spt},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_LM6), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_V6), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_LM7), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CNP_I219_V7), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_LM8), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_V8), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_LM9), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ICP_I219_V9), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_LM10), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_V10), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_LM11), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_CMP_I219_V11), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_LM13), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_V13), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_LM14), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_V14), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_LM15), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_TGP_I219_V15), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_LM16), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_V16), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_LM17), board_pch_cnp},
+    {PCI_VDEVICE(INTEL, E1000_DEV_ID_PCH_ADL_I219_V17), board_pch_cnp},
 
-	{ 0, 0, 0, 0, 0, 0, 0 }	/* terminate list */
+    {0, 0, 0, 0, 0, 0, 0} /* terminate list */
 };
 MODULE_DEVICE_TABLE(pci, e1000_pci_tbl);
 
 #ifdef CONFIG_PM
 #ifndef USE_LEGACY_PM_SUPPORT
 static const struct dev_pm_ops e1000_pm_ops = {
-	.suspend	= e1000e_pm_suspend,
-	.resume		= e1000e_pm_resume,
-	.freeze		= e1000e_pm_freeze,
-	.thaw		= e1000e_pm_thaw,
-	.poweroff	= e1000e_pm_suspend,
-	.restore	= e1000e_pm_resume,
-	SET_RUNTIME_PM_OPS(e1000e_pm_runtime_suspend, e1000e_pm_runtime_resume,
-			   e1000e_pm_runtime_idle)
-};
+    .suspend = e1000e_pm_suspend,
+    .resume = e1000e_pm_resume,
+    .freeze = e1000e_pm_freeze,
+    .thaw = e1000e_pm_thaw,
+    .poweroff = e1000e_pm_suspend,
+    .restore = e1000e_pm_resume,
+    SET_RUNTIME_PM_OPS(e1000e_pm_runtime_suspend, e1000e_pm_runtime_resume,
+		       e1000e_pm_runtime_idle)};
 #endif /* USE_LEGACY_PM_SUPPORT */
 #endif
 
 /* PCI Device API Driver */
 static struct pci_driver e1000_driver = {
-	.name     = e1000e_driver_name,
-	.id_table = e1000_pci_tbl,
-	.probe    = e1000_probe,
+    .name = e1000e_driver_name,
+    .id_table = e1000_pci_tbl,
+    .probe = e1000_probe,
 #ifdef HAVE_CONFIG_HOTPLUG
-	.remove   = __devexit_p(e1000_remove),
+    .remove = __devexit_p(e1000_remove),
 #else
-	.remove   = e1000_remove,
+    .remove = e1000_remove,
 #endif
 #ifdef CONFIG_PM
 #ifndef USE_LEGACY_PM_SUPPORT
-	.driver   = {
-		.pm = &e1000_pm_ops,
-	},
+    .driver = {
+	.pm = &e1000_pm_ops,
+    },
 #elif defined(CONFIG_PM_SLEEP)
-	.suspend  = e1000e_pm_suspend,
-	.resume   = e1000e_pm_resume,
+    .suspend = e1000e_pm_suspend,
+    .resume = e1000e_pm_resume,
 #endif /* USE_LEGACY_PM_SUPPORT */
 #endif
 #ifndef USE_REBOOT_NOTIFIER
-	.shutdown = e1000_shutdown,
+    .shutdown = e1000_shutdown,
 #endif
 #ifdef HAVE_PCI_ERS
-	.err_handler = &e1000_err_handler
+    .err_handler = &e1000_err_handler
 #endif
 };
 
@@ -9028,7 +9568,10 @@
 	pr_info("Intel(R) PRO/1000 Network Driver - %s\n",
 		e1000e_driver_version);
 	pr_info("Copyright(c) 1999 - 2020 Intel Corporation.\n");
-
+#ifdef ENTL
+	pr_info("Earth Computing ECNL extension\n");
+	pr_info("Copyright(c) 2016 - 2021 Earth Computing Corporation.\n");
+#endif
 #ifndef USE_REBOOT_NOTIFIER
 	return pci_register_driver(&e1000_driver);
 #else
@@ -9055,8 +9598,18 @@
 }
 module_exit(e1000_exit_module);
 
+#ifdef ENTL
+#define HEDGEHOG_384
+#include <linux/sched/signal.h>
+#include "entl_device.c"
+#endif
+#ifdef ENTL
+MODULE_AUTHOR("Intel Corporation, <linux.nics@intel.com>, Earth Computing Corporation");
+MODULE_DESCRIPTION("Earth Computing ECNL, Based on Intel(R) PRO/1000 Network Driver");
+#else
 MODULE_AUTHOR("Intel Corporation, <linux.nics@intel.com>");
 MODULE_DESCRIPTION("Intel(R) PRO/1000 Network Driver");
+#endif
 MODULE_LICENSE("GPL");
 MODULE_VERSION(DRV_VERSION);
 
diff -ruN intel-e1000e-3.8.4/src/netdev_entl_if.h e1000e-3.8.4/src/netdev_entl_if.h
--- intel-e1000e-3.8.4/src/netdev_entl_if.h	1969-12-31 16:00:00.000000000 -0800
+++ e1000e-3.8.4/src/netdev_entl_if.h	2021-04-25 17:41:46.972474472 -0700
@@ -0,0 +1,27 @@
+#ifndef _NETDEV_ENTL_IF_H_
+#define _NETDEV_ENTL_IF_H_
+
+#ifdef _IN_NETDEV_C_
+
+// back references to netdev.c
+static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb, struct net_device *netdev);
+
+// references from netdev.c patch into (included) entl_device.c
+static void entl_device_init(entl_device_t *dev);
+static void entl_device_link_down(entl_device_t *dev);
+static void entl_device_link_up(entl_device_t *dev);
+static bool entl_device_process_rx_packet(entl_device_t *dev, struct sk_buff *skb);
+static void entl_device_process_tx_packet(entl_device_t *dev, struct sk_buff *skb);
+static int entl_do_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd);
+static void entl_e1000_configure(struct e1000_adapter *adapter);
+static void entl_e1000_set_my_addr(struct e1000_adapter *adapter, const uint8_t *addr);
+#ifdef ENTL_TX_ON_ENTL_ENABLE
+static netdev_tx_t entl_tx_transmit(struct sk_buff *skb, struct net_device *netdev);
+#endif
+
+// dead code
+// static int entl_tx_queue_has_data(entl_device_t *dev);
+// static void entl_tx_pull(struct net_device *netdev);
+
+#endif
+#endif
